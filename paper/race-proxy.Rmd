---
title: "Estimation of Racial Disparities when Race is Not Observed"
author: 
date: '`r format(Sys.Date(), "%B %e, %Y")`'
abstract: |
    Abstract here.
keywords: 
    - keyword 1
    - keyword 2
bibliography: references.bib
biblio-style: apalike
output:
    bookdown::pdf_document2:
        template: "template.tex"
        number_sections: true
        keep_tex: true
        includes: 
            in_header: "header.tex"
        latex_engine: pdflatex
        citation_package: natbib
editor_options: 
    markdown: 
        wrap: sentence
---

```{r setup, include=FALSE}
library(knitr)
library(here)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE,
               fig.path=here("paper/figures/"), fig.align="center",
               fig.width=(8.5-2*1)/2, out.width="100%", fig.asp=0.8)
options(bookdown.theorem.preamble = FALSE)
```

# Introduction

BISG [@fiscella2006bisg; @elliott2008bisg].

# Problem and Identification

## Setup and BISG Procedure

For each individual $i$ in our dataset, we have the following variables.

-   $R_i\in \cR$: race (unknown)
-   $X_i\in\cX$: outcome
-   $G_i\in\cG$: location of residence
-   $Z_i\in\cZ$: other covariates
-   $S_i\in\cS$: surname <!-- -   $Y_i\in\cY$: algorithmic outcome (does not depend on race) -->

We assume throughout that these variables are discrete.
In practice, location is discrete, since joint information about location, race, and other variables is only available down to the Census block level.
Continuous outcomes here must be discretized, though there are natural extensions of the model that would make use of any continuous structure.

The BISG(Z) procedure assumes that $S\indep G, Z\mid R$.
With this assumption, $$
    \Pr(R=r\mid G=g, Z=z, S=s) \ \propto\ 
    \Pr(G=g, Z=z\mid R=r)\Pr(S=s \mid R=r)\Pr(R=r),
$$ by Bayes' Rule.
The first and last terms are taken from decennial Census tables or the American Community Survey (ACS), while the second term comes from the Census Bureau's surname tables [@censusnames].
Throughout, let $\hat{\vb r}_i \dfeq \Pr(R_i\mid G_i,Z_i,S_i)$ be these BISG(Z) probabilities.

Then a natural estimator of $\Pr(X=x\mid R=r)$ is a weighted average: $$
    \hat\mu^{(\text{wtd})}_{X|R}(x\mid r) 
    = \frac{\sum_{i=1}^N \ind\{X_i=x\}\hat r_{ir}}{
        \sum_{i=1}^N \hat r_{ir}}.
$$ However natural, this estimator should raise suspicions as to being unbiased for $\Pr(X=x\mid R=r)$, since BISG makes no assumptions about the relationship of $X$ to any of $R$, $G$, $Z$, or $S$.
Indeed, the estimator is biased, with the amount of bias controlled by the residual correlation of $X$ and $R$ after controlling for $G$, $Z$, and $S$.

```{=tex}
\begin{theorem}[Theorem 3.1 of \citealt{chen2019fairness}]
If race is binary (so $\cR=\{0,1\}$), then as $N\to\infty$, \[
    \hat\mu^{(\text{wtd})}_{X|R}(x\mid r) - \Pr(X=x\mid R=r)
    \cvas -\frac{\E[\Cov(\ind\{X=x\}, \ind\{R=r\}\mid G,Z,S)]}{\Pr(R=r)}.
\]
\end{theorem}
```
If any racial disparity in $X$ is mediated purely through $G$, $Z$, and $S$, so that $X\indep R\mid G,Z,S$, then the estimator will be unbiased.
While almost certainly untrue in most practical settings, there may be a few cases in which this is an appropriate assumption.
For example, for a manager reviewing job applications, race is typically unobserved, but the manager may be influenced by racial or gender cues in a candidate's name [@park2009names; @aaslund2012names].
Then the assumption $X\indep R\mid G,Z,S$ is naturally satisfied and $\hat\mu^{(\text{wtd})}_{X|R}(x\mid r)$ will be asymptotically unbiased.
But outside of these cases, the natural BISG estimator will fail, unless by pure luck biases for some combinations of the covariates cancel biases for other combinations of the covariates once the outer expectation is taken.

## Identification

We seek to estimate $\Pr(X=x\mid R=r$ without the bias inherent in the default BISG estimator $\hat\mu^{(\text{wtd})}_{X|R}(x\mid r)$.
Our key identifying assumption is the following (cf. the implicit BISG assumption $X\indep R\mid G,Z,S$).

```{=tex}
\begin{assump}\label{assump:xs}
For all $1\le i\le N$, $X_i\indep S_i\mid R_i,G_i,Z_i$.
\end{assump}
```
To better understand this assumption, consider the causal model depicted by the directed acyclic graph (DAG) in Figure \ref{fig:dag}.

```{=tex}
\begin{figure}[ht]
\begin{center}
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,thick]
    
        \node[main node] (G) {$G$};
        \node[main node] (R) [below of=G] {$R$};
        \node[main node] (S) [below left=0.6cm and 0.6cm of R] {$S$};
        \node[main node] (X) [right of=G] {$X$};
        \node[main node] (Z) [below of=X] {$Z$};
        %\node[main node] (Y) [below right=0.5cm and 1cm of X] {$Y$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (Z)
        (R) edge node  {} (X)
        (R) edge node  {} (S)
        (G) edge node  {} (Z)
        (G) edge node  {} (X)
        (Z) edge node  {} (X);
        %(Z) edge node  {} (Y)
        %(X) edge node  {} (Y);
        
        \path[every node/.style={font=\sffamily\small}, <->]
        (G) edge [dashed, bend right] node  {} (S)
        (Z) edge [dashed, bend left] node  {} (S);
    \end{tikzpicture}
\end{center}
\caption{One possible causal structure for which the proposed model applies, represented as a directed acyclic graph.}
\label{fig:dag}
\end{figure}
```
The DAG implies that $X\indep S\mid R,G,Z$, since all paths from $S$ to $X$ are blocked by $R$, $G$, or $Z$ (for more details, see \citealt{pearl1995causal}).
Of course, other causal structures, such as a version of Figure \ref{fig:dag} with additional unobserved confounding between $G$, $Z$, $R$, and $X$, are possible; as long as $X\indep S\mid R,G,Z$, then $\Pr(X=x\mid R=r)$ is identified, as we will see below.

In any particular application it is important to consider whether names have a causal effect on the outcome, or whether name and outcome are caused by an unobserved confounder that is not controlled for in $Z$.
In these cases (such as in the hiring example above), our proposed method may not be appropriate.

But in many settings, including our motivating examples of tax audit decisions and voter turnout, name can reasonably be considered independent of the outcome, conditional on race, location, and perhaps one or two other variables (such as income).
The following theorem records the precise conditions needed for identification.

\begin{theoremrep}
For all $g\in\cG$, $z\in\cZ$, and $x\in\cX$, define a matrix $\hat{\vb R}^{(xgz)}$ with entries $\hat r^{(xgz)}_{rs}=\Pr(R=r\mid G=g, Z=z, S=s)$ has rank $|\cR|$, and a vector $\vb b^{(xgz)}$ with entries $b^{(xgz)}_s=\Pr(X=x\mid G=g, Z=z, S=s)$.
Then under Assumption~\ref{assump:xs}, and assuming knowledge of the joint distribution $\Pr(R,G,Z,S)$, the conditional probabilities $\Pr(X=x\mid R, G=g, Z=z)$ are identified if and only if both $\hat{\vb R}^{(xgz)}$ and the augmented matrix $\mqty(\hat{\vb R}^{(xgz)}&\vb b^{(xgz)})$ have rank $|\cR|$.
\end{theoremrep}
\begin{proof}
Applying the law of total probability and our conditional independence relation $S\indep X\mid R,G,Z$, we have,
for all $x\in\cX$, $g\in\cG$, $z\in\cZ$, and $s\in\cS$,
\begin{align*}
    \Pr(X=x\mid G=g, Z=z, S=s)
    &= \sum_{r\in\cR} \Pr(X=x\mid R=r, G=g, Z=z, S=s)\Pr(R=r\mid G=g, Z=z, S=s) \\
    &= \sum_{r\in\cR} \Pr(X=x\mid R=r, G=g, Z=z)\Pr(R=r\mid G=g, Z=z, S=s).
\end{align*}
The left-hand side is estimable from the data and the rightmost term $\Pr(R=r\mid G=g, Z=z, S=s)$ is assumed known.
So for each $x\in\cX$, $g\in\cG$, and $z\in\cZ$, this relation is a linear system in unknown parameters $\Pr(X=x\mid R=r, G=g, Z=z)$.
These parameters are identified if and only if this system has a unique solution, i.e. if the coefficient matrix $\hat{\vb R}^{(xgz)}$ has rank $|\cR|$ and so does the augmented matrix $\mqty(\hat{\vb R}^{(xgz)}&b^{(xgz)})$.
\end{proof}

See @kuroki2014measurement and @miao2018identifying for similar ideas in the context of causal identification with unmeasured confounders with noisy proxies.

## Comparison with Weighted Estimator

The above identification equation may bring to mind a least-squares estimator.
This is not a good idea for several reasons, not least of which is the fact that the unknown parameters are probabilities and thus constrained to lie in $[0,1]$ and sum to 1.
However, the ordinary least squares estimator provides useful intuition as to how existing methods fail, and why there is additional information "left on the table" to use in producing an accurate estimation of racial disparities.

Continue with the above notation but suppose that $X$ is binary.
Then the usual weighted estimator of $\Pr[X=x\mid R=r]$ may be written $$
    \hat\mu^{(\text{wtd})}_{X|R}(r) 
    = \frac{\hat{\vb r}_{\cdot r}^\top \vb X}{\hat{\vb r}_{\cdot r}^\top \vb 1}
    = \frac{\norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb X)}}{
        \norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb 1)}},
$$ the ratio of the projected length of the outcome vector $\vb X$ and the constant vector $\vb 1$ onto $\hat{\vb r}_{\cdot r}$.
For the OLS estimator, we can rewrite the identification equation in this case on an observation level as $$
    \vb X = \hat{\vb R} \hat{\vb*\mu}^{(\text{OLS})}_{X|R} ,
$$ where $\hat{\vb R}$ is the $n$-by-$r$ matrix with columns $\hat{\vb r}_{\cdot r}$.
Then the estimator is $$
    \hat{\vb*\mu}^{(\text{OLS})}_{X|R} = (\hat{\vb R}^\top\hat{\vb R})^{-1}\hat{\vb R}^\top\vb X
    = \mathrm{coord}_{\hat{\vb R}}(\proj_{\hat{\vb R}}(\vb X)),
$$ where $\mathrm{coord}_{\hat{\vb R}}$ is the function that returns the coordinates of its input vector in the $\hat{\vb R}$ basis (by assumption $\hat{\vb R}$ has rank $|\cR|$ and so its columns are linearly independent).
To make the comparison even easier, notice that we can break the projection $\proj_{\hat{\vb r}_{\cdot r}}$ into two steps, writing it instead as $\proj_{\hat{\vb r}_{\cdot r}} = \proj_{\hat{\vb r}_{\cdot r}} \circ \proj_{\hat{\vb R}},$ since $\hat{\vb r}$ is one of the columns of $\hat{\vb R}$.
Letting $\vb X_{\proj}=\proj_{\hat{\vb R}}(X)$, then, we can rewrite our estimators as $$
    \hat\mu^{(\text{wtd})}_{X|R}(r) 
    = \frac{\norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb X_\proj)}}{
        \norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb 1)}}  \qand
    \hat{\vb\mu}^{(\text{OLS})}_{X|R}(r)
    = \mathrm{coord}_{\hat{\vb R}}(\vb X_\proj)_r.
$$ From this representation, we can easily intuit that these two estimators should agree when the columns of $\hat{\vb R}$ are orthogonal, because then one may compute coordinate representations by projecting onto the basis vectors.
This is indeed the case, as the following result precisely states.

\begin{theoremrep}
$\hat{\vb*\mu}^{(\text{wtd})}_{X|R}=\hat{\vb*\mu}^{(\text{OLS})}_{X|R}$ if and only if
for every pair $j,k\in\cR$, either the BISGZ probabilities perfectly discriminate (i.e., $\Pr(R_i=j\mid G_i,Z_i,S_i)>0$ implies $\Pr(R_i=k\mid G_i,Z_i,S_i)=0$ and vice versa) or 
$\hat{\mu}^{(\text{wtd})}_{X|R}(j)=\hat{\mu}^{(\text{wtd})}_{X|R}(k)$ .
\end{theoremrep}
\begin{proof}
Since the individual BISGZ probabilities are nonnegative and sum to 1, a pair $j,k\in\cR$ of races has perfectly discriminating BISGZ probabilities if and only if the corresponding columns of $\hat{\vb R}$ are orthogonal, i.e., $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}$.
Begin by writing $\vb X_\proj$ in terms of the $\hat{\vb R}$ basis, so \[
    \vb X_\proj = \sum_{j\in\cR} c_j\hat{\vb r}_{\cdot j},
\] and thus $\hat{\vb\mu}^{(\text{OLS})}_{X|R}(j)=c_j$.
Without loss of generality, suppose the $c_j$ are numbered as $c_1\ge c_2\ge \cdots\ge c_{|\cR|}$.
We can also expand $\vb 1$ in the same basis. Since the individual probabilities must sum to one, in fact we have \(
    \vb 1 = \sum_{j\in\cR} \hat{\vb r}_{\cdot j}.
\)

For the forward direction, we assume $\hat{\mu}^{(\text{wtd})}_{X|R}(j)=\hat{\mu}^{(\text{OLS})}_{X|R}(j)=c_j$;
multiplying out the denominator of the weighted estimator, we have $\hat{\vb r}_{\cdot j}^\top \vb X=c_j\hat{\vb r}_{\cdot j}^\top\vb 1$ for all $j$; substituting the basis expansions of $\vb X_\proj$ and $\vb 1$, this yields \[
    \sum_{k\in\cR} c_k \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}
    = \sum_{k\in\cR} c_j \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}, \qq{so}
    \sum_{k\in\cR} (c_j-c_k) \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0.
\] Now fix $j\in J_1=\argmax_j c_j$; this relation still holds, but now every term in the sum is nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1$.
Therefore we must have $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$ for all $k\not\in J_1$.
Then fix $j\in J_2=\argmax_{j\not\in J_1} c_j$; since $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot l}=0$ for all $l\in J_1$, every term in the sum is still nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1\cup J_2$.
Therefore we must have $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$ for all $k\not\in J_1\cup J_2$.
Proceeding this way through all sets of common values in the $c_j$ we find that for all $j,k\in\cR$, either $c_j=c_k$ or  $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$.

For the reverse direction, fix $j\in\cR$ and let $J=\{k\in\cR:c_k=c_j\}$, so that by assumption $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$ for all $k\not\in J$.
Then by the above basis expansion, $\hat{\mu}^{(\text{OLS})}_{X|R}(j)=c_j$, and \[
    \hat{\mu}^{(\text{wtd})}_{X|R}(j) 
    = \frac{\sum_{k\in\cR} c_k \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}{
        \sum_{k\in\cR}\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}
    = \frac{c_j \sum_{k\in J} \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}{
        \sum_{k\in J}\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}
    = c_j = \hat{\mu}^{(\text{OLS})}_{X|R}(j). \qedhere
\]
\end{proof}

The theorem shows that if the BISGZ probabilities aren't perfect and the weighted estimator isn't constant across races, then OLS will give a different result.
But it doesn't provide much intuition as to why the estimators are different, and which is properly using all of the available information.
That can be illustrated well by the following stylized example, where for simplicity we consider a binary race and outcome.

Take the race vector $\vb R$ as fixed, and suppose that there is complete disparity, so $\vb X=\vb R$.
As we have so far, assume the BISGZ probabilities are well-calibrated, so $\hat{\vb R}=\Pr(\vb R\mid\vb G,\vb Z,\vb S)$.
Then the OLS estimator may be written as $$
    \hat{\vb*\mu}^{(\text{OLS})}_{X|R}
    = \vb*\mu_{X|R} + (\hat{\vb R}^\top\hat{\vb R})^{-1})\hat{\vb R}^\top\vb*\eps,
$$ where $\vb*\mu_{X|R}=\mqty(0&1)^\top$ is the true conditional expectation and $\vb*\eps=\vb X-\hat{\vb r}_{\cdot 1}$ here is the true residual.
Since \begin{align*}
    \E[\hat{\vb R}^\top(\vb X-\hat{\vb r}_{\cdot 1})]
    &=\E\qty[\E[\hat{\vb R}^\top(\vb X-\hat{\vb r}_{\cdot 1})\mid \vb G,\vb Z,\vb S]] \\
    &=\E\qty[\hat{\vb R}^\top\E[\vb X\mid \vb G,\vb Z,\vb S]-\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}] \\
    &=\E\qty[\hat{\vb R}^\top\E[\vb R\mid \vb G,\vb Z,\vb S]-\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}] \\
    &=\E[\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}-\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}] = 0,
\end{align*} the OLS estimator is unbiased for $\vb*\mu_{X|R}$, regardless of the specific distribution of $\hat{\vb R}$--i.e., *no matter* how well $\vb G$, $\vb Z$, and $\vb S$ predict $\vb R$!
(Subject, of course, to the identification rank condition.) In contrast, $\hat{\vb*\mu}^{(\text{wtd})}_{X|R}$ is highly dependent on the distribution of $\hat{\vb R}$.
Suppose in the extreme case that all but a few values of $\hat{\vb R}$ were equal to the population average $\E[R]$.
Then $\hat{\mu}^{(\text{wtd})}_{X|R}(r)\approx \E[R]$ for both $r=0$ and $r=1$, despite the perfect disparity.
The weighted estimator ignores the fact that the most high-information data points are those with $\hat{\vb r}_i$ close to 0 or 1; instead, it allows "low-quality" data points with poor racial predictions to overwhelm the estimation.

Despite the valuable intuition provided by the OLS estimator, it is not a good choice in general to estimate racial disparities.
Instead, we propose a Bayesian model, which we describe next.

# Model

## General model

From the DAG shown in Figure \@ref(fig:dag), we can write down the following general model.
\begin{align*}
    X_i \mid R_i, G_i, Z_i, \Theta &\sim \Categorical_\cX(\vb*\theta_{\cdot R_i}(G_i,Z_i)) \\
    (G_i, Z_i) \mid R_i &\sim \Categorical_{\cG\times\cZ}(\vb p_{GZ|R_i}) \\
    S_i \mid R_i &\sim \Categorical_\cS(\vb p_{S|R_i}) \\
    R_i &\sim \Categorical_\cR(\vb p_R) \\
    \Theta &\sim \pi(\Theta)
\end{align*} The parameter $\Theta$ can be viewed as a matrix of functions, with each entry $\theta_{xr}$ mapping values of $g$ and $z$ to a conditional probability $\Pr(X=x\mid R=r, G=g, Z=z)$.

Our primary quantity of interest is the marginalized parameter $$
    \bar\Theta = \sum_{g\in\cG,z\in\cZ} \Theta(g, z) p_{gz},
$$ the conditional distribution of $X$ given $R$.
Here $p_{gz}$ are the marginal probabilities of every $(g, z)$ combination in our population of interest.

The posterior is then

\begin{align*}
    \pi(\Theta, \vb R\mid \vb X, \vb G, \vb Z, \vb S)
    &\propto \pi(\Theta)\prod_{i=1}^N \pi(X_i\mid G_i, Z_i, R_i, \Theta)
            \pi(G_i, Z_i\mid R_i)\pi(S_i\mid R_i)\pi(R_i) \\
    &= \pi(\Theta) \prod_{i=1}^N \theta_{X_iR_i}(G_i, Z_i) p_{G_iZ_i\mid R_i} p_{S_i\mid R_i} p_{R_i} \\
    &\propto \pi(\Theta) \prod_{i=1}^N \theta_{X_iR_i}(G_i, Z_i)\hat{r_i}_{R_i},
\end{align*} where we let $$
    \hat{\vb r}_i\dfeq 
    \frac{\vb p_{G_iZ_i\mid R} \circ \vb p_{S_i\mid R} \circ \vb p_{R}}{
    \vb 1^\top(\vb p_{G_iZ_i\mid R} \circ \vb p_{S_i\mid R} \circ \vb p_{R})
    }
$$ denote the vector of BISGZ race probabilities, which depend on external data represented in $\vb p_{GZ\mid R}$, $\vb p_{S\mid R}$, and $\vb p_R$, but not on the parameters $\Theta$ and $\vb R$.

## Marginalized Posterior

Since $\vb R$ is high-dimensional, discrete, and not of primary interest, we marginalize it out to improve sampling: \begin{align*}
    \pi(X_i,G_i,Z_i,S_i\mid \Theta)
    &\propto \sum_{r\in\cR} \pi(X_i,R_i,G_i,Z_i,S_i\mid \Theta) \\
    &= \sum_{r\in\cR} \theta_{X_i r}(G_i, Z_i)\hat{r}_{ir} 
    = \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i.
\end{align*} So the marginalized posterior is \begin{align}
    \pi(\Theta\mid \vb X, \vb G, \vb Z, \vb S)
    &= \pi(\Theta) \prod_{i=1}^N \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i.
    \label{eq:post-marg}
\end{align}

## Prior on $\Theta$

A natural prior on $\Theta$ would be to allow for maximum flexibility with a *fully saturated* specification: \begin{align*}
    \Theta_{xr}(g, \vb z) &= \theta_{xrg\vb z},  \\
    \vb*\theta_{\cdot rgz}&\iid\Dirichlet(\alpha)
\end{align*} This prior fully captures the DAG in Figure \@ref(fig:dag) with no additional assumptions.
However, it suffers from the curse of dimensionality as more covariates are added to $Z$, especially since in practice $G$ will be quite large, covering many blocks or ZIP codes.
For example, for a binary outcome and nationwide data, if we use block-level race data, five racial categories, and just four binary covariates, the model has over 650 *million* parameters, which is nearly double the entire U.S. population.
With all of these parameters *a priori* independent under the fully saturated prior, there is no opportunity for sharing statistical strength; the prior will dominate the likelihood for individual parameters.
This problem could be partially remedied by adding some hierarchical structure to the priors on $\vb*\theta_{\cdot rgz}$, e.g., by grouping parameters at the county or state level.

We take a slightly different approach, motivated in part by the fact that we are primarily interested only in $\bar\Theta$, and by a belief that the interaction structure is primarily low-dimensional.
It is reasonable to assume that the relationship between $X$ and $R$ could vary widely by geographies, or within demographic subgroups.
But we do not believe that the magnitude of higher-order interactions is particularly large; while $X\mid R$ could be more extreme within some demographic subgroups, any further variation in this across small geographic units is unlikely to be substantial.
This leads us to the following *additive* prior specification, which can be thought about as a random intercept model: \begin{align*}
    \Theta_{xr}(g, \vb z) &= g^{-1}\qty(\log\theta_{xr} + \beta^{(0)}_{xrg} + 
        \sum_{i=1}^{|\cZ|} \beta^{(i)}_{xrz_i})\\
        \vb*\theta_{\cdot r} &\iid \Dirichlet(\alpha) \\
        \vb*\beta^{(i)}\mid\sigma^2_i &\iid \Norm(0, \sigma^2_i) \\
        \sigma^2_i &\iid \Cauchy^+(0, L)
\end{align*} The hierarchical priors on the $\vb*\beta^{(i)}$ allow the model to shrink towards zero covariates which are not predictive of a varying relationship between $X$ and $R$.
To the extent one wishes to model higher-order interactions, those can be added as additional random intercepts; the key efficiency gain is in separating the large number of geographic parameters $\beta^{(0)}_{xrg}$ from the other covariates.
We have found, on smaller datasets where fitting both the additive and fully saturated specifications is possible, that both give similar inferences for $\bar\Theta$, but the additive specification quickly becomes more accurate even for medium-sized problems.

# Error and Bias

The primary source of error in the model \eqref{eq:post-marg} comes from inaccurate input tables $\vb p_{GZ\mid R}$, $\vb p_{S\mid R}$, and $\vb p_R$.
We can consider the "true" values of these tables, which would yield "true" individual race probabilities $\vb r^*_i$.
The question then becomes one of quantifying how a particular error in these probabilities $\vb r^*_i - \hat{\vb r}_i$ translates into error in the posterior.

Denote by $\pi_{\vb*\delta}$ the posterior constructed using $\hat{\vb r}_i+\vb*\delta_i$ as the probabilities, so $\pi_{\vb*\delta^*}$ is the "true" posterior with $\vb*\delta_i^* \dfeq \vb r^*_i - \hat{\vb r}_i$.
Estimating how $\pi$ and $\pi_{\vb*\delta}$ differ in general is difficult, but we can gain insight by focusing on "small enough" $\vb*\delta$, such that a linear approximation is appropriate.

Defining perturbation weights $$
    w(\Theta,\vb*\delta^*) \dfeq 
    \prod_{i=1}^N 
    \qty(1 + \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i^*}{
    \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i})
    \propto \frac{\pi_{\vb*\delta^*}(\Theta\mid\vb X,\vb G,\vb Z,\vb S)}{\pi(\Theta\mid\vb X,\vb G,\vb Z,\vb S)},
$$ we can write the bias for a particular quantity of interest $g(\Theta)$ as \begin{align*}
    \E_{\pi_{\vb*\delta^*}}[g(\Theta)] - \E_\pi[g(\Theta)]
    &= \eval{\dv{\E_{\pi_{\vb*\delta}}[g(\Theta)]}{\vb*\delta}}_{\vb*\delta=0}^\top
        \vb*\delta^* + o(\norm{\vb*\delta^*}) \\
    &= \Cov_\pi\qty(g(\Theta), 
        \eval{\dv{\log w(\Theta,\vb*\delta)}{\vb*\delta}}_{\vb*\delta=0})^\top
        \vb*\delta^* + o(\norm{\vb*\delta^*}),
        \numberthis\label{eq:bias-dv}
\end{align*} where the second equality is Theorem 2.1 of @giordano2018cov (see also the idea of *local sensitivity* from @gustafson1996local).

The rest of this section will apply the representations like \eqref{eq:bias-dv} to provide bounds on the posterior error that may be used as part of a sensitivity analysis.
Throughout, we will let $\vec(\Theta)$ denote the vector of parameters $\theta_{xr}(g, z)$.

## Magnitude

\begin{theoremrep} \label{thm:bound}
Define $\widetilde{\Theta}_{ir}\dfeq 
\frac{\theta_{X_ir}(G_i,Z_i)}{\vb*\theta_{X_i}(G_i,Z_i)^\top\hat{\vb r}_i}$.
Then for any input probabilities with total error $\norm{\vb*\delta^*}^2
=\sum_{i=1}^n\norm{\vb*\delta_i^*}^2\le \Delta^2$,
\begin{align*}
    |\E_{\pi^*}[g(\Theta)] - \E_\pi[g(\Theta)]|
    &\lesssim \Delta\norm{\Cov_\pi(g(\Theta), \widetilde{\Theta})},
\end{align*}
as $\Delta\to 0$.
\end{theoremrep}
\begin{proof}
This is immediate from \eqref{eq:bias-dv} once we compute 
\begin{align*}
    \dv{\log w(\Theta,\vb*\delta)}{\delta_{ir}} 
    &= \dv{\delta_{ir}} \sum_{i=1}^N 
        \log(1+\frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}) \\
    &= \dv{\delta_{ir}}
        \log(1+\frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}) \\
    &= \frac{1}{1+\frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}} \times
        \frac{\theta_{X_ir}(G_i, Z_i)}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i} \\
    &= \frac{\theta_{X_ir}(G_i, Z_i)}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top(\hat{\vb r}_i+\vb*\delta_i)}
\end{align*}
and evaluate at $\vb*\delta=0$, since the worst-case bias for a fixed total error can be obtained by having the maximum allowable $\vb*\delta$ point in the direction of the gradient of $\log w(\Theta,\vb*\delta)$.
\end{proof}

The bound provided by Theorem \ref{thm:bound} may be computed readily from posterior draws, but it obscures the qualitative features of how the error depends on $\vb*\delta^*$, and in particular the sample size. 

## Sign

Beyond the linear approximation, we also can show the following for a linear $g(\Theta)=\vec(\Theta)^\top\vb b$.

\begin{toappendix}
For the proofs of the results about the direction of the bias, we can write the bias for a particular quantity of interest $g(\Theta)$ as
\begin{align*}
    \E_{\pi_{\vb*\delta^*}}[g(\Theta)] - \E_\pi[g(\Theta)]
    &= \frac{\E_\pi[w(\Theta,\vb*\delta^*)g(\Theta)]}{\E_\pi[w(\Theta,\vb*\delta^*)]} - 
        \E_\pi[g(\Theta)] \\
    &= \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta^*)}{\E_\pi[w(\Theta,\vb*\delta^*)]}, g(\Theta)),
\end{align*}
where $\vb Y=(\vb X,\vb G, \vb Z,\vb S)$ is the observed data.
This holds exactly, and is not just a linear approximations (notice the absence of a logarithm around the perturbation weights). 

\begin{lemma} \label{lem:biassign}
For a vector $\vb b$, if \[
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i} \ge 0
\] for all $1\le i\le N$ and $j\in\cR$, then \[
    \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]}, 
        \vec(\Theta)^\top\vb b\gvn\vb Y) \ge 0.
\] Conversely, if \(
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i} \le 0
\) for all $1\le i\le N$ and $j\in\cR$, then \[
    \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]}, 
        \vec(\Theta)^\top\vb b\gvn\vb Y) \le 0.
\]
\end{lemma}
\begin{proof}
We will leverage the covariance inequality, which states that for a monotonically increasing function $g$ and monotonically increasing (decreasing) functions $f$, and any random variable $X$, $\Cov(f(X),g(X))$ is nonnegative (nonpositive).
Since $w$ is nonnegative, we need only consider $\Cov_\pi\qty(w(\Theta,\vb*\delta), \vec(\Theta)^\top\vb b\mid\vb Y)$, without the normalization constant $\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]$.
In fact, since $w$ is a product of nonnegative terms, and the product of nonnegative monotonic functions is again monotonic, by the covariance inequality it suffices to show that each term \[
    w_i \dfeq 1 + \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i} 
\] is monotonic in $\vec(\Theta)^\top\vb b$.

Partial derivatives with respect to elements of $\vec(\Theta)$ are clearly zero except for those corresponding to $\vec(\Theta)_{X_i\cdot G_iZ_i}$. These are \[
    \pdv{w_i}{\vec(\Theta)_{X_irG_iZ_i}}
    = \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i\delta_{ir} -
        \vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i \hat r_{ir}}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}.
\] The directional derivative along $\vb b$ is then 
\begin{align*}
    \partial_{\vb b}w_i 
    &= \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i
                \vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} - 
                \vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i
                \hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}}{
            (\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i)^2} \\
    &= \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top}{
            (\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i)^2}
            (\hat{\vb r}_i\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} - 
            \vb*\delta_i\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}).
\end{align*}
Since $\vb*\theta_{X_i}(G_i, Z_i)$ is nonnegative, a sufficient condition for the directional derivative to be nonnegative, and thus for $w_i$ to be monotonically increasing in $\vec(\Theta)^\top\vb b$, is for
$\hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} - \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i} \ge 0$ for all $j\in\cR$. Similarly, by reversing this inequality, we have a sufficient condition for $w_i$ to be monotonically decreasing in $\vec(\Theta)^\top\vb b$.
\end{proof}
\end{toappendix}

\begin{theoremrep}
Let $r\in\cR$, $I\subseteq\{1,\dots,N\}$, and $\vb b$ a vector such that $\vb b_{X_i\cdot G_iZ_i}= c\vb e_r$ for all $i\in I$ and $0$ elsewhere, where $\vb e_r\in\R^{|\cR|}$ is the standard basis vector in the $r$ direction and $c>0$.
Then if for all $i\in I$, $\delta_{ir}>0$ and $\delta_{ij}<\delta_{ir}$ for $j\neq r$, \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb b\mid\vb Y] \ge \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y].
\]
\end{theoremrep}
\begin{proof}
Let $i\in I$. Then by assumption $\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i}=c\delta_{ir}>0$. 
Let $j\in\cR$. If $j=r$, \[
    \hat r_{ir}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ir}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ir}c\delta_{ir} - \delta_{ir}\hat r_{ir}c = 0,
\]
If $j\neq r$, we have \[
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ij}c\delta_{ir} - \delta_{ij}\hat r_{ij}c
    = c\hat r_{ij}(\delta_{ir} - \delta_{ij})\ge 0.
\]
So the condition of Lemma~\ref{lem:biassign} is satisfied for all $j\in\cR$, and
thus by the lemma we conclude that \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb b\mid\vb Y] - \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y]
    = \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)]}, 
    \vec(\Theta)^\top\vb b \gvn\vb Y) \ge 0. \qedhere
\]
\end{proof}

By reversing the sign of $\vb*\delta_i$, we can reverse the direction of the bias.
For the binary race case, we have the following translation of this result.

\begin{corollary}
Suppose that race is binary, i.e., $\cR=\{0,1\}$.
Then for any $x\in\cX$, the model-based estimate of $\Pr[X=x\mid R=1]$, denoted $\E_\pi[\bar\Theta_{x1}\mid\vb Y]$, 
will be biased downwards (upwards) if, for all $i$ with $X_i=x$, the BISGZ probability $\hat r_{i1}$ is biased downwards (upwards).
\end{corollary}

\begin{theoremrep}
Let $r,s\in\cR$, $I\subseteq\{1,\dots,N\}$, and $\vb b$ a vector such that $\vb b_{X_i\cdot G_iZ_i}= c_1(\vb e_r-\vb e_s)$ for all $i\in I$ and $0$ elsewhere, for some $c_1>0$.
Then if for all $i\in I$, $\vb*\delta_i=c_{2i}(e_r-e_s)$ for some $c_{2i}>0$, \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb b\mid\vb Y] \ge \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y].
\]
\end{theoremrep}
\begin{proof}
Let $i\in I$. Then by assumption $\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i}=c(|\delta_{ir}|+|\delta_{is}|)>0$. 
Let $j\in\cR$. If $j=r$ we have \[
    \hat r_{ir}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ir}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ir}c_1(2c_{2i}) - c_{2i}c(\hat r_{ir}-\hat r_{is}) 
    = c_1c_{2i}(\hat r_{ir} +\hat r_{is}) \ge 0.
\]
If $j=s$ we have \[
    \hat r_{is}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{is}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{is}c_1(2c_{2i}) + c_{2i}c(\hat r_{ir}-\hat r_{is}) 
    = c_1c_{2i}(\hat r_{is} +\hat r_{i+}) \ge 0.
\] If $j\not\in\{r,s\}$ then $\delta_{ij}=0$, so \[
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ij}c_1(2c_{2i}) \ge 0.
\] So the condition of Lemma~\ref{lem:biassign} is satisfied for all $j\in\cR$, and
thus by the lemma we conclude that \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb b\mid\vb Y] - \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y]
    = \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)]}, 
    \vec(\Theta)^\top\vb b \gvn\vb Y) \ge 0. \qedhere
\]
\end{proof}

As above, we can reverse the direction of the bias by reversing the sign of $\vb*\delta_i$.

\begin{corollary}
Suppose both the race and outcome variables are binary, i.e., $\cX=\cR=\{0,1\}$.
Suppose that race is binary, i.e., $\cR=\{0,1\}$.
Then for any $x\in\cX$, the model-based estimate of the disparity $\Pr[X=x\mid R=1]-\Pr[X=x\mid R=0]$, denoted $\E_\pi[\bar\Theta_{11}-\bar\Theta_{10}\mid\vb Y]$, 
will be biased downwards (upwards) if, for all $i$ with $X_i=x$, the BISGZ probability $\hat r_{i1}$ is biased downwards (upwards).
\end{corollary}

Certainly more results in this direction are possible, but these two suffice to understand how bias could arise in estimating a conditional probability and in estimating a disparity.

# Validation Study

- Intro NC data
- figure: estimated versus actual outcome rates by race and method, with error bars. Facet by several different outcome variables to show it's not just party
- table/text: TV distance on joint distribution by method

# Application

- Treasury data, no covariates

# Discussion

# (APPENDIX) Appendix {.unnumbered}
