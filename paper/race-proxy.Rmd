---
title: "Estimation of Racial Disparities when Race is Not Observed"
author: 
date: '`r format(Sys.Date(), "%B %e, %Y")`'
abstract: |
    Abstract here.
keywords: 
    - keyword 1
    - keyword 2
bibliography: references.bib
biblio-style: apalike
output:
    bookdown::pdf_document2:
        template: "template.tex"
        number_sections: true
        keep_tex: true
        includes: 
            in_header: "header.tex"
        latex_engine: pdflatex
editor_options: 
    markdown: 
        wrap: sentence
---

```{r setup, include=FALSE}
library(knitr)
library(here)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE,
               fig.path=here("paper/figures/"), fig.align="center",
               fig.width=(8.5-2*1)/2, out.width="100%", fig.asp=0.8)
```

# Introduction

# Problem and Identification

```{=tex}
\begin{figure}[h]
\begin{center}
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,thick]
    
        \node[main node] (G) {$G$};
        \node[main node] (R) [below of=G] {$R$};
        \node[main node] (S) [below left=1cm and 1cm of R] {$S$};
        \node[main node] (X) [right of=G] {$X$};
        \node[main node] (Z) [below of=X] {$Z$};
        \node[main node] (Y) [below right=0.5cm and 1cm of X] {$Y$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (Z)
        (R) edge node  {} (X)
        (R) edge node  {} (S)
        (G) edge [dashed] node  {} (S)
        (G) edge node  {} (Z)
        (G) edge node  {} (X)
        (Z) edge node  {} (X)
        (Z) edge node  {} (Y)
        (Z) edge [dashed] node  {} (S)
        (X) edge node  {} (Y);
    \end{tikzpicture}
\end{center}
\caption{DAG}
\label{fig:dag}
\end{figure}
```
-   $R_i\in \cR$: race
-   $X_i\in\cX$: outcome (may depend on race)
-   $Y_i\in\cY$: algorithmic outcome (does not depend on race)
-   $G_i\in\cG$: location of residence
-   $Z_i\in\cZ$: other covariates
-   $S_i\in\cS$: surname

The above DAG implies the following conditional independence relations:

-   $S\indep X\mid R$ (without dashed lines)
-   $S\indep X\mid R,G,Z$ (with dashed lines)
-   $Y\indep R\mid X, Z$

## Identification

\begin{theorem}
For all $g\in\cG$, $z\in\cZ$, and $x\in\cX$, define a matrix $\hat{\vb R}^{(xgz)}$ with entries $\hat r^{(xgz)}_{rs}=\Pr(R=r\mid G=g, Z=z, S=s)$ has rank $|\cR|$, and a vector $\vb b^{(xgz)}$ with entries $b^{(xgz)}_s=\Pr(X=x\mid G=g, Z=z, S=s)$.
Then the conditional racial disparities $\Pr(X=x\mid R, G=g, Z=z)$ are identified if and only if both $\hat{\vb R}^{(xgz)}$ and the augmented matrix $\mqty(\hat{\vb R}^{(xgz)}&\vb b^{(xgz)})$ have rank $|\cR|$.
\end{theorem}
\begin{proof}
Applying the law of total probability and our conditional independence relation $S\indep X\mid R,G,Z$, we have,
for all $x\in\cX$, $g\in\cG$, $z\in\cZ$, and $s\in\cS$,
\begin{align*}
    \Pr(X=x\mid G=g, Z=z, S=s)
    &= \sum_{r\in\cR} \Pr(X=x\mid R=r, G=g, Z=z, S=s)\Pr(R=r\mid G=g, Z=z, S=s) \\
    &= \sum_{r\in\cR} \Pr(X=x\mid R=r, G=g, Z=z)\Pr(R=r\mid G=g, Z=z, S=s).
\end{align*}
The left-hand side is estimable from the data and the rightmost term $\Pr(R=r\mid G=g, Z=z, S=s)$ is just the BISG+ probability.
So for each $x\in\cX$, $g\in\cG$, and $z\in\cZ$, this relation is a linear system in unknown parameters $\Pr(X=x\mid R=r, G=g, Z=z)$.
These parameters are identified if and only if this system has a unique solution, i.e. if the coefficient matrix $\hat{\vb R}^{(xgz)}$ has rank $|\cR|$ and so does the augmented matrix $\mqty(\hat{\vb R}^{(xgz)}&b^{(xgz)})$.
\end{proof}

## Comparison with Weighted Estimator
The above identification equation may bring to mind a least-squares estimator.
This is not a good idea for several reasons, not least of which is the fact that the unknown parameters are probabilities and thus constrained to lie in $[0,1]$ and sum to 1.
However, the ordinary least squares estimator provides useful intuition as to how existing methods fail, and why there is additional information "left on the table" to use in producing an accurate estimation of racial disparities.

Let $\hat{\vb r}_i=\Pr(R_i\mid G_i,Z_i,S_i)$, the BISGZ probabilities, for i.i.d. data $\{(X_i,G_i,Z_i,S_i,R_i)\}_{i=1}^N$, with $X_i$ binary and $R_i$ unobserved
Then the usual weighted estimator of $\E[X\mid R=r]$ is \[
    \hat\mu^{(\text{wtd})}_{X|R}(r) 
    = \frac{\hat{\vb r}_{\cdot r}^\top \vb X}{\hat{\vb r}_{\cdot r}^\top \vb 1}
    = \frac{\norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb X)}}{
        \norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb 1)}},
\] the ratio of the projected length of the outcome vector $\vb X$ and the constant vector $\vb 1$ onto $\hat{\vb r}_{\cdot r}$.
For the OLS estimator, we can rewrite the identification equation in this case on an observation level as \[
    \vb X = \hat{\vb R} \hat{\vb*\mu}^{(\text{OLS})}_{X|R} ,
\] where $\hat{\vb R}$ is the $n$-by-$r$ matrix with columns $\hat{\vb r}_{\cdot r}$. Then the estimator is \[
    \hat{\vb*\mu}^{(\text{OLS})}_{X|R} = (\hat{\vb R}^\top\hat{\vb R})^{-1}\hat{\vb R}^\top\vb X
    = \mathrm{coord}_{\hat{\vb R}}(\proj_{\hat{\vb R}}(\vb X)),
\] where $\mathrm{coord}_{\hat{\vb R}}$ is the function that returns the coordinates of its input vector in the $\hat{\vb R}$ basis (by assumption $\hat{\vb R}$ has rank $|\cR|$ and so its columns are linearly independent).
To make the comparison even easier, notice that we can break the projection $\proj_{\hat{\vb r}_{\cdot r}}$ into two steps, writing it instead as \(
    \proj_{\hat{\vb r}_{\cdot r}} = \proj_{\hat{\vb r}_{\cdot r}} \circ \proj_{\hat{\vb R}},
\) since $\hat{\vb r}$ is one of the columns of $\hat{\vb R}$. Letting $\vb X_{\proj}=\proj_{\hat{\vb R}}(X)$, then, we can rewrite our estimators as \[
    \hat\mu^{(\text{wtd})}_{X|R}(r) 
    = \frac{\norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb X_\proj)}}{
        \norm{\proj_{\hat{\vb r}_{\cdot r}}(\vb 1)}}  \qand
    \hat{\vb\mu}^{(\text{OLS})}_{X|R}(r)
    = \mathrm{coord}_{\hat{\vb R}}(\vb X_\proj)_r.
\]
From this representation, we can easily intuit that these two estimators should agree when the columns of $\hat{\vb R}$ are orthogonal, because then one may compute coordinate representations by projecting onto the basis vectors.
This is indeed the case, as the following result precisely states.

\begin{theorem}
$\hat{\vb*\mu}^{(\text{wtd})}_{X|R}=\hat{\vb*\mu}^{(\text{OLS})}_{X|R}$ if and only if
for every pair $j,k\in\cR$, either the BISGZ probabilities perfectly discriminate (i.e., $\Pr(R_i=j\mid G_i,Z_i,S_i)>0$ implies $\Pr(R_i=k\mid G_i,Z_i,S_i)=0$ and vice versa) or 
$\hat{\mu}^{(\text{wtd})}_{X|R}(j)=\hat{\mu}^{(\text{wtd})}_{X|R}(k)$ .
\end{theorem}
\begin{proof}
Since the individual BISGZ probabilities are nonnegative and sum to 1, a pair $j,k\in\cR$ of races has perfectly discriminating BISGZ probabilities if and only if the corresponding columns of $\hat{\vb R}$ are orthogonal, i.e., $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}$.
Begin by writing $\vb X_\proj$ in terms of the $\hat{\vb R}$ basis, so \[
    \vb X_\proj = \sum_{j\in\cR} c_j\hat{\vb r}_{\cdot j},
\] and thus $\hat{\vb\mu}^{(\text{OLS})}_{X|R}(j)=c_j$.
Without loss of generality, suppose the $c_j$ are numbered as $c_1\ge c_2\ge \cdots\ge c_{|\cR|}$.
We can also expand $\vb 1$ in the same basis. Since the individual probabilities must sum to one, in fact we have \(
    \vb 1 = \sum_{j\in\cR} \hat{\vb r}_{\cdot j}.
\)

For the forward direction, we assume $\hat{\mu}^{(\text{wtd})}_{X|R}(j)=\hat{\mu}^{(\text{OLS})}_{X|R}(j)=c_j$;
multiplying out the denominator of the weighted estimator, we have $\hat{\vb r}_{\cdot j}^\top \vb X=c_j\hat{\vb r}_{\cdot j}^\top\vb 1$ for all $j$; substituting the basis expansions of $\vb X_\proj$ and $\vb 1$, this yields \[
    \sum_{k\in\cR} c_k \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}
    = \sum_{k\in\cR} c_j \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}, \qq{so}
    \sum_{k\in\cR} (c_j-c_k) \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0.
\] Now fix $j\in J_1=\argmax_j c_j$; this relation still holds, but now every term in the sum is nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1$.
Therefore we must have $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$ for all $k\not\in J_1$.
Then fix $j\in J_2=\argmax_{j\not\in J_1} c_j$; since $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot l}=0$ for all $l\in J_1$, every term in the sum is still nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1\cup J_2$.
Therefore we must have $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$ for all $k\not\in J_1\cup J_2$.
Proceeding this way through all sets of common values in the $c_j$ we find that for all $j,k\in\cR$, either $c_j=c_k$ or  $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$.

For the reverse direction, fix $j\in\cR$ and let $J=\{k\in\cR:c_k=c_j\}$, so that by assumption $\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}=0$ for all $k\not\in J$.
Then by the above basis expansion, $\hat{\mu}^{(\text{OLS})}_{X|R}(j)=c_j$, and \[
    \hat{\mu}^{(\text{wtd})}_{X|R}(j) 
    = \frac{\sum_{k\in\cR} c_k \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}{
        \sum_{k\in\cR}\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}
    = \frac{c_j \sum_{k\in J} \hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}{
        \sum_{k\in J}\hat{\vb r}_{\cdot j}^\top\hat{\vb r}_{\cdot k}}
    = c_j = \hat{\mu}^{(\text{OLS})}_{X|R}(j). \qedhere
\]
\end{proof}

The theorem shows that if the BISGZ probabilities aren't perfect and the weighted estimator isn't constant across races, then OLS will give a different result.
But it doesn't provide much intuition as to why the estimators are different, and which is properly using all of the available information.
That can be illustrated well by the following stylized example, where for simplicity we consider a binary race and outcome.

Take the race vector $\vb R$ as fixed, and suppose that there is complete disparity, so $\vb X=\vb R$.
As we have so far, assume the BISGZ probabilities are well-calibrated, so $\hat{\vb R}=\Pr(\vb R\mid\vb G,\vb Z,\vb S)$.
Then the OLS estimator may be written as \[
    \hat{\vb*\mu}^{(\text{OLS})}_{X|R}
    = \vb*\mu_{X|R} + (\hat{\vb R}^\top\hat{\vb R})^{-1})\hat{\vb R}^\top\vb*\eps,
\] where $\vb*\mu_{X|R}=\mqty(0&1)^\top$ is the true conditional expectation and $\vb*\eps=\vb X-\hat{\vb r}_{\cdot 1}$ here is the true residual. Since
\begin{align*}
    \E[\hat{\vb R}^\top(\vb X-\hat{\vb r}_{\cdot 1})]
    &=\E\qty[\E[\hat{\vb R}^\top(\vb X-\hat{\vb r}_{\cdot 1})\mid \vb G,\vb Z,\vb S]] \\
    &=\E\qty[\hat{\vb R}^\top\E[\vb X\mid \vb G,\vb Z,\vb S]-\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}] \\
    &=\E\qty[\hat{\vb R}^\top\E[\vb R\mid \vb G,\vb Z,\vb S]-\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}] \\
    &=\E[\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}-\hat{\vb R}^\top\hat{\vb r}_{\cdot 1}] = 0,
\end{align*}
the OLS estimator is unbiased for $\vb*\mu_{X|R}$, regardless of the specific distribution of $\hat{\vb R}$--i.e., _no matter_ how well $\vb G$, $\vb Z$, and $\vb S$ predict $\vb R$! (Subject, of course, to the identification rank condition.)
In contrast, $\hat{\vb*\mu}^{(\text{wtd})}_{X|R}$ is highly dependent on the distribution of $\hat{\vb R}$.
Suppose in the extreme case that all but a few values of $\hat{\vb R}$ were equal to the population average $\E[R]$.
Then $\hat{\mu}^{(\text{wtd})}_{X|R}(r)\approx \E[R]$ for both $r=0$ and $r=1$, despite the perfect disparity.
The weighted estimator ignores the fact that the most high-information data points are those with $\hat{\vb r}_i$ close to 0 or 1; instead, it allows "low-quality" data points with poor racial predictions to overwhelm the estimation.

# Model

## General model

From the DAG shown in Figure \@ref(fig:dag), we can write down the following fully saturated model.
\begin{align*}
    X_i \mid R_i, G_i, Z_i, \Theta &\sim \Categorical_\cX(\vb*\theta_{\cdot R_i}(G_i,Z_i)) \\
    (G_i, Z_i) \mid R_i &\sim \Categorical_{\cG\times\cZ}(\vb p_{GZ|R_i}) \\
    S_i \mid R_i &\sim \Categorical_\cS(\vb p_{S|R_i}) \\
    R_i &\sim \Categorical_\cR(\vb p_R) \\
    \Theta &\sim \pi(\Theta)
\end{align*}
The parameter $\Theta$ can be viewed as a matrix of functions, with each entry $\theta_{xr}$ mapping values of $G$ and $Z$ to a conditional probability $\Pr(X=x\mid R=r, G=g, Z=z)$.

Our primary quantity of interest is the marginalized parameter $$
    \bar\Theta = \sum_{g\in\cG,z\in\cZ} \Theta(g, z) p_{gz},
$$ the conditional distribution of $X$ given $R$.
Here $p_{gz}$ are the marginal probabilities of every $(g, z)$ combination in our population of interest.

The posterior is then

\begin{align*}
    \pi(\Theta, \vb R\mid \vb X, \vb G, \vb Z, \vb S)
    &\propto \pi(\Theta)\prod_{i=1}^N \pi(X_i\mid G_i, Z_i, R_i, \Theta)
            \pi(G_i, Z_i\mid R_i)\pi(S_i\mid R_i)\pi(R_i) \\
    &= \pi(\Theta) \prod_{i=1}^N \theta_{X_iR_i}(G_i, Z_i) p_{G_iZ_i\mid R_i} p_{S_i\mid R_i} p_{R_i} \\
    &\propto \pi(\Theta) \prod_{i=1}^N \theta_{X_iR_i}(G_i, Z_i)\hat{r_i}_{R_i},
\end{align*} where we let \[
    \hat{\vb r}_i\dfeq 
    \frac{\vb p_{G_iZ_i\mid R} \circ \vb p_{S_i\mid R} \circ \vb p_{R}}{
    \vb 1^\top(\vb p_{G_iZ_i\mid R} \circ \vb p_{S_i\mid R} \circ \vb p_{R})
    }
\] denote the vector of BISGZ race probabilities, which depend on external data represented in $\vb p_{GZ\mid R}$, $\vb p_{S\mid R}$, and $\vb p_R$, but not on the parameters $\Theta$ and $\vb R$.

## Marginalized Posterior

Since $\vb R$ is high-dimensional, discrete, and not of primary interest, we marginalize it out to improve sampling:
\begin{align*}
    \pi(X_i,G_i,Z_i,S_i\mid \Theta)
    &\propto \sum_{r\in\cR} \pi(X_i,R_i,G_i,Z_i,S_i\mid \Theta) \\
    &= \sum_{r\in\cR} \theta_{X_i r}(G_i, Z_i)\hat{r}_{ir} 
    = \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i.
\end{align*}
So the marginalized posterior is
\begin{align}
    \pi(\Theta\mid \vb X, \vb G, \vb Z, \vb S)
    &= \pi(\Theta) \prod_{i=1}^N \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i.
    \label{eq:post-marg}
\end{align}

## Prior on $\Theta$

A natural prior on $\Theta$ would be to allow for maximum flexibility with a _fully saturated_ specification:
\begin{align*}
    \Theta_{xr}(g, \vb z) &= \theta_{xrg\vb z},  \\
    \vb*\theta_{\cdot rgz}&\iid\Dirichlet(\alpha)
\end{align*}
This prior fully captures the DAG in Figure \@ref(fig:dag) with no additional assumptions.
However, it suffers from the curse of dimensionality as more covariates are added to $Z$, especially since in practice $G$ will be quite large, covering many blocks or ZIP codes.
For example, for a binary outcome and nationwide data, if we use block-level race data, five racial categories, and just four binary covariates, the model has over 650 _million_ parameters, which is nearly double the entire U.S. population.
With all of these parameters _a priori_ independent under the fully saturated prior, there is no opportunity for sharing statistical strength; the prior will dominate the likelihood for individual parameters.
This problem could be partially remedied by adding some hierarchical structure to the priors on $\vb*\theta_{\cdot rgz}$, e.g., by grouping parameters at the county or state level.

We take a slightly different approach, motivated in part by the fact that we are primarily interested only in $\bar\Theta$, and by a belief that the interaction structure is primarily low-dimensional.
It is reasonable to assume that the relationship between $X$ and $R$ could vary widely by geographies, or within demographic subgroups.
But we do not believe that the magnitude of higher-order interactions is particularly large; while $X\mid R$ could be more extreme within some demographic subgroups, any further variation in this across small geographic units is unlikely to be substantial.
This leads us to the following _additive_ prior specification, which can be thought about as a random intercept model:
\begin{align*}
    \Theta_{xr}(g, \vb z) &= g^{-1}\qty(\log\theta_{xr} + \beta^{(0)}_{xrg} + 
        \sum_{i=1}^{|\cZ|} \beta^{(i)}_{xrz_i})\\
        \vb*\theta_{\cdot r} &\iid \Dirichlet(\alpha) \\
        \vb*\beta^{(i)}\mid\sigma^2_i &\iid \Norm(0, \sigma^2_i) \\
        \sigma^2_i &\iid \Cauchy^+(0, L)
\end{align*}
The hierarchical priors on the $\vb*\beta^{(i)}$ allow the model to shrink towards zero covariates which are not predictive of a varying relationship between $X$ and $R$.
To the extent one wishes to model higher-order interactions, those can be added as additional random intercepts;
the key efficiency gain is in separating the large number of geographic parameters $\beta^{(0)}_{xrg}$ from the other covariates.
We have found, on smaller datasets where fitting both the additive and fully saturated specifications is possible, that both give similar inferences for $\bar\Theta$, but the additive specification quickly becomes more accurate even for medium-sized problems.


# Error and Bias

The primary source of error in the model \eqref{eq:post-marg} comes from inaccurate input tables $\vb p_{GZ\mid R}$, $\vb p_{S\mid R}$, and $\vb p_R$.
We can consider the "true" values of these tables, which would yield "true" individual race probabilities $\vb r^*_i$.
The question then becomes one of quantifying how a particular error in these probabilities $\vb r^*_i - \hat{\vb r}_i$ translates into error in the posterior.

Denote by $\pi^*$ the posterior constructed using the $\vb r^*_i$, and let
$\vb*\delta_i^* \dfeq \vb r^*_i - \hat{\vb r}_i$. 
Notice that $\vb 1^\top\vb*\delta_i=0$ and all $\delta_{ij}\in[-1,1]$.
Defining perturbation weights \[
    w(\Theta,\vb*\delta^*) \dfeq 
    \prod_{i=1}^N 
    \qty(1 + \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i^*}{
    \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i})
    \propto \frac{\pi^*(\Theta\mid\vb X,\vb G,\vb Z,\vb S)}{\pi(\Theta\mid\vb X,\vb G,\vb Z,\vb S)},
\] we can write the bias for a particular quantity of interest $g(\Theta)$ as
\begin{align*}
    \E_{\pi^*}[g(\Theta)\mid\vb Y] - \E_\pi[g(\Theta)\mid\vb Y]
    &= \frac{\E_\pi[w(\Theta,\vb*\delta^*)g(\Theta)\mid\vb Y]}{\E_\pi[w(\Theta,\vb*\delta^*)\mid\vb Y]} - 
        \E_\pi[g(\Theta)\mid\vb Y] \\
    &= \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta^*)}{\E_\pi[w(\Theta,\vb*\delta^*)]}, g(\Theta)\gvn\vb Y),
\end{align*}
where $\vb Y=(\vb X,\vb G, \vb Z,\vb S)$ is the observed data.
To understand the effect of error, we will attempt to write the sign and magnitude of this covariance in terms of $\vb*\delta^*$.
Throughout, we will let $\vec(\Theta)$ denote the vector of parameters $\theta_{xr}(g, z)$.

## Sign
We have the following result for a linear $g(\Theta)=\vec(\Theta)^\top\vb b$.

\begin{lemma} \label{lem:biassign}
For a vector $\vb b$, if \[
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i} \ge 0
\] for all $1\le i\le N$ and $j\in\cR$, then \[
    \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]}, 
        \vec(\Theta)^\top\vb b\gvn\vb Y) \ge 0.
\] Conversely, if \(
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i} \le 0
\) for all $1\le i\le N$ and $j\in\cR$, then \[
    \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]}, 
        \vec(\Theta)^\top\vb b\gvn\vb Y) \le 0.
\]
\end{lemma}
\begin{proof}
We will leverage the covariance inequality, which states that for a monotonically increasing function $g$ and monotonically increasing (decreasing) functions $f$, and any random variable $X$, $\Cov(f(X),g(X))$ is nonnegative (nonpositive).
Since $w$ is nonnegative, we need only consider $\Cov_\pi\qty(w(\Theta,\vb*\delta), \vec(\Theta)^\top\vb b\mid\vb Y)$, without the normalization constant $\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]$.
In fact, since $w$ is a product of nonnegative terms, and the product of nonnegative monotonic functions is again monotonic, by the covariance inequality it suffices to show that each term \[
    w_i \dfeq 1 + \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i} 
\] is monotonic in $\vec(\Theta)^\top\vb b$.

Partial derivatives with respect to elements of $\vec(\Theta)$ are clearly zero except for those corresponding to $\vec(\Theta)_{X_i\cdot G_iZ_i}$. These are \[
    \pdv{w_i}{\vec(\Theta)_{X_irG_iZ_i}}
    = \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i\delta_{ir} -
        \vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i \hat r_{ir}}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}.
\] The directional derivative along $\vb b$ is then 
\begin{align*}
    \partial_{\vb b}w_i 
    &= \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i
                \vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} - 
                \vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i
                \hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}}{
            (\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i)^2} \\
    &= \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top}{
            (\vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i)^2}
            (\hat{\vb r}_i\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} - 
            \vb*\delta_i\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}).
\end{align*}
Since $\vb*\theta_{X_i}(G_i, Z_i)$ is nonnegative, a sufficient condition for the directional derivative to be nonnegative, and thus for $w_i$ to be monotonically increasing in $\vec(\Theta)^\top\vb b$, is for
$\hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} - \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i} \ge 0$ for all $j\in\cR$. Similarly, by reversing this inequality, we have a sufficient condition for $w_i$ to be monotonically decreasing in $\vec(\Theta)^\top\vb b$.
\end{proof}


\begin{theorem}
Let $r\in\cR$, $I\subseteq\{1,\dots,N\}$, and $\vb b$ a vector such that $\vb b_{X_i\cdot G_iZ_i}= c\vb e_r$ for all $i\in I$ and $0$ elsewhere, where $\vb e_r\in\R^{|\cR|}$ is the standard basis vector in the $r$ direction and $c>0$.
Then if for all $i\in I$, $\delta_{ir}>0$ and $\delta_{ij}<\delta_{ir}$ for $j\neq r$, \[
    \E_{\pi^*}[\vec(\Theta)^\top\vb b\mid\vb Y] \ge \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y].
\]
\end{theorem}
\begin{proof}
Let $i\in I$. Then by assumption $\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i}=c\delta_{ir}>0$. 
Let $j\in\cR$. If $j=r$, \[
    \hat r_{ir}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ir}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ir}c\delta_{ir} - \delta_{ir}\hat r_{ir}c = 0,
\]
If $j\neq r$, we have \[
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ij}c\delta_{ir} - \delta_{ij}\hat r_{ij}c
    = c\hat r_{ij}(\delta_{ir} - \delta_{ij})\ge 0.
\]
So the condition of Lemma~\ref{lem:biassign} is satisfied for all $j\in\cR$, and
thus by the lemma we conclude that \[
    \E_{\pi^*}[\vec(\Theta)^\top\vb b\mid\vb Y] - \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y]
    = \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)]}, 
    \vec(\Theta)^\top\vb b \gvn\vb Y) \ge 0. \qedhere
\]
\end{proof}

By reversing the sign of $\vb*\delta_i$, we can reverse the direction of the bias.
For the binary race case, we have the following translation of this result.

\begin{corollary}
Suppose that race is binarized, i.e., $\cR=\{0,1\}$.
Then for any $x\in\cX$, the model-based estimate of $\Pr[X=x\mid R=1]$, denoted $\E_\pi[\bar\Theta_{x1}\mid\vb Y]$, 
will be biased downwards (upwards) if, for all $i$ with $X_i=x$, the BISGZ probability $\hat r_{i1}$ is biased downwards (upwards).
\end{corollary}


\begin{theorem}
Let $r,s\in\cR$, $I\subseteq\{1,\dots,N\}$, and $\vb b$ a vector such that $\vb b_{X_i\cdot G_iZ_i}= c_1(\vb e_r-\vb e_s)$ for all $i\in I$ and $0$ elsewhere, for some $c_1>0$.
Then if for all $i\in I$, $\vb*\delta_i=c_{2i}(e_r-e_s)$ for some $c_{2i}>0$, \[
    \E_{\pi^*}[\vec(\Theta)^\top\vb b\mid\vb Y] \ge \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y].
\]
\end{theorem}
\begin{proof}
Let $i\in I$. Then by assumption $\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i}=c(|\delta_{ir}|+|\delta_{is}|)>0$. 
Let $j\in\cR$. If $j=r$ we have \[
    \hat r_{ir}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ir}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ir}c_1(2c_{2i}) - c_{2i}c(\hat r_{ir}-\hat r_{is}) 
    = c_1c_{2i}(\hat r_{ir} +\hat r_{is}) \ge 0.
\]
If $j=s$ we have \[
    \hat r_{is}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{is}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{is}c_1(2c_{2i}) + c_{2i}c(\hat r_{ir}-\hat r_{is}) 
    = c_1c_{2i}(\hat r_{is} +\hat r_{i+}) \ge 0.
\] If $j\not\in\{r,s\}$ then $\delta_{ij}=0$, so \[
    \hat r_{ij}\vb*\delta_i^\top\vb b_{X_i\cdot G_iZ_i} -
    \delta_{ij}\hat{\vb r}_i^\top\vb b_{X_i\cdot G_iZ_i}
    = \hat r_{ij}c_1(2c_{2i}) \ge 0.
\] So the condition of Lemma~\ref{lem:biassign} is satisfied for all $j\in\cR$, and
thus by the lemma we conclude that \[
    \E_{\pi^*}[\vec(\Theta)^\top\vb b\mid\vb Y] - \E_{\pi}[\vec(\Theta)^\top\vb b\mid\vb Y]
    = \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)]}, 
    \vec(\Theta)^\top\vb b \gvn\vb Y) \ge 0. \qedhere
\]
\end{proof}

As above, we can reverse the direction of the bias by reversing the sign of $\vb*\delta_i$.

\begin{corollary}
Suppose both the race and outcome variables are binary, i.e., $\cX=\cR=\{0,1\}$.
Then the model-based estimate of the disparity $\E[X\mid R=1]-\E[X\mid R=0]$, denoted $\E_\pi[\bar\Theta_{11}-\bar\Theta_{10}\mid\vb Y]$, 
will be biased downwards (upwards) if, for all $i$ with $X_i=1$, the BISGZ probability $\hat r_{i1}$ is biased downwards (upwards).
\end{corollary}

Certainly more results in this direction are possible, but these two suffice to understand how bias could arise in estimating a conditional probability and in estimating a disparity.


## Magnitude



## Old work

From Theorem 4 of @weiss1996approach, we can bound $\mathrm{RAE}(g)$ with the variance of these perturbation weights after normalization, which is also the $\chi^2$-divergence between $\pi^*$ and $\pi$: \[
    \mathrm{RAE}_{\pi^*}(g)^2  \le \chi^2(\pi^*) 
    = \frac{\E_\pi[w(\Theta,\vb*\delta^*)^2]}{\E_\pi[w(\Theta,\vb*\delta^*)]^2} - 1
\]

Since we don't know $\vb r^*_i$, we will focus on deriving a worst-case bound on $\chi^2(\pi^*)$ in terms of the average size of the $\vb*\delta_i^*$, measured as \[
    \Delta^* = \sqrt{\sum_{i=1}^N\sum_{j\in\cR} {\delta_{ij}^*}^2}.
\]

We'll first simplify the problem by moving the supremum to the inside: if $\Delta^*\le \eps$, then
\begin{align*}
    \chi^2(\pi^*) \le \sup_{\Delta=\eps}
        \frac{\E_\pi[w(\Theta,\vb*\delta)^2]}{\E_\pi[w(\Theta,\vb*\delta)]^2} - 1 
    \le \E_\pi\qty[\sup_{\Delta=\eps} \qty(\frac{w(\Theta,\vb*\delta)}{
        \E_\pi[w(\Theta,\vb*\delta)]})^2] - 1 \\
    = \E_\pi\qty[\exp \qty(2\cdot\sup_{\Delta=\eps}(
        \log w(\Theta,\vb*\delta) - \log\E_\pi[w(\Theta,\vb*\delta)]))] - 1.
        \numberthis\label{eq:sup-simp}
\end{align*}

We can leverage the inequality $\log(1+x)\le x$ to upper-bound the first term inside the supremum:
\begin{align*}
    \log w(\theta,\vb*\delta) 
    &= \sum_{i=1}^N \log(1 + \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}) \\
    &\le \sum_{i=1}^N \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}.
\end{align*}

For the second term, the opposite inequality $\frac{x}{1+x}\le \log(1+x)$ yields
\begin{align*}
    \log \E_\pi w(\Theta,\vb*\delta) &\ge \E_\pi \log w(\Theta,\vb*\delta) \\
    &= \sum_{i=1}^N \E_\pi \log(1 + \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i}) \\
    &\ge \sum_{i=1}^N \E_\pi\qty[
        \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i + 
            \vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}] \\
    &\le \sum_{i=1}^N \E_\pi\qty[
        \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \norm{\vb*\theta_{X_i}(G_i, Z_i)}\norm{\hat{\vb r}_i + \vb*\delta_i}}]
    \le \sum_{i=1}^N \E_\pi\qty[
        \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top \vb*\delta_i}{
        \norm{\vb*\theta_{X_i}(G_i, Z_i)}}].
\end{align*}

Combining these results,
\begin{align*}
    \log w(\theta,\vb*\delta) - \log \E_\pi w(\Theta,\vb*\delta) 
    &\le \sum_{i=1}^N \qty(
        \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i} -
        \E_\pi\qty[
        \frac{\vb*\theta_{X_i}(G_i, Z_i)^\top}{
            \norm{\vb*\theta_{X_i}(G_i, Z_i)}}])  \vb*\delta_i \\
    &\le \Delta \sqrt{\sum_{i=1}^N \norm{
        \frac{\vb*\theta_{X_i}(G_i, Z_i)}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i} -
        \E_\pi\qty[
        \frac{\vb*\theta_{X_i}(G_i, Z_i)}{
            \norm{\vb*\theta_{X_i}(G_i, Z_i)}}]}^2} \\
\end{align*}

Finally, substituting into \eqref{eq:sup-simp} and taking a square root, \[
    \mathrm{RAE}_{\pi^*}(g) \le  \sqrt{\E_\pi
    \exp\qty(\eps^2\sum_{i=1}^N \norm{
        \frac{\vb*\theta_{X_i}(G_i, Z_i)}{
        \vb*\theta_{X_i}(G_i, Z_i)^\top \hat{\vb r}_i} -
        \E_\pi\qty[
        \frac{\vb*\theta_{X_i}(G_i, Z_i)}{
            \norm{\vb*\theta_{X_i}(G_i, Z_i)}}]}^2) - 1}
\]


# References {-}
