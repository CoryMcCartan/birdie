---
title: "Estimating Racial Disparities When\\ Race\\ is\\ Not\\ Observed"
shorttitle: "Estimating Racial Disparities when Race is Not Observed"
authors: 
    - name: Cory McCartan
      department: Department of Statistics
      affiliation: Harvard University
    - name: Jacob Goldin
      department: Law School
      affiliation: University of Chicago 
    - name: Daniel E. Ho
      department: Law School and Department of Political Science
      affiliation: Stanford University
    - name: Kosuke Imai
      department: Department of Government and Department of Statistics
      affiliation: Harvard University
      thanks: "Corresponding author. Email: imai@harvard.edu, URL: https://imai.fas.harvard.edu.  We thank Bruce Willsie, CEO of L2, Inc., for providing us with the geocoded voter file we use in this paper.  We also thank Hiroto Katsumata, Soichiro Yamauchi, and an anonymous reviewer of the Alexander and Diviya Magaro Peer Pre-Review Program for useful feedback."
abstract: |
    The estimation of racial disparities in health care, financial services, voting, and other contexts is often hampered by the lack of individual-level racial information in administrative records.
    In many cases, the law prohibits the collection of such information to prevent direct racial discrimination.
    As a result, many analysts have adopted Bayesian Improved Surname Geocoding (BISG), which combines individual names and addresses with Census data to predict race.
    Although BISG tends to produce well-calibrated racial predictions, its residuals are often correlated with the outcomes of interest, yielding biased estimates of racial disparities, commonly in the direction of minimizing the disparity.
    We propose an alternative identification strategy that corrects this bias.
    The proposed strategy is applicable whenever one's surname is conditionally independent of the outcome given their (unobserved) race, residence location, and other observed characteristics.
    Leveraging this identification strategy, we introduce a new class of models, Bayesian Instrumental Regression for Disparity Estimation (BIRDiE), that estimate racial disparities by using surnames as a high-dimensional instrumental variable for race.
    Our estimation method is scalable, making it possible to analyze large-scale administrative data.
    We also show how to address potential violations of the key identification assumptions.
    A validation study based on the North Carolina voter file shows that BIRDiE reduces error by up to 84% in comparison to the standard approaches for estimating racial differences in party registration.
    Open-source software is available which implements the proposed methodology.
keywords: 
    - BISG
    - ecological inference
    - instrumental variable
    - proxy variable
    - race imputation
    - sensitivity analysis
bibliography: references.bib
biblio-style: apalike
output:
    bookdown::pdf_document2:
        template: "template.tex"
        number_sections: true
        keep_tex: true
        includes: 
            in_header: "header.tex"
        latex_engine: pdflatex
        citation_package: natbib
editor_options: 
    markdown: 
        wrap: sentence
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(here)
library(scales)
library(stringr)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE,
               fig.path=here("paper/figures/"), fig.align="center",
               fig.width=(8.5-2*1)/2, out.width="100%", fig.asp=0.8)
options(bookdown.theorem.preamble = FALSE)

x = readRDS(here("paper/data/nc_disp_ex.rds"))
bias_weight = with(x$weight, disp_wb - disp_wb_true)
bias_birdie = with(x$birdie_mmm, disp_wb - disp_wb_true)
bias_red = (bias_weight - bias_birdie) / bias_weight
```

\newpage 

# Introduction

The identification and estimation of racial disparities is of paramount importance to researchers, policymakers and organizations in a variety of areas including public health [@van2003paved; @williams2005social], employment [@conway1983reverse; @greene1984reverse], voting [@gay2001effect; @hajnal2005turnout; @barreto2007isi], criminal justice [@berk2021fairness; @chouldechova2017fair; @dressel2018accuracy], economic policy [@brown2022whiteness], taxation [@black2022algorithmic; @elzayn2023measuring], housing [@kermani2021racial], lending [@chen2018fair], and technology and fairness [@alao2021meta].
Within the U.S. government, efforts to identify and remedy racial disparities have taken on greater urgency with the recent issuance of Executive Order 13985, which in part directs agencies to conduct equity assessments by developing appropriate methodology.

In many of these areas, however, racial information is not available at the individual level.
The unavailability of individual racial information makes it impossible for analysts to simply tabulate variables of interest against race to identify disparities among different racial groups.
In fact, in some areas, the law explicitly prohibits the collection of racial information even as it demands fair treatment on the basis of race (see, e.g., the U.S. Equal Credit Opportunity Act).
This creates a dilemma for organizations who wish to measure possible disparities in order to monitor the fairness of their decision-making or service provision.

To estimate racial disparities without individual-level racial data, some researchers have turned to ecological inference methods [@goodman1953ecological; @king1997solution; @king2004ecological; @wakefield2004ecological; @imai2008bayesian; @greiner2009r].
These methods, however, require strong assumptions, which can be difficult to verify and may provide misleading results [@cho2008ecological].
Additionally, they all rely on accurate marginal information about race, which may not always be available.

Where the analysis of racial disparities involves large-scale administrative data, many analysts have adopted Bayesian Improved Surname Geocoding (BISG), which generates individual probabilities of belonging to different racial groups using Bayes’ rule applied to last names and geographic location [@fiscella2006bisg; @elliott2008bisg; @imai2016improving; @imai2022addressing].
BISG leverages residential racial segregation and the hereditary association between self-reported race and surname to produce generally accurate and calibrated predictions of self-reported individual race [@kenny2021das; @deluca2022validating].

Much attention has been given to ways of increasing the accuracy of BISG and related methods for race *prediction*.
Unfortunately, broadly accurate BISG racial prediction alone is not sufficient for the unbiased estimation of racial *disparities*, the primary goal of this paper.
To estimate disparities, BISG probabilities must be combined with information on the outcome variable for which the disparities are of interest.
But the most common techniques for doing so are known to be biased when race is correlated with the outcome even after controlling on name and location [@chen2019fairness; @argyle2022misclassification].
These approaches include *weighting* the outcome variable by the BISG probabilities, and *thresholding* the BISG probabilities to produce point estimates of individual race (e.g., imputing "Black" as the race for an individual with a 61% probability of being Black according to BISG).

In fact, these methods often *underestimate* the true magnitude of racial disparities, which is problematic for policymakers and analysts who aim to identify and reduce these disparities.
As formally discussed in Section \@ref(sec:id), the standard methods of racial disparity estimation based on BISG predictions implicitly require individuals' race to be conditionally independent of the outcome given their residence location, surnames, and other observable attributes.
This key identification assumption, however, is unlikely to hold because race affects many aspects of society even after accounting for residence location, surnames, and other observable attributes.
Other researchers have noted the implausibility of this assumption and have proposed methods to address it, but these alternative approaches are more general than the racial disparity setting and require additional data such as a partially labeled subset [@fong2021machine].

To address this challenge, in Section \@ref(sec:est), we propose an alternative identification strategy.
Specifically, we assume that the outcome is conditionally independent of surname given (unobserved) individual's race, residence location, and other observed attributes.
This assumption is a type of exclusion restriction where surname serves as an instrumental variable for unobserved race.
It implies that for two individuals who live in the same area, belong to the same racial group, and share the observable attributes, their surnames have no predictive power of the outcome.
Somewhat counter-intuitively, the high-dimensionality of surnames aids rather than hinders identification because it provides a large number of instruments.
We argue that this new identification assumption is more credible than the commonly invoked assumption unless surname is directly used to determine the outcome of interest (i.e., name-based discrimination).

Leveraging this identification strategy, we introduce a new class of models, Bayesian Instrumental Regression for Disparity Estimation (BIRDiE), that accurately estimates racial disparities.
This approach includes built-in flexibility for researchers to make problem-specific modeling choices.
We propose an EM algorithm for fitting BIRDiE models that can scale to hundreds of thousands or millions of observations.
BIRDiE also produces updated BISG probabilities that incorporate the outcome variable, and are likely to be more accurate than the BISG probabilities based only on surnames and geolocation.
Finally, we address potential violations of the key identification assumption, such as name-based discrimination, by exploiting auxiliary information about the relations between names and more specific ethnic groups.
All of the proposed methodology is implemented in an open-source software package, `birdie`, that is made available with the paper.^[The software and accompanying documentation are available at <https://corymccartan.com/birdie/>.]

<!--
In large datasets, sampling-based uncertainty is overwhelmed by error that arises from violations of our assumptions about the data.
This is particularly true in this context where the key variable of interest is unobserved.
Section \@ref(sec:sens) proposes a method to evaluate the sensitivity of estimates about racial disparities to the potential violation of model assumptions.
-->

In Section \@ref(sec:valid), we validate the proposed methodology using real-world data taken from the voter file in North Carolina, where self-reported individual-race is observed and can be used to construct the ground-truth of racial disparities.
BIRDiE substantially outperforms existing estimators across different error measures, two different outcome variables, and multiple levels of geolocation specificity.
For example, the most popular existing BISG disparity estimator pegs the gap at Democratic party registration between White and Black voters at `r round(-100*x$weight$disp_wb, 1)` percentage points, while the actual gap is `r round(-100*x$weight$disp_wb_true, 1)` percentage points---more than double.
Our preferred BIRDiE model yields an estimate of `r round(-100*x$birdie_mmm$disp_wb, 1)` percentage points.
This represents about a `r round(100*bias_red)`% reduction in bias.

```{=html}
<!--# In Section \@ref(sec:appl), we apply BIRDiE to large-scale administrative data from the U.S. Internal Revenue Service.
We produce the first-ever estimates of claim rates of the home mortgage interest deduction (HMID) by race…. [fill in]. **KI: I would like to introduce this application in Section 2 as a motivating example.  Then, we can come back to its analysis in Section \@ref(sec:appl).** -->
```
Section \@ref(sec:disc) gives concluding remarks.

# Bias of the Standard Methodology {#sec:id}

In this section, we review the assumptions of the standard BISG-based methodology for estimating racial disparities when individual race is not observed.
We show that the racial disparity estimates based on the standard methodology are biased unless the outcome variable is independent of race given surname, residence location, and other observed covariates.
We argue that this assumption is likely to be violated given the significant role race plays in many aspects of our society.

## Setup and BISG Procedure

Suppose that we have an i.i.d. sample of $N$ individuals from a population of infinite size.
For each individual $i=1,2,\ldots,N$, we define a tuple $(Y_i, R_i, G_i, X_i, S_i)$, where $Y_i\in\cY$ is the outcome of interest for individual $i$, $R_i\in \cR$ is the (unobserved) race of the individual, $G_i\in\cG$ is the (geo)location of the individual's residence, $X_i\in\cX$ are other observed characteristics, and $S_i\in\cS$ is the individual's surname.
When we are not referring to a particular individual, we will drop the subscripts for notational simplicity.
Note that individual race is unobservable but all other variables are assumed to be observed.
The availability of particular (or any) $X$ is not required for either the standard or proposed methodology.

We assume throughout that these variables are discrete, taking a finite set of values, i.e., $|\cY|=C_Y$, $|\cR|=C_R$, $|\cG|=C_G$, $|\cX|=C_X$, and $|\cS|=C_S$ where $C_Y$, $C_R$, $C_G$, $C_X$, and $C_S$ are constants.
Note that typically $S$ is high-dimensional as there exist a large number of unique surnames.
In practice, residence location $G$ is also discrete, since joint information about location, race, and other variables is generally only available down to the Census block level.
For the sake of simplicity, we assume that the outcome variable $Y$ is also discrete, though it is possible to extend the standard and proposed methodologies to continuous outcome variables.

Typically, BISG relies on data from the decennial Census or the American Community Survey (ACS), which provide information on the joint distribution of $R$ and $G$ (and any other covariates $X$, such as gender or age).
It then combines this information with data from the Census Bureau's surname tables [@censusnames], which provide information on the joint distribution of $R$ and $S$.
We summarize this set of information from the Census by two conditional probabilities, $\vb q_{GX|R}$ and $\vb q_{S|R}$, and one marginal probability, $\vb q_R$.

The BISG estimator of the probability that individual $i$ belongs to race $r \in \cR$ can then be written as [@fiscella2006bisg; @elliott2008bisg] \begin{equation} \label{eq:pr-bisg}
    \hat{P}_{ir} \dfeq \frac{q_{G_iX_i|r}\, q_{S_i|r}\, q_r}{
        \sum_{r^\prime\in\cR}q_{G_iX_i|r^\prime}\, q_{S_i|r^\prime}\, q_{r^\prime}},
\end{equation} where, for example, $q_{G_iX_i|r}$ indicates the estimated conditional probability of residence location $G_i$ and covariates $X_i$ given race $r$, taken from the Census table $\vb q_{GX|R}$.

The BISG estimator relies on two key assumptions.
The first is that the Census tables reflect the true population distributions of $R$, $S$, $G$, and $X$.

\begin{assump}[Data accuracy] \label{a:cens-acc}
    For all $i$, 
    \begin{align*}
        \Pr(R_i=r) &= q_r \\
        \Pr(S_i=s\mid R_i = r) &= q_{s|r} \\
        \Pr(G_i=g, X_i=x\mid R_i = r) &= q_{gx|r}
    \end{align*}
\end{assump}

Despite the best efforts of the Census Bureau, Assumption \ref{a:cens-acc} may never hold exactly in practice.
The decennial census has intrinsic error, including undercounting minority racial groups [@census2022undercount; @censuscount; @racecounts], as well as error introduced by privacy-preserving mechanisms [@abowd2020].
And because of births, deaths, and moves, census data are often out-of-date from the moment of publication.
These errors have led further extensions of the BISG estimator to account for some of this measurement error [@imai2022addressing], which can help with accuracy for smaller racial groups.
The plausibility of Assumption \ref{a:cens-acc} is stretched even further when the study population is a subset of the whole U.S. population, and so is not covered by national census data.
<!--# For example, in our motivating example, .... -->
In these cases, analysts should set $\vb q_R$ to the known or estimated marginal racial distribution in the study population, rather than the national racial distribution.
It may be more plausible then to assume that the conditional distributions $\Pr(S\mid R)$ and $\Pr(G,X\mid R)$ match the census distributions, even if $\Pr(R)$ does not.

The second assumption required by BISG is the following conditional independence relation between an individual's surname and residence location (as well as other characteristics) given their unobserved race.

\begin{assump}[Conditional independence of name and other proxy variables] \label{a:indep-sgx}
    For all $i$, $$S_i \indep \{G_i, X_i\}\mid R_i.$$
\end{assump}

Assumption \ref{a:indep-sgx} implies, for example, that once we know an individual is White, knowing their surname is Smith tells us nothing about their residence location and other observed characteristics.
Although this assumption appears to be reasonable, the lack of granularity in the coding of race may lead to its violation.
For example, people with Chinese, Indian, Filipino, Vietnamese, Korean, or Japanese are all coded as one racial group "Asian" in the census.
These groups, however, have varying sets of surnames and have different demographic and geographic distributions.
For instance, unlike the Smith example, knowing that an Asian individual's surname is Gupta makes it more likely that they have a higher income and live in the Eastern U.S [@budiman2019key].
<!--# Nevertheless, Assumption \ref{a:indep-sgx} appears to approximately hold in practice for White, Black, and Hispanic individuals, which make up a substantial majority of the U.S. population. -->
@greengard2023bisg relax this assumption by raking BISG probabilities with auxiliary data.

Even though Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc} may not hold exactly, researchers find that BISG produces accurate and generally well-calibrated estimates in practice [@imai2016improving; @kenny2021das; @deluca2022validating].
We observe this pattern as well in the validation study in Section \@ref(sec:valid).
New methods continue to be developed that improve the calibration of BISG probabilities, including some machine learning methods based on labeled data [@imai2022addressing; @argyle2022misclassification; @decter2022; @greengard2023bisg].

Under Assumption \ref{a:indep-sgx}, by Bayes' Rule, $$
    \Pr(R_i=r\mid G_i,X_i,S_i)
    \propto \Pr(G_i, X_i\mid R_i=r)\Pr(S_i\mid R_i=r)\Pr(R_i=r).
$$ This justifies the estimator given in Equation \eqref{eq:pr-bisg}, and provides us with the following immediate result.

\begin{prop}[Accuracy of BISG] \label{p:bisg}
Under Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc}, the BISG estimator produces correct probabilities. That is, we have
$\hat P_{ir} = \Pr(R_i=r\mid G_i,X_i,S_i)$.
\end{prop}

## Bias of Racial Disparity Estimates based on BISG Probabilities

To estimate racial disparities, BISG probabilities must be combined with the outcome variable.
There are several common ways researchers do this.

The most frequent is perhaps the *thresholding* or *classification* estimator, which deterministically assigns individuals to a predicted racial category based on the BISG estimates $\vb{\hat P}_i$ (either the largest $\hat P_{ir}$ or the one which exceeds a predetermined threshold).

Another common approach, which attempts to capture the uncertainty inherent in race prediction, is the following *weighting* estimator: \[
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid r) 
        = \frac{\sum_{i=1}^N \ind\{Y_i=y\}\hat P_{ir}}{
            \sum_{i=1}^N \hat P_{ir}}.
\]

Unfortunately, accurate and calibrated estimates of individual race predictions alone are not sufficient for unbiased estimation of racial disparities using these standard methodologies.^[Of course, if the predictions are perfectly accurate, then there is no bias.]

This should come as no surprise for the thresholding estimator, since it does not take into account prediction uncertainty in the BISG probabilities.
This is akin to ignoring measurement errors in the covariates of a regression, something which has long been known to lead to biased coefficient estimates.
Unlike the classical errors-in-variables setting, however, the bias of the thresholding estimator is not consistently in the same direction, making it hard to reason about [@chen2019fairness].

But the weighting estimator is biased as well.
The reason is that the prediction error of race probabilities may be correlated with the outcome variable of interest.
\citet{chen2019fairness} show that the asymptotic bias of this weighting estimator is controlled by the residual correlation of $Y$ and $R$ after adjusting for $G$, $X$, and $S$.
We reproduce this result here.

\begin{theorem}[Theorem 3.1 of \citealt{chen2019fairness}] \label{thm:wt-bias}
If race is binary (so $\cR=\{0,1\}$), then as $N\to\infty$, \[
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid r) - \Pr(Y=y\mid R=r)
    \cvas -\frac{\E[\Cov(\ind\{Y=y\}, \ind\{R=r\}\mid G,X,S)]}{\Pr(R=r)}.
\]
\end{theorem}

This result implies that when the BISG residuals $\ind\{R=r\}-\Pr(R=r\mid G,X,S)$ are correlated with the outcome, estimates will be biased, even with infinite data.
In fact, the weighting estimator will often underestimate the magnitude of a disparity, as the following corollary shows. Thus, for instance, in measuring disparities in loan approval ($Y$), if Blacks are less likely to be approved for loans across all locations and surnames than Whites, then the weighting estimator would understate the resulting overall White-Black disparity in loan approval rates.

\begin{restatable}{corollary}{corunder} \label{cor:under}
Let $y\in\cY$. If race is binary (so $\cR=\{0,1\}$), and 
$\Pr(Y=y\mid R=1, G=g, X=x, S=s)>\Pr(Y=y\mid R=0, G=g, X=x, S=s)$ for all $g\in\cG$, $x\in\cX$, and $s\in\cS$,
then \[
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid 1) - \hat\mu^{(\text{wtd})}_{Y|R}(y\mid 0)
    < \Pr(Y=y\mid R=1)-\Pr(Y=y\mid R=0).
\]
\end{restatable}

Conversely, Theorem \ref{thm:wt-bias} implies that conditional independence between individual's race and outcome given their surname, residence location, and other characteristics is sufficient to eliminate the asymptotic bias of the weighting estimator.

\begin{assump}[Conditional independence of outcome and race] \label{a:indep-yr}
For all $i$, $$Y_i\indep R_i\mid G_i,X_i,S_i.$$
\end{assump}

<!-- A similar assumption is made in the analysis of surrogate outcomes in causal inference, where the effect of treatment on the outcome is assumed to occur entirely through a set of surrogate variables [see, e.g., @athey2016estimating].-->
Figure \@ref(fig:dag)(a) shows a causal directed acyclic graph (DAG) that satisfies Assumption \ref{a:indep-yr} as well as Assumption \ref{a:indep-sgx}.
The dashed node border for $R$ represents the fact that race is unobserved.
The causal structure in Figure \@ref(fig:dag)(a) implies the conditional independence relation $Y\indep R\mid G,X,S$, because all paths from $R$ to $Y$ are blocked by $G$, $X$, or $S$.
The key causal assumption of this DAG is that the effect of race $R$ on the outcome $Y$ must be entirely mediated by surname $S$, residence location $G$, and other observed characteristics $X$.
This type of exclusion restriction may not be credible in many practical settings because race can affect the outcome through so many factors, biasing the weighting estimator.

```{=tex}
\begin{figure}[ht]
\begin{center}
\begin{subfigure}[b]{0.4\textwidth}
\centering
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.6cm,thick]
    
        \node[main node] (G) {$G$};
        \node[main node] (Y) [right of=G] {$Y$};
        \node[main node] (S) [below left=1.3cm and 1.3cm of Y] {$S$};
        \node[sub node] (R) [below left=0.8cm and 0.8cm of S] {$R$};
        \node[main node] (X) [below of=Y] {$X$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (X)
        (S) edge node  {} (Y)
        (R) edge node  {} (S)
        (G) edge [dash pattern=on 0.0pt off 20pt, bend left=15] node  {} (X)
        (X) edge [dashed, bend right=15] node  {} (G)
        (G) edge node  {} (Y)
        (X) edge node  {} (Y);
    \end{tikzpicture}
    \caption{Assumption \ref{a:indep-yr}: Conditional independence of $Y$ and $R$ given $(G, S, X)$}
\end{subfigure}
\hspace{1em}
\begin{subfigure}[b]{0.4\textwidth}
\centering
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,thick]
    
        \node[main node] (G) {$G$};
        \node[sub node] (R) [below of=G] {$R$};
        \node[main node] (S) [below left=0.5cm and 0.5cm of R] {$S$};
        \node[main node] (Y) [right of=G] {$Y$};
        \node[main node] (X) [below of=Y] {$X$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (X)
        (R) edge node  {} (Y)
        (R) edge node  {} (S)
        (G) edge [dash pattern=on 0.0pt off 20pt, bend left=15] node  {} (X)
        (X) edge [dashed, bend right=15] node  {} (G)
        (G) edge node  {} (Y)
        (X) edge node  {} (Y);
        
        %\path[every node/.style={font=\sffamily\small}, <->]
        %(G) edge [dashed, bend right] node  {} (S)
        %(X) edge [dashed, bend left] node  {} (S);
    \end{tikzpicture}
    \caption{Assumption \ref{a:indep-ys}: Conditional independence of $Y$ and $S$ given $(G, R, X)$}
\end{subfigure}
\end{center}

\caption[Causal DAGs comparing Assumptions \ref{a:indep-yr} and \ref{a:indep-ys}]{
Possible causal structures for which each of the labeled assumptions is satisfied, represented as a directed acyclic graph (DAG) where $G$ is residence location, $R$ is race, $S$ is surname, $X$ is observed covariates, and $Y$ is the outcome. Race $R$ is unobserved, which is signified by a dashed node boundary. Both DAGs also satisfy Assumption \ref{a:indep-sgx}: Conditional independence of $S$ and $(G,X)$ given $R$.}
\label{fig:dag}
\end{figure}
```

But in other settings, Assumption \ref{a:indep-yr} may be more plausible.
For example, for a manager reviewing job applications on paper, race is typically unobserved, but the manager may be influenced by racial or gender cues in a candidate's name or address [@park2009names; @aaslund2012names].
So long as we observe all information used by the manager and use it in the BISG estimation, the weighting estimator would give an asymptotically unbiased estimate.
Similarly, in evaluating the fairness of algorithmic decision-making, as long as all the information used by the algorithm is incorporated into BISG, Assumption \ref{a:indep-yr} would be appropriate.
Outside of these cases, however, the weighting estimator is likely to be biased.

Finally, while the discussion in this section has been focused on the BISG methodology, the qualitative results and necessary assumptions carry over to other approaches which produce probabilistic predictions of individual race, such as those recently developed by @argyle2022misclassification and @decter2022.
Just as with BISG, well-calibrated probabilities are not generally sufficient to produce unbiased estimates of racial disparities using the standard weighting or thresholding estimators.

# The Proposed Methodology {#sec:est}

In this section, we propose an alternative identification strategy for racial disparities.
Specifically, we show that racial disparity is identifiable if surname is conditionally independent of the outcome given race, residence geolocation, and other observed information, under the aforementioned assumptions that guarantee the nonparametric identification of race probabilities with BISG or its variants.
We then develop a class of statistical models, called Bayesian Instrumental Regression for Disparity Estimation (BIRDiE), that estimate racial disparity under this identification condition by using surnames as a high-dimensional instrumental variable for race.
These models take as inputs the BISG probabilities, and so can be easily applied on top of existing analysis pipielines, including with alternative probabilistic race prediction methodologies.
We next discuss computation for BIRDiE models, as well as how the methodology can be extended to include an additional explanatory variable that was not used at the BISG stage.
Finally, we show how to addresses the potential violations of the key identification assumptions, such as occurs with name-based discrimination.

## Identification Strategy {#subsec:ident}

To reduce the potential bias of the weighting estimator, we propose an alternative identification assumption that may be applicable when Assumption \ref{a:indep-yr} is not credible.
Specifically, we assume that surname, rather than race, satisfies the exclusion restriction conditional on (unobserved) race, residence location, and other observed characteristics.

\begin{assump}[Conditional independence of outcome and name] \label{a:indep-ys}
For all $i$, $$Y_i\indep S_i\mid R_i,G_i,X_i.$$
\end{assump}

Figure \@ref(fig:dag)(b) shows one possible causal DAG that meets this assumption as well as Assumption \ref{a:indep-sgx}.
In this DAG, race can have a direct effect on the outcome $Y$ as well as on residence location $G$ and other observed characteristics $X$, while all paths from $S$ to $Y$ are blocked by $G$, $X$, or $R$.

This causal structure is plausible in many real-world settings because, unlike Assumption \ref{a:indep-yr}, Assumption \ref{a:indep-ys} allows race to directly affect the outcome.
For an outcome like party registration, Assumption \ref{a:indep-ys} would mean that among White voters in a particular geographic region, voters named Smith would *a priori* be no more or less likely to identify with one party than voters named Thomas.
In contrast, Assumption \ref{a:indep-yr} would mean that among voters named Smith in a particular geographic region, White voters would be *a priori* be no more or less likely to identify with one party than Black voters.
In this case, Assumption \ref{a:indep-yr} is likely to be violated, while  Assumption \ref{a:indep-ys} is plausible.
<!--# This causal structure is plausible in many real-world settings, such as our motivating example of HMID claims.
In this case, we can rule out a direct effect of name on the outcome.
(**CM: revisit this more carefully once we've settled on outcomes. Dan notes e.g. mortgage application success may depend on discrimination via names**.) -->
It is important to note a key trade-off between the two assumptions.
While Assumption \ref{a:indep-ys} rules out the possibility that surname directly affects the outcome (e.g., name-based discrimination), such a direct effect is allowed under Assumption \ref{a:indep-yr}.
Section \ref{sec:sens} revisits this important issue.

The appropriateness of each assumption depends on a specific application.
In the above hiring example, if the manager reviews applicants anonymously, there will be no name-based discrimination and the assumption is likely to be satisfied.
The assumption may be violated in other contexts, however.
For example, in studying turnout, if campaigns use the surnames of individual voters to decide whether to mobilize them, Assumption \ref{a:indep-ys} will be violated.
Yet another possible violation of the assumption is the existence of unobserved confounder that affects both outcome and surname.
The country of origin for an immigrant may represent such a confounder: where surnames are informative of country of origin, even within racial groups (as is often the case for Asian individuals), variations in outcomes by country of origin will likely violate \ref{a:indep-ys}.
This reflects the limitations of the relatively coarse racial classifications used in BISG, as discussed above.
Even in these cases, however, conditioning on (unobserved) race is likely to substantially reduce the magnitude of association between outcome and surnames.
In Section \ref{sec:sens}, we show how to address this potential violation of Assumption \ref{a:indep-ys}.

We briefly note that Assumptions \ref{a:indep-yr} and \ref{a:indep-ys} are not necessarily mutually exclusive.
For example, neither race or surname could have a direct causal effect on the outcome.
In these cases, both the weighting estimator and our approach proposed below could give reasonable answers, though the data requirements of each may differ.

The following theorem shows that it is possible to nonparametrically identify racial disparities under Assumption \ref{a:indep-ys}.
The proof of the theorem and all other results in this paper are deferred to the appendix.

\begin{restatable}[Nonparametric Identification]{theorem}{thmid} \label{thm:id}
For any given $g\in\cG$, $x\in\cX$, and $y\in\cY$, define a matrix $\vb P$ with entries $p_{rs}=\Pr(R=r\mid G=g, X=x, S=s)$  and a vector $\vb b$ with entries $b_s=\Pr(Y=y\mid G=g, X=x, S=s)$.
Then under Assumption~\ref{a:indep-ys}, and assuming knowledge of the joint distribution $\Pr(R,G,X,S)$, the conditional probabilities $\Pr(Y=y\mid R, G=g, X=x)$ are identified if and only if both $\vb P$ and the augmented matrix $\mqty(\vb P&\vb b)$ have rank $|\cR|$.
\end{restatable}

The essence of this identification result is the following simple observation.
Under Assumption \ref{a:indep-ys}, we have, for all $y\in\cY$, $g\in\cG$, $x\in\cX$, and $s\in\cS$, \begin{equation} \label{eq:tprob} 
    \Pr(Y\,{=}\,y\mid G\,{=}\,g, X\,{=}\,x, S\,{=}\,s)
    = \sum_{r\in\cR} \Pr(Y\,{=}\,y\mid R\,{=}\,r, G\,{=}\,g, X\,{=}\,x)
    \Pr(R\,{=}\,r\mid G\,{=}\,g, X\,{=}\,x, S\,{=}\,s).
\end{equation} The leftmost term is estimable from the data and corresponds to the vector $\vb b$ in Theorem \ref{thm:id}, while the rightmost term is the BISG estimand and corresponds to the matrix $\vb P$ in Theorem \ref{thm:id}.
Lastly, the remaining term in the middle can be solved for, since Equation \eqref{eq:tprob} holds across all combinations of $Y$, $G$, $X$, and $S$, leading to a large system of linear equations. 
Specifically, we have $(|\cY|-1)\times|\cG|\times|\cX|\times|\cS|$ equations with $(|\cY|-1)\times|\cG|\times|\cX|\times|\cR|$ unknowns.
Since $|\cR| \ll |\cS|$, we can identify these unknowns as long as the linear system has sufficient rank.
Our result is closely related to causal identification based on proxy variables in the presence of unmeasured confounding [@kuroki2014measurement; @miao2018identifying; @knox2022proxy].
Here, we use surname as a proxy variable for (unobserved) race to identify racial disparities.

Together with Proposition \ref{p:bisg}, Theorem \ref{thm:id} implies that racial disparities can be identified under Assumptions \ref{a:indep-sgx}, \ref{a:cens-acc}, and \ref{a:indep-ys}.
The identifying equation \eqref{eq:tprob} shows that $\Pr(Y=y\mid G=g, X=x, S=s)$ is linear in the BISG estimands $\Pr(R=r\mid G=g, X=x, S=s)$.
Thus, it is natural to consider the following least-squares estimator of $\Pr(Y=y\mid R, G=g, X=x)$ under this alternative identification strategy, $$
    \hat{\vb*\mu}^{(\text{ols})}_{Y\mid RGX}(y\mid \cdot, g,x) 
    =  (\hat{\vb P}_{\cI(xg)}^\top \hat{\vb P}_{\cI(xg)})^{-1}\hat{\vb P}_{\cI(xg)}\,\ind\{{\vb Y}_{\cI(xg)} = y\},
$$ where as above $\hat{\vb P}$ is the matrix of BISG probabilities, and $\cI(xg)$ is the set of individuals $i$ with $X_i=x$ and $G_i=g$.
Here and throughout the paper, a dot will indicate a vector constructed over that index, so $\hat{\vb*\mu}^{(\text{ols})}_{Y\mid RGX}(y\mid \cdot, g,x)$ is a vector of conditional probabilities for a particular outcome level $y$ across all racial groups in $\cR$.
By post-stratifying this estimator across the $(G,X)$ cells, we arrive at an estimator of $\Pr(Y=y\mid R)$, $$
\hat{\vb*\mu}^{(\text{p-ols})}_{Y\mid R}(y\mid r) 
= \sum_{x\in\cX,g\in\cG} (\hat{\vb P}_{\cI(xg)}^\top \hat{\vb P}_{\cI(xg)})^{-1} 
\hat{\vb P}_{\cI(xg)}\,\ind\{{\vb Y}_{\cI(xg)} = y\})_r q_{gx|r} ,
$$ since $q_{gx|r}=\Pr(G=g,X=x\mid R=r)$ under Assumption \ref{a:cens-acc}.
<!-- and where $\vb Y_{gx}$ is the vector of outcome variables for the individuals with $G_i=g$ and $X_i=x$. -->
This estimator is unbiased, as the following theorem shows.

\begin{restatable}[Unbiasedness of OLS Estimator]{theorem}{thmolsunb} \label{thm:ols-unb}
If Assumptions \ref{a:indep-sgx}, \ref{a:cens-acc}, and \ref{a:indep-ys} hold, 
and the identification conditions in Theorem~\ref{thm:id} are satisfied,
then for all $y\in\cY$ and $r\in\cR$, \[
    \E[\hat{\mu}^{(\text{p-ols})}_{Y\mid R}(y\mid r)] = \Pr(Y=y\mid R=r).
\]
\end{restatable}

It is worth comparing this OLS estimator with the weighting estimator $\hat{\mu}^{(\text{wtd})}_{y\mid r}$.
The next theorem shows that within the $(G,X)$ cells these two estimators are guaranteed to disagree, unless either the BISG probabilities perfectly discriminate or the weighting estimator is constant across races.
Unfortunately, these two conditions are almost never met in practice.
This underscores the importance of selecting the appropriate assumption (Assumption \ref{a:indep-yr} or \ref{a:indep-ys}) for a particular analysis, since they imply different estimators with different results.

\begin{restatable}[Necessary and Sufficient Condition for Equality of the Weighting and OLS Estimators]{theorem}{thmwtdols} \label{thm:wtd-vs-ols}
For any $y\in\cY$, $g\in\cG$ and $x\in\cX$, within the set of individuals with $G_i=g$ and $X_i=x$, we have that $\hat{\vb*\mu}^{(\text{wtd})}_{Y|R}(y\mid\cdot) = \hat{\vb*\mu}^{(\text{ols})}_{Y|R}(y\mid\cdot)$
if and only if for every pair $j,k\in\cR$, either the BISG probabilities perfectly discriminate 
(i.e., $\Pr(R_i=j\mid G_i,X_i,S_i)>0$ implies $\Pr(R_i=k\mid G_i,X_i,S_i)=0$ and vice versa) or 
$\hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid j)=\hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid k)$.
\end{restatable}

Despite potential advantages over the weighting estimator, the OLS estimator is not well-suited to estimation in practice, since it ignores the fact that the unknown parameters are probabilities and thus constrained to be nonnegative and sum to 1.
As a result, in any particular sample, the estimator can produce impossible or contradictory estimates.
This is particularly problematic because a large number of unique surnames make both $\vb P$ and $\vb b$ high-dimensional.
To address this challenge, and to open the door to more flexible modeling, we next propose a Bayesian modeling approach that is based on our identification strategy and yet satisfies necessary constraints.

## Bayesian Instrumental Regression for Disparity Estimation {#sec:birdie}

The BIRDiE approach combines a user-specified complete-data outcome model $\pi(Y\mid R, G, X, \Theta)$, parametrized by $\Theta$, with the BISG model in order to estimate the distribution $Y\mid R$ that is of interest.
In this regard it mirrors the two-stage approach to instrumental variables estimation: a first stage (BISG) that estimates the relationship between instrument (surname) and variable of interest (race), and a second stage (BIRDiE) that uses the first-stage estimates to produce valid estimates of the quantity of interest.
However, unlike two-stage IV, the BIRDiE approach is not an approximation: as we describe below, the model is a coherent joint distribution of data, unknown parameters, and race.

We first present the general BIRDiE model and describe potential choices of complete-data outcome models before discussing our computational approach.
The BIRDiE posterior is obtained by applying Assumptions \ref{a:indep-sgx}, \ref{a:cens-acc}, and \ref{a:indep-ys} to the joint distribution $\pi(Y, R, G, X, S, \Theta)$: \begin{align}
    \pi(\Theta, \vb R\mid \vb Y, \vb G, \vb X, \vb S)
    &\propto \pi(\Theta, \vb R, \vb Y, \vb G, \vb X, \vb S) \nonumber \\
    &\propto \pi(\Theta)\prod_{i=1}^N \pi(Y_i\mid R_i, G_i, X_i, \Theta)
            \pi(R_i\mid G_i, X_i, S_i) \nonumber \\
    &= \pi(\Theta) \prod_{i=1}^N \pi(Y_i\mid R_i, G_i, X_i, \Theta) \hat{P_i}_{R_i}, \label{eq:posterior}
\end{align} where as above $\hat{\vb P}_i$ are the BISG probability estimates for individual $i$, which depend on Census data represented in $\vb q_{GX\mid R}$, $\vb q_{S\mid R}$, and $\vb q_R$, but not on the outcome-model parameters $\Theta$.
As a result these "first-stage" BISG estimates can be plugged directly into the BIRDiE posterior computation without any loss of Bayesian coherency.^[This remains true if a more complex model is used in place of the BISG probabilities [such as those of @imai2022addressing; @argyle2022misclassification; and @decter2022], so long as the parameters of the first-stage model are *a priori* independent of the parameters of the BIRDiE model.]

To apply this general BIRDiE model to a particular analysis requires choosing a complete-data outcome model, given by the likelihood $\pi(Y_i\mid R_i, G_i, X_i, \Theta)$ and prior $\pi(\Theta)$.
Since $Y$ is discrete, a categorical regression model is appropriate for $\pi(Y_i\mid R_i, G_i, X_i, \Theta)$.
The parametrization of the categorical regression will depend on the analyst's goals, computational resources, and prior beliefs about the structure of the problem.
We present here several reasonable alternatives that trade off modeling flexibility and computational efficiency.

<!--# The level-5 header is equivalent to \paragraph{} -->

##### Complete-pooling model.

The simplest possible model is one in which the relationship between $Y$ and $R$ does not vary with $G$ or $X$.
This model is parametrized by $\Theta=\{\vb*\theta_r\}_{r\in\cR}$, which describe the distribution of $Y$ within every level of $R$: \begin{align*}
        Y_i\mid R_i, G_i, X_i, \Theta &\sim \Categorical_{\cY}(\vb*\theta_{R_i}) \\
        \vb*\theta_r &\iid \Dirichlet(\vb*\alpha),
\end{align*}
where $\Categorical_{\cY}$ denotes a discrete (categorical) distribution on the set $\cY$.
With known $\vb R$, the posterior of $\vb*\theta_r$ is conjugate, a fact which will make computation under the EM scheme described in Section \@ref(sec:compute) below extremely efficient.
Of course, this efficiency comes at the cost of a restrictive model that allows for no role of $G$ and $X$.
If in reality $\Pr(Y\mid R)$ does vary along these dimensions, it is possible that the posterior of $\vb*\theta_r$ will not accurately estimate $\Pr(Y\mid R)$.
In any case, if the analyst is interested in subgroup or small-area estimates of $\Pr(Y\mid R)$, the complete-pooling model will be of little use.

##### Saturated (no-pooling) model.

At the other end of the spectrum from the complete-pooling model is a *saturated* or no-pooling model, which estimates a different distribution of $Y\mid R$ within every level of $G$ and $X$:
\begin{align*}
        Y_i\mid R_i, G_i, X_i, \Theta &\sim \Categorical_{\cY}(\vb*\theta_{R_iG_iX_i}) \\
        \vb*\theta_{rgx} &\iid \Dirichlet(\vb*\alpha).
\end{align*}
This model is closest to the OLS estimator, though it ensures that all probability estimates lie in $[0, 1]$.
As with the complete-pooling model, the posterior of $\vb*\theta_{rgx}$ is conjugate to its prior, and so computation can be made efficient.
Additionally, this model allows for any arbitrary relationship between $Y$, $R$, $G$, and $X$.
Since it is fully nonparametric, the posterior will converge to the true $\Pr(Y\mid R, G, X)$ with enough data in each $(G, X)$ cell.
However, in practice, the model can suffer from the curse of dimensionality: the number of $(G, X)$ cells may be relatively large compared to the amount of available data, or even exceed it, especially since $G$ can be quite large, covering many blocks or ZIP codes.
In these cases, the prior will dominate the data in each cell, which could have a large biasing effect even on overall inferences about $\Pr(Y\mid R)$.

##### General mixed-effects model.

As a compromise between the complete-pooling and no-pooling model, a partial pooling approach based on a multinomial mixed-effects model can be used.
Properly specified, the mixed-effects model maintains the flexibility of the saturated model while avoiding its high bias and variance in finite samples.
\begin{align*}
    Y_i\mid R_i, G_i, X_i, \Theta &\sim \Categorical_{\cY}(g^{-1}(\vb*\mu_{rgx})) \\
    \mu_{rgxy} &= \vb W\vb*\beta_{ry} + \vb Z\vb u_{ry} \\
    \quad \vb u_{ry}\mid \phi_{ry} &\sim \Norm(0, \Sigma(\vb*\phi_{ry})) \\
    \vb*\beta_{ry} &\iid f_\beta, \quad \vb*\phi_{ry} \iid f_\phi ,
\end{align*}
where $g^{-1}$ is a softmax or other link function, $\vb W$ and $\vb Z$ are matrices of fixed and random effects, respectively, $\vb*\phi$ is a vector of random-effect parameters, and $f_\beta$ and $f_\phi$ are some priors for the subscripted parameters.
Some fixed or random effects could be shared across combinations of $R$ and $Y$, though this could complicate computation.
We recommend including $X$ and especially $G$ in the model as random effects, with hierarchical structure as appropriate.
Such a structure partially pools estimates of $\Pr(Y\mid R, X, G)$ towards an overall
estimate of $\Pr(Y\mid R)$, allowing the model to share information between geographic areas.
This should prove especially useful in cases where some areas have few or no observations for certain racial groups.

We also recommend including group-level covariates as fixed effects, which will help share information across random effects and significantly improve generalization performance to unseen random effect levels [@buttice2013does].
For example, if $G$ records counties, analysts could include racial and socioeconomic variables measured at the county level as predictors.
This would help produce more accurate estimates of $\Pr(Y\mid R, G)$ to the extent that variation in these probabilities is associated with these racial and socioeconomic variables.
Ultimately the structure of this general model will have to be chosen based on the data and the relevant research question.

## Computation {#sec:compute}

The posterior in Equation \eqref{eq:posterior} contains the high-dimensional discrete nuisance parameter $\vb R$, which poses a challenge for computation.
We suggest two approaches for handling $\vb R$, one suited to small sample sizes, and one suited to large sample sizes.

##### Small samples: Inference directly on the marginal likelihood.

Since $\vb R$ is discrete, we can marginalize it out as follows: $$
    \pi(\Theta\mid \vb Y, \vb G, \vb X, \vb S)
    = \sum_{\vb r\in \cR^N} \pi(\Theta, \vb r\mid \vb Y, \vb G, \vb X, \vb S)
    \propto \pi(\Theta) \prod_{i=1}^N \sum_{r\in\cR} \pi(Y_i\mid r, G_i, X_i, \Theta)\hat{P}_{ir}.
    \numberthis \label{eq:post-marg}
$$ This decouples the total number of parameters from the sample size.
Additionally, Equation \eqref{eq:post-marg} has only continuous parameters, and so can be used with any general Bayesian inference procedure such as Markov chain Monte Carlo (MCMC).
However, the moderate-to-high dimensionality in practical settings, even after integrating out $\vb R$, makes MCMC algorithms computationally too expensive outside of relatively small datasets.

##### Large samples: Expectation-Maximization.

When the number of individuals exceeds a thousand or so, we propose an Expectation-Maximization (EM) algorithm [@dempster1977] to calculate the maximum *a posteriori* (MAP) estimate of $\Theta$ for BIRDiE models.
The EM algorithm alternates between an E-step which calculates the expected log posterior density $Q$, averaging over the missing $\vb R$, and an M-step which maximizes $Q$ over values of $\Theta$.

Specifically, given a current parameter estimate $\Theta^{(t)}$, the expected log posterior density can be written as \begin{align*}
    Q(\Theta^{(t+1)}\mid \Theta^{(t)})
    &= \E_t[\log \pi(\Theta^{(t+1)}, \vb R \mid \vb Y, \vb G, \vb X, \vb S)] \\
    &= \sum_{\vb r\in\cR^N} 
        \pi(\vb r \mid \Theta^{(t)}, \vb Y, \vb G, \vb X, \vb S)
        \log \pi(\Theta^{(t+1)}, \vb r \mid \vb Y, \vb G, \vb X, \vb S) \\
    &= \log \pi(\Theta^{(t+1)}) + \sum_{i=1}^N\sum_{r\in\cR} \Big\{
        \qty(\log \pi(Y_i\mid r, G_i,, X_i, \Theta^{(t+1)}) + \log\hat P_{ir}) \\
        &\qquad\qquad\qquad\qquad\qquad\qquad\times 
        \pi(R_i=r \mid \Theta^{(t)}, Y_i, G_i, X_i, S_i) \Big\} \\
    &= C + \log \pi(\Theta^{(t+1)}) + \sum_{i=1}^N\sum_{r\in\cR} 
        \tilde{P}_{ir\mid Y}^{(t)} \log \pi(Y_i\mid r, G_i, X_i, \Theta^{(t+1)}),
\end{align*}
where $C$ is a constant that can be ignored as it does not depend on
$\Theta^{(t+1)}$, and 
$\tilde{\vb P}_{\mid Y}^{(t)}=\pi(\vb R \mid \Theta^{(t)}, \vb Y, \vb G, \vb X, \vb S)$
are the BISG probabilities updated using Bayes' rule: 
\begin{equation}
    \tilde{P}_{ir\mid Y}^{(t)}
    = \frac{\pi(Y_i\mid r, G_i,, X_i, \Theta^{(t)}) \hat P_{ir}}{
        \sum_{r'\in\cR} \pi(Y_i\mid r', G_i,, X_i, \Theta^{(t)}) \hat P_{ir'} }. \label{eq:update-bisg}
\end{equation}
At the M-step, $Q(\Theta^{(t+1)}\mid \Theta^{(t)})$ is straightforward to maximize, since it is just the log complete-data posterior, with likelihood weights given by the $\tilde{P}_{ir\mid Y}^{(t)}$.
Additionally, if $\Theta$ can be partitioned into parameters which only affect individuals in each racial group (as is the case with all the models described in Section \ref{sec:birdie} above), then the maximization can be performed separately on each group of individuals.

A critical advantage of this EM scheme over working directly with the marginal likelihood is that the maximization in the M-step can be performed using sufficient statistics calculated as part of the E-step, rather than on all of the individual entries in the data.
Since the M-step is usually the practical (if not also asymptotic) bottleneck in the computation, this is enormously helpful---the problem size scales with $|\cY|\times|\cX|\times|\cG|$ rather than with $N$.
Specifically, notice that we can rewrite $Q(\Theta^{(t+1)}\mid \Theta^{(t)})$ (dropping the unnecessary constant) as \begin{align*}
    Q(\Theta^{(t+1)}\mid \Theta^{(t)})
    &= \log \pi(\Theta^{(t+1)}) + \sum_{i=1}^N\sum_{r\in\cR} 
        \tilde{P}_{ir\mid Y}^{(t)} \log \pi(Y_i\mid r, G_i, X_i, \Theta^{(t+1)}) \\
    &= \log \pi(\Theta^{(t+1)}) + \sum_{r\in\cR}\sum_{y\in\cY}\sum_{x\in\cX}\sum_{g\in\cG} 
        \log \pi(y\mid r, g, x, \Theta^{(t+1)}) 
        \qty(\sum_{i\in\cI(yxg)} \tilde{P}_{ir\mid Y}^{(t)}),
\end{align*}
where analogously to above $\cI(yxg)$ is the set of individuals with $Y_i=y$, $X_i=x$, and $G_i=g$.

For BIRDiE models where the complete-data likelihood is conjugate to the prior, such as the complete- and no-pooling models, these sufficient statistics are used in the M-step anyway, and can be efficiently calculated during the E-step as well.
In combination with the acceleration scheme described next, this allows the entire EM algorithm to be run to convergence on data with hundreds of thousands or millions of individuals in a matter of seconds.

While EM algorithms are highly stable, due to their monotonic increasing of the marginal likelihood, they are also often slow to converge [@laird1993].
To address this, we propose, and include in our open-source software implementation, the use of fixed-point iteration accelerators such as Anderson acceleration or SQUAREM [@varadhan2008simple].
These techniques can substantially reduce the overall computational time without meaningfully affecting the stability of inference.

Finally, it is important to note that the EM algorithm does not provide any uncertainty quantification.
However, in large samples, sampling and model-based uncertainty is dominated by biases caused by even small violations of the underlying assumptions, a problem we discuss below in Section \ref{sec:sens} and the accompanying appendix.
For BIRDiE models with few parameters, such as the complete-pooling model, bootstrapping is computationally feasible and can be used to approximate the asymptotic covariance matrix of the MAP estimate.

## Updated Individual Race Probabilities {#sec:update-bisg}

The EM algorithm produces the updated individual race probabilities given in Equation \eqref{eq:update-bisg}.
One feature of these updated probabilities is that it is appropriate to apply the weighting estimator to them to estimate disparities.
This is because the updated probabilities condition on $\vb Y$, and so the asymptotic bias term in Theorem \ref{thm:wt-bias} becomes zero.
In fact, the weighting estimate from the updated probabilities is identical to the BIRDiE estimate.
While more study is required, for downstream settings where weights are needed, generating these weights with BISG followed by BIRDiE will likely produce more accurate results than simply using BISG weights alone.


## Additional Explanatory Variables {#sec:addlcov}

Often, researchers are interested in not just $\Pr(Y\mid R)$ but also $\Pr(Y\mid W, R)$, for some variable $W\in\cW$ which is not part of the BISG predictors $(X, G)$.
For example, a lending firm auditing potential racial disparities in lending decisions would likely be interested both in how the rate of loan approval ($Y$) varies by race, but also how loan approval varies by race, conditional on a measure of creditworthiness ($W$).
The unconditional disparities reflect realities of systemic racism and inequity, while the conditional disparities measure the fairness of the firm's lending decisions after controlling for these systemic factors.
Such estimates could be used to compute various measures of algorithmic fairness, including calibration parity and false positive error rate balance.
Another scenario is a policy evaluation study, where researchers are interested in how the impact of policy varies across racial groups.
Such an anlysis requires incorporating an interaction between race and the treatment variable.

There are two main ways to perform such an analysis with our proposed methodology.
The first, and perhaps simplest, is to apply the methodology to the combined variable $\underline{YW}\in\cY\times\cW$.
This will produce estimates of $\Pr(Y, W\mid R)$, from which $\Pr(Y\mid W, R)$ can be straightforwardly calculated by appropriate normalization.
This approach will work well if $|\cY|$ and $|\cW|$ are both small, so that $|\cY\times\cW|$ is of manageable size.
If one of these variables has many levels, however, directly estimating the distribution of $Y, W\mid R$ could be less efficient, as it does not account for information about the marginal distributions $Y\mid R$ and $W\mid R$.

An alternative approach is to first apply the proposed methodology to estimate $\Pr(W\mid R)$.
This allows for calculation of model-updated BISG probabilities $\tilde{\vb P}_{\mid W}=\pi(\vb R \mid \hat\Theta, \vb W, \vb G, \vb X, \vb S)$, which are also computed as a byproduct of the EM algorithm described above.
Then, the methodology can be applied again, using $\tilde{\vb P}_{\mid W}$ as the input probabilities rather than the original BISG probabilities, to estimate $\Pr(Y\mid W, R)$.
This approach will likely perform better when $W$ consists of multiple predictors or if either $|\cY|$ or $|\cW|$ are large.

Both of these approaches require the following identifying assumption, which generalizes Assumption \ref{a:indep-ys}.

\begin{assump}[Conditional independence of outcome, predictor and name] \label{a:indep-yws}
For all $i$, $(Y_i, W_i)\indep S_i\mid R_i,G_i,X_i$, or, equivalently, \[
    W_i\indep S_i\mid R_i,G_i,X_i \qand
    Y_i\indep S_i\mid W_i,R_i,G_i,X_i. 
\]
\end{assump}

In the lending example, this would translate to the assumption that a measure of creditworthiness is independent of last name after controlling for race, location, and covariates, and that lending decisions are independent of last names after controlling for creditworthiness, race, location, and covariates.

## Addressing the Potential Violations of the Assumptions {#sec:sens}

BIRDiE crucially relies on Assumption \ref{a:indep-ys} for identification. 
In addition, like the weighting and thresholding estimators, it also relies upon Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc}, which are required for the BISG race probabilities to be accurate.
Unfortunately, these assumptions may not exactly hold in practice, and are also not testable in observed data.
In this section and Appendix \ref{app:sens}, we develop sensitivity analyses that assess how violations of these assumptions affect the estimates of racial disparities.

First, BIRDiE assumes that conditional on unobserved race and observed covariates, outcomes and surnames are independent.
As discussed in Section \ref{subsec:ident}, however, association between the outcome and country of origin or racial subgroups may lead to correlation between surnames and outcome even after controlling for race and geography.
To address this problem, suppose that a low-dimensional summary statistic of surname, $f:\cS\to\R^d$, $d\ll|\cS|$, is available, where $f$ may map each surname to a finer ethnic group within each racial category.
For example, Imai is a Japanese name whereas McCartan is a name of Irish origin.
If $f$ can classify surnames into finer racial subgroups or countries of origin---even approximately---then it can be used to control for this channel of possible violations of Assumption \ref{a:indep-ys}.
Formally, we relax Assumption \ref{a:indep-ys} as follows.

\begin{assump}[Partial conditional independence of outcome and name] \label{a:indep-ysf}
For all $i$, $$Y_i\indep S_i\mid f(S_i),R_i,G_i,X_i.$$
\end{assump}

<!---
Figure \ref{fig:dag2} compares possible causal DAGs that satisfy Assumption \ref{a:indep-ys} and \ref{a:indep-ysf}.
In contrast to Assumption \ref{a:indep-ys}, Assumption \ref{a:indep-ysf} allows for limited confounding between surnames and outcome, or even a direct effect of surnames on the outcome via $f(S)$.

```{=tex}
\begin{figure}[ht]
\begin{center}
\begin{subfigure}[b]{0.4\textwidth}
\centering
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,thick]
    
        \node[main node] (G) {$G$};
        \node[main node] (R) [below of=G] {$R$};
        \node[main node] (S) [below left=0.5cm and 0.5cm of R] {$S$};
        \node[main node] (Y) [right of=G] {$Y$};
        \node[main node] (X) [below of=Y] {$X$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (X)
        (R) edge node  {} (Y)
        (R) edge node  {} (S)
        (G) edge node  {} (X)
        (G) edge node  {} (Y)
        (X) edge node  {} (Y);
        
        %\path[every node/.style={font=\sffamily\small}, <->]
        %(G) edge [dashed, bend right] node  {} (S)
        %(X) edge [dashed, bend left] node  {} (S);
    \end{tikzpicture}
    \caption{Assumption \ref{a:indep-ys}: Conditional independence of $Y$ and $S$ given $(G, R, X)$}
\end{subfigure}
\hspace{1em}
\begin{subfigure}[b]{0.4\textwidth}
\centering
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.6cm,thick]
    
        \node[main node] (G) {$G$};
        \node[main node] (R) [below of=G] {$R$};
        \node[main node] (S) [below left=0.5cm and 0.5cm of R] {$S$};
        \node[main node] (fS) [above left=0.65cm and 0.05cm of S] {\small $f(S)$};
        \node[main node] (Y) [right of=G] {$Y$};
        \node[main node] (X) [below of=Y] {$X$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (X)
        (R) edge node  {} (Y)
        (R) edge node  {} (S)
        (S) edge node  {} (fS)
        (fS) edge[bend left=15,dashed] node {} (Y)
        (Y) edge[bend right=15,dashed] node {} (fS)
        (G) edge node  {} (X)
        (G) edge node  {} (Y)
        (X) edge node  {} (Y);
    \end{tikzpicture}
    \caption{Assumption \ref{a:indep-ysf}: Conditional independence of $Y$ and $S$ given $(f(S), R, G, X)$}
\end{subfigure}
\end{center}

\caption[Causal DAGs comparing Assumptions \ref{a:indep-ys} and \ref{a:indep-ysf}]{
Possible causal structures for which each of the labeled assumptions is satisfied, represented as a directed acyclic graph (DAG). 
Double-ended dashed arrows indicate causal confounding of the connected variables.
Both DAGs also satisfy Assumption \ref{a:indep-sgx}: Conditional independence of $S$ and $(G,X)$ given $R$.}
\label{fig:dag2}
\end{figure}
```
--->

The next theorem shows that it is still possible to nonparametrically identify racial disparities under Assumption \ref{a:indep-ysf} under the identification condition, which is only slightly stronger than for Theorem \ref{thm:id}.

\begin{restatable}[Nonparametric Identification Under Assumption \ref{a:indep-ysf}]{theorem}{thmidrel} \label{thm:id2}
Let $f:\cS\to\R^d$, $d< |\cS|$, with range $f(\cS)$.
For any given $g\in\cG$, $x\in\cX$, $z\in f(\cS)$, and $y\in\cY$, define a matrix $\vb P$ with entries $p_{rs}=\Pr(R=r\mid G=g, X=x, S=s)$  and a vector $\vb b$ with entries $b_s=\Pr(Y=y\mid G=g, X=x, S=s)$.
Then under Assumption~\ref{a:indep-ysf}, and assuming knowledge of the joint distribution $\Pr(R,G,X,S)$, the conditional probabilities $\Pr(Y=y\mid R,f(S)=z,G=g, X=x)$ are identified if and only if both $\vb P$ and the augmented matrix $\mqty(\vb P&\vb b)$ have rank $|\cR|$.
\end{restatable}

As long as the dimension $d$ of the surname summary statistic $f(S)$ is much smaller than the (usually large) number of surnames $|\cS|$, racial disparities are likely to be identified under Theorem \ref{thm:id2} when they are already identified under Theorem \ref{thm:id}.
Thus, Assumption \ref{a:indep-ysf} and Theorem \ref{thm:id2} can be used in conjunction with carefully chosen $f$ in order to probe likely failure modes of the more restrictive Assumption \ref{a:indep-ys}.
If estimates are not much affected by the inclusion of $f(S)$, then researchers can be more confident in the plausibility of Assumption \ref{a:indep-ys}.

Second, bias can also arise from violations of the assumptions underlying the BISG methodology (Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc}).
Of course, this is not unique to the proposed methodology: violations of these assumptions will also affect the validity of other disparity estimators such as weighting or thresholding.
However, since as discussed above the BISG assumptions may rarely hold exactly in practice, we provide in Appendix \ref{app:sens} several results characterizing how the model's estimates are affected by bias in the BISG probabilities.

# Empirical Validation {#sec:valid}

To better understand how BIRDiE performs in real-world contexts, we apply it to North Carolina voter registration data.
This voter file contains individual-level self-reported race for almost all voters and hence the "ground truth" relationship between outcome and race is known.
We compare the performance of BIRDiE models against those of the weighting and thresholding estimators.
We also evaluate how the estimation error depends on the level of geographic precision used in the BISG probabilities.
Finally, we briefly demonstrate various extensions of the BIRDiE methodology: small-area estimates, improved individual race predictions, estimation conditional on an additional explanatory variable (as discussed in Section \ref{sec:addlcov}), and sensitivity analysis for potential assumption violation (Section \ref{sec:sens}).

## North Carolina Voter File

Like most other Southern states, which have a history of disenfranchising minority voters, the state of North Carolina asks (and previously required) every voter to self-report their race upon registration.
This data, along with voters' names, addresses, gender, party registration (if any), and voting history, is part of the voter file that the secretary of state makes publicly available.
This feature makes the voter file an ideal validation setting for the proposed methodology.
The outcome we examine here, party registration, is the product of many unobservable factors, and is known to differ across racial groups.
Since self-reported race is available, inferences about these racial disparities using the estimators discussed here can be compared to the corresponding ground truth.

Estimation of party registration by race is of substantive interest as well, especially in the context of the Voting Rights Act of 1965 (VRA).
The relationship between these variables is critical for understanding the impact of policy changes such as redistricting or election rules on compliance with the VRA, and for establishing legal standing to challenge these policies under the VRA.
As many states do not ask for self-reported race during voter registration, and other states no longer require reporting race, methods like BIRDiE are important tools for evaluating VRA compliance.

We use a subset of the October 2022 voter file which could be linked to a proprietary voter file provided by L2, Inc., a leading national non-partisan firm and the oldest organization in the United States that supplies voter data and related technology to candidates, political parties, pollsters, and consultants for use in campaigns.
The L2 file geocoded each address to a Census block, which allows for the finest block-level BISG predictions.
We also removed any records without individual race information, since our goal is validation compared to some ground truth, rather than inference about the entire population of registered North Carolina voters.
Altogether, `r percent(1 - 5754912/7389159, 0.1)` of records either had missing race information or could not be linked to the L2 file.

```{r nc-overview, out.width="6in"}
#| fig.cap: |
#|     Distribution of party registration by race 
#|     for a sample of 1,000,000 North Carolina voters. Parties are Libertarian (LIB),
#|     Republican (REP), Independent (IND), and Democratic (DEM).
#| fig.scap: Distribution of party registration by race for North Carolina voters
knitr::include_graphics("figures/nc_overview.pdf")
```

The overall merged voter file contains 5,754,912 voters, 71.1% of which are White, 21.1% of which are Black, and 7.8% of which belong to another race.
To reduce computational burden, we further subsampled this file by selecting 1,000,000 records at random without replacement.
This sample size is large enough to ensure that sampling error in the estimates is negligibly small.

Figure \@ref(fig:nc-overview) shows the distribution of party registration by self-reported race in this subsample.
White voters disproportionately register as Republicans, while Black voters disproportionately register as Democrats.
This serves as the ground-truth in our validation analysis presented below.

## The Model Setup

We first calculate BISG probabilities using 2010 Census data at the census block, tract, ZIP code tabulation area (ZCTA), and county level.
Every record in the voter file contains county information, while roughly 13% of records are missing ZIP codes and 27% of records are missing blocks/tracts; when these finer geographic identifiers were missing, we used county-level Census tables in the BISG calculations.

```{r}
x = readRDS("data/nc_bisg.rds")
acc = percent(x$acc, 0.1)
score = number(x$score, 0.001)
```

The BISG probabilities are broadly accurate.
Using the maximum a posteriori racial category as a prediction, we obtain the accuracy of `r acc["county"]` for the county probabilities, `r acc["zip"]` for the ZIP code probabilities, `r acc["tract"]` for the tract probabilities, and `r acc["block"]` for the block probabilities.
An alternative measure of the quality of the BISG probabilities is the logarithmic score, a proper scoring rule which rewards precise and calibrated probabilistic estimates (higher values are better).
The logarithmic scores for the BISG probabilities are -`r score["county"]` for counties, -`r score["zip"]` for ZIP codes, -`r score["tract"]` for tracts, and -`r score["block"]` for blocks.
For comparison, the prior-only logarithmic score (i.e., using no name or geographic information) is -`r number(x$base_score, 0.001)`.
The worsened performance for block-level versus tract-level probabilities likely stems from the larger impact of census measurement error at smaller geographies, a problem that could be addressed using newer BISG methods such as those of @imai2022addressing.

For a given set of BISG probabilities, we estimate the conditional distribution of each outcome variable given race using BIRDiE with both saturated pooling and multinomial mixed-effects models introduced above.
We then compare the resulting estimates based on these BIRDiE models against those of the two existing estimators --- the weighting estimator as well as a thresholding estimator that deterministically assigns each individual the maximum *a posteriori* racial category.
For the saturated BIRDiE model, we use geographic effects matching the geographic level used in the BISG probabilities (e.g., county effects for the county-level BISG probabilities), except for the block-level probabilities.
Due to the large number of individual census blocks, we use tract-level effects instead for this particular model.

We use our software's default priors for both BIRDiE models.
This is a uniform prior for the saturated model and a $\Norm(0, 1)$ prior on $\vb*\beta$ and a $\Gam(2, 10)$ prior on the random intercept standard deviation for the mixed model.
With standardized covariates, the prior on the fixed effects $\vb*\beta$ is weakly informative, corresponding to a prior belief that a one-standard-deviation increase in the covariate level will generally correspond to a change in the linear predictor between --2 and 2.
The weakly informative prior on the random intercept scale is centered at 0.2 and places most of its mass between 0.024 and 0.56.
To give an idea of the computational efficiency of the proposed method, the maximum runtime of the saturated of the BIRDiE models fit to estimate party registration was 6.9 seconds (on a modern laptop with 8GB RAM), and the maximum runtime for the mixed model estimates was 22.6 seconds.

## Estimates of Racial Disparity in Party Registration

```{r nc-disp, fig.width=7}
#| fig.cap: |
#|     Error in the White-Black and White-Hispanic disparity estimates for
#|     party registration, by estimation method.
#|     All methods used block-level data for this figure; the results 
#|     for other levels of geographic detail are generally similar.
#|     Estimation uncertainty is minimal and hence suppressed from 
#|     the figure for clarity.
#| fig.scap: Error in racial disparity estimates for party registration, by estimation method
x = readRDS(here("paper/data/nc_disp_ex.rds"))
same_sat_mmm = if (abs(x$birdie_sat$disp_wb - x$birdie_mmm$disp_wb) < 0.1) "also" else ""
knitr::include_graphics("figures/nc_disp.pdf")
```

We first examine the relative accuracy of the proposed methods in estimating the disparity between White and Black, and White and Hispanic voters, in party registration.
For example, the true difference in Democratic registration between Black and White voters in the sample is $`r percent(-x$weight$disp_wb_true, 0.1)`$ percentage points, meaning Black voters register Democratic at a much higher rate.
However, the standard weighting approach produces an estimate of only $`r percent(-x$weight$disp_wb, 0.1)`$ percentage points for this disparity---less than half the true value.
This is consistent with Corollary \ref{cor:under}, which states that the weighting estimator tends to underestimate the magnitude of racial disparity.
The thresholding estimator, while slightly better, also misses the mark, with an estimate of $`r percent(-x$thresh$disp_wb, 0.1)`$ percentage points.
In contrast, the saturated BIRDiE model produces an estimate of $`r percent(-x$birdie_sat$disp_wb, 0.1)`$ percentage points, and the mixed BIRDiE model `r same_sat_mmm` estimates $`r percent(-x$birdie_mmm$disp_wb, 0.1)`$.
These estimates are only slightly lower than the ground truth.

Figure \@ref(fig:nc-disp) compares the empirical performance of the BIRDiE models against that of the weighting and thresholding estimators across all of these possible disparity measurements, using the block-level BISG predictions.
The true disparity is subtracted off from each estimate for ease of comparison, and so the figure plots the estimation error of racial disparity between two racial groups.
For White--Black (left plot) and White-Hispanic (right plot) disparities in party registration, the BIRDiE models (solid circles and squares) substantially outperform the two commonly used estimators (open circles and crosses). 
For two major parties, both the weighting and thresholding estimators exhibit a substantial amount of estimation error, for example, exceeding 20 percentage points for the White-Black disparity for the Democratic party.
In contrast, the two BIRDiE models yield a much smaller estimation error that ranges within several percentage points for all racial disparity estimates.
The saturated and mixed-effects BIRDiE models perform similarly with no discernible difference. 

```{r nc-tv}
#| fig.cap: |
#|     Total variation distance between the estimated and actual distribution of
#|     party registration, 
#|     by estimation method and level of geographic detail 
#|     used in the BISG predictions.
#|     The left plot shows the overall total variation distance,
#|     whlie the right plot decomposes it by racial group.
#| fig.scap: |
#|     Total variation distance between the estimated and actual distribution of
#|     party registration, by estimation method and level of geographic detail 
#|     used in the BISG predictions
knitr::include_graphics("figures/nc_tv.pdf")
```

For a more comprehensive look at the error in the estimated partisanship-by-race distributions, we turn to the total variation (TV) distance, which is calculated as
$$
d_\text{TV}(\hat{\vb*\mu}_{Y|R}, \vb*\mu_{Y|R}) 
= \half\sum_{y\in\cY}\sum_{r\in\cR} |\hat{\mu}_{Y,R}(y, r) - \mu_{Y,R}(y, r)|,
$$ 
where $\mu_{Y,R}$ is the joint distribution of $Y$ and $R$.
The TV distance is an upper bound on the error in *any* probability calculated from the estimated joint distribution, and as such is useful general-purpose measure of estimation error.
The left plot of Figure \@ref(fig:nc-tv) shows the TV distance for each estimator, not just for the block-level BISG estimates used in Figure \@ref(fig:nc-disp) but also across the range of geographic levels used in the BISG calculation (x-axis).
We find that both BIRDiE models substantially outperform every alternative at every geographic level.
In general, the estimates based on the BIRDiE models exhibit a total variation distance whose magnitude is about one third and one fourth of that for the thresholding and weighting estimators, respectively.
As before, the saturated and mixed-effects BIRDiE models perform similarly.
According to this measure, we find that finer geographic data provide only minor improvements in accuracy for the BIRDiE or conventional estimates.
While possibly counterintuitive, this finding underscores the fact that calibrated BISG probabilities, rather than highly precise probabilities, are all that is needed for accurate disparity estimation.^[Of course, both calibrated and precise probabilities are to be preferred to imprecise but calibrated probabilities.
However, in practice there may be a tradeoff between the two.
For example, including first names in the BISG predictions increases their precision.
But first names may lead to worse calibration, since BISG methods which use surnames make an somewhat unrealistic conditional independence assumption, and data on first names by race come from non-census sources.
Additionally, unlike surnames, first names (which are usually chosen by parents) can be more correlated with socioeconomic variables, leading to violations of Assumptions \ref{a:indep-ys}.]


It is also possible to measure the TV distance for each conditional distribution by race:
$$
d_\text{TV}^{(r)}(\hat{\vb*\mu}_{Y|r}, \vb*\mu_{Y|r}) 
= \half\sum_{y\in\cY} |\hat{\mu}_{Y,r}(y, r) - \mu_{Y,r}(y, r)|.
$$ 
The right plot of Figure \@ref(fig:nc-tv) shows these within-race TV distances, to illuminate how the estimators perform on each subgroup.
In general, the BIRDiE models are more accurate than the weighting and thresholding estimators for the White and Black racial groups which together make up 96% of the sample.
All estimators perform roughly equally well for Hispanic, Asian, and Native voters (though the thresholding estimator performs well for Hispanic voters), exhibiting relatively small estimation error.
The weighting and thresholding estimators perform particularly poorly for Black voters and for the "Other" racial group.

## Small-area Estimation {#sec:small}

An advantage of the BIRDiE methodology is its explicit modeling of $\Pr(Y\mid R,G,X)$, which produces not only estimates of the marginal $\Pr(Y\mid R)$ but also subgroup estimates of how these conditional distributions vary across covariates and geographic areas.
This section examines the accuracy of the saturated and mixed-effects BIRDiE models in recovering small-area relationships between party registration and race, compared to standard methodology that simply applies the weighting and thresholding estimators within each geographic area.
We study accuracy at the county, ZIP code, and tract level, using the BISG probabilities and BIRDiE models that were applied to each.
Since in fitting the BIRDiE model to block-level BISG probabilities we used tract-level random intercepts, we do not present block-level estimates.

```{r nc-smallarea, out.width="5.5in"}
#| fig.cap: |
#|     Accuracy of small-area estimates by race, as measured by 
#|     the average total variation distance.
#| fig.scap: Accuracy of small-area party registration estimates by race
knitr::include_graphics("figures/nc_smallarea.pdf")
```

We evaluate the small-area estimates by calculating the mean total variation distance between the estimated and true conditional distributions of party registration by race (averaging across geographic areas).
Figure \ref{fig:nc-smallarea} summarizes our results, which qualitatively track the patterns found overall in Figure \ref{fig:nc-tv}.
The two BIRDiE models exhibit substantially lower error than the weighting and thresholding estimators.
Across all methods, the error is lower for White voters, who make up the bulk of the sample.
Somewhat surprisingly, the amount of error does not appear to vary much for the BIRDiE models across different levels of geography---tract-level estimates are roughly as accurate as county-level estimates, on average.

Between the BIRDiE models, the mixed-effects model slightly outperforms the saturated model.
This reflects the value in partially pooling estimates through the random effect structure.

## Improved Individual Race Probabilities

```{r}
x = readRDS("data/nc_bisg.rds")
acc = percent(x$acc, 0.1)
score = number(x$score, 0.001)

x2 = readRDS("data/nc_bisg_post_party.rds")
acc2 = percent(x2$acc, 0.1)
score2 = number(x2$score, 0.001)
```

As discussed in Section \ref{sec:update-bisg}, we can use the conditional distribution $\Pr(Y\mid R, X, G)$ estimated with a BIRDiE model to create model-updated BISG probabilities $\tilde{\vb P}_{\mid Y}=\pi(\vb R \mid \hat\Theta, \vb Y, \vb G, \vb X, \vb S)$ by applying Bayes' rule.
These updated probabilities may be more accurate than the original BISG probabilities.

```{r nc-roc}
#| fig.cap: |
#|     Race probability predictive accuracy, as measured by the area under the 
#|     receiver operating characteristic (ROC) curve, for the input BISG 
#|     probabilities as well as the BIRDiE-updated probabilities, by race and
#|     level of geographic precision.
#|     Larger values indicate more precise predicions.
#| fig.scap: |
#|     Race probability predictive accuracy for the input BISG and 
#|     BIRDiE-updated probabilities, by race and level of geographic precision
knitr::include_graphics("figures/nc_roc.pdf")
```

For example, using the estimates produced by the mixed BIRDiE model applied to party registration with block-level BISG estimates, the MAP prediction accuracy increases from `r acc["block"]` with the input probabilities to `r acc2["mmm"]` with the updated probabilities.
These increases are significantly larger than the differences in accuracy between BISG probabilities using different levels of geographic precision.

The improvements are reflected in other accuracy  measures as well.
Figure \ref{fig:nc-roc} shows the accuracy of the predictions by race, as measured by the area under the receiver operating characteristic (ROC) curve.
The updated probabilities are noticeably more accurate than the input probabilities for White and Black voters, about as accurate for Hispanic, Asian, and Native voters, and slightly less accurate for "Other" voters.
Since White and Black voters together constitute the vast majority of the sample, these patterns translate to improvement in the logarithmic scores as well: the score for the input probabilities is -`r score["block"]`, and this increases to -`r score2["mmm"]` with the updated probabilities.

## Estimates Conditional on an Additional Variable

```{r}
x = readRDS("data/nc_addlcov.rds") 
```

The North Carolina voter file also provides an opportunity to demonstrate the methodology described in Section \ref{sec:addlcov} to produce estimates conditional on another predictor variable that is not used in the BISG probabilities.
We will estimate party registration rates by race among voters and nonvoters in the 2020 election.
Following the discussion in Section \ref{sec:addlcov}, we will compute these estimates two ways: (1) by estimating party registration and 2020 turnout jointly by race, and (2) by first estimating 2020 turnout by race, then estimating party registration by race and 2020 turnout.

We will use a multinomial mixed-effects BIRDiE model applied to the block-level BISG probabilities, with random intercepts by tracts, for all the estimation.
Fitting this model to a combined 2018/2020 turnout variable (i.e., one with four levels: no/no, no/yes, and so on) produces estimates of the joint distribution of 2018 and 2020 turnout by race.
Normalizing these probabilities within 2018 turnout and race groups produces estimates of 2020 turnout by race and 2018 turnout.
The total variation distance between these estimates and the true distribution is `r number(x$joint, 1e-3)`, which indicates close agreement.

```{r nc-addlcov}
#| fig.cap: |
#|     Error in the conditional expectation estimates for
#|     party registration by turnout and race, by estimation method.
#|     All methods used block-level data for this figure.
#|     Estimation uncertainty is minimal and hence suppressed from 
#|     the figure for clarity.
#| fig.scap: | 
#|     Error in the conditional expectation estimates for
#|     party registration by turnout and race, by estimation method
knitr::include_graphics("figures/nc_addlcov_error.pdf")
```

Figure \ref{fig:nc-addlcov} presents the errors in the estimates for this method (solid square), the two-step method (solid circle), and for the weighting (open circle) and thresholding (cross) estimators.
The TV distance between these two-stage estimates and the true distribution is `r number(x$two, 1e-3)`, very similar to the error in the joint-estimation approach.
The two-step approach produces similar estimates to the joint-estimation approach, though the latter performs better in the "Other" category.
As discussed in Section \ref{sec:addlcov}, while both approaches produce highly accurate estimates in this example, we would expect the two-stage approach to be superior when one or both of the variables has more levels.

In contrast to the BIRDiE models, the weighting and thresholding estimates of party registration by 2020 turnout and race include large errors, especially for Black voters with all the errors exceeding 10 percentage points.
The TV distance for the weighting estimator is `r number(x$weight, 1e-3)`, and the distance for the thresholding estimator is `r number(x$thresh, 1e-3)`---around 3--4 times higher than for the estimates based on the BIRDiE models.

## Sensitivity Analysis

Finally, we examine the sensitivity of our party registration estimates to potential violations of the key identifying Assumption \ref{a:indep-ys}, following the method outlined in Section \ref{sec:sens} that is based on a low-dimensional summary statistic of surnames.
We use a publicly available sample of 5\% of the individual records for the 1930 Census [@cens1930], which contains individual names, individual and parental birthplace, and detailed race, ethnicity, and tribal codes.
Since many regions of Asia, particularly Vietnam, experienced little emigration to the United States before 1930, we further supplement this data with around 3,000 Asian surnames classified into six regional subgroups: Chinese, Filipino, Indian, Japanese, Korean, Vietnamese, NHPI, and Other [@asiannames].
 
Using these subgroups and the 1930 birthplace and racial data, we can classify most surnames in the voter file into nine groups (see Appendix \ref{app:surgrp} for a brief description of the groupings and the most common 50 surnames for each group).
While somewhat arbitrary, these groups are chosen to combine countries of origin which had significant immigration to the United States during similar periods.

We first evaluate the plausibility of Assumption \ref{a:indep-ys} by examining the correlation between the residuals of the BIRDiE model fit and indicator variables for each of the nine surname groups.
Under Assumption \ref{a:indep-ys}, this residual correlation should be zero everywhere.
As Figure \ref{fig:nc-sens} shows, however, for many groups and party labels, the correlation is small but deviates from zero more than would be expected given only sampling variation.
Here, we use the residuals from the county-level saturated model specification, but the results are not sensitive to this choice.

```{r nc-sens}
#| fig.cap: |
#|     Residual correlation between party registration and nine surname groups,
#|     after controlling for race and location.
#|     Correlations whose 90\% confidence intervals exclude zero are marked with
#|     an asterisk.
#|     See Appendix \ref{app:surgrp} for details on the surname groups.
#| fig.scap: | 
#|     Residual correlation between party registration and nine surname groups 
#|     in North Carolina.
knitr::include_graphics("figures/nc_sens_grp.pdf")
```

Notably, voters with names in the Anlglosphere and Black surname group, which includes surnames that are relatively more common among many-generation residents of the U.S., such as Smith, Williams, and Brown, are significantly less likely to register as Democrats and independents, and more likely to identify as Republicans, even after controlling for race and geography.
Meanwhile, voters with names in the First and Second wave European immigration surname groups, which include surnames more common among 19th and 20th century immigrants from Europe, display the opposite pattern.
Differences among surname groups designed to correlate with membership in various Asian subgroups are also visible.
However, all of these correlations are quite small in magnitude, with most on the order of 0.01 or so.
Thus, we expect our party-by-race estimates to be little affected by the inclusion of the surname group indicators.

```{r}
x = readRDS(here("paper/data/nc_sens_diff.rds")) # diffs between sens and original
max_x = max(abs(x))
max_loc = which(abs(x) == max_x, arr.ind=TRUE)
max_race = str_to_title(colnames(x)[max_loc[, "col"]])
max_party = c(dem="Democratic", ind="Independent", rep="Republican")[rownames(max_loc)]
```

Indeed, re-fitting the county-level saturated model with an additional surname group covariate produces nearly identical results, albeit at a moderately higher computational cost given the increased number of $(G, X, f(S))$ cells.
This re-fitting requires Assumption \ref{a:indep-ysf}, which relaxes Assumption \ref{a:indep-ys}.
We find that the average party registration rate estimate changes only by `r number(mean(abs(x)), 0.001)`, with the largest change being `r number(max_x, 0.001)`, for the rate of `r max_party` registration among `r max_race` voters.
All in all, this analysis provides confidence that violations of Assumption \ref{a:indep-ys} for the North Carolina voter file are likely minor and would have minimal effects on quantitaive and qualitative conclusions.

# Discussion {#sec:disc}

We have introduced a new identifying assumption and accompanying model, BIRDiE, and clarified other assumptions implicit in approaches to disparity estimation from proxy data.
In many real-world applications, we believe the new model and identification condition are appropriate and will produce significantly improved estimates.
However, there is no one-size-fits-all approach for the estimation of disparities.
For example, the existence of name-based discrimination may violate our identification assumption especially when racial categories, for which data are available, are coarse.
Careful consideration of the underlying causal and information structure is required to avoid making the incorrect conclusions.
We also provide a sensitivity analysis that partially addresses this concern.

As our empirical demonstration showed, in realistic settings BIRDiE can substantially outperform existing estimators of racial disparities, both in aggregate and for small areas.
The BIRDiE methodology also produces improved BISG probabilities, and can be used to estimate disparities conditional on other variables.
These additional features should prove helpful in practical settings.

Much work remains to be done in accurately and reliably measuring racial disparities.
First, the BISG probabilities themselves can be improved.
One approach to doing so is given by @imai2022addressing, which accounts for some Census measurement error while remaining computationally tractable.
Beyond the BISG probabilities, further empirical analyses could determine useful additional variables to condition on, which could allow analysts to weaken the required assumption.
Finally, the BIRDiE model could be extended to directly model more complex types of outcome variables.

\bibliography{references.bib}

\newpage

# (APPENDIX) Appendix {.unnumbered}

```{=tex}
\renewcommand\thefigure{\thesection\arabic{figure}}
\setcounter{figure}{0}
```

# Proofs of Propositions

\corunder*
\begin{proof}
For notational simplicity, let $\mu_r=\Pr(Y=y\mid R=r)$ and $\hat\mu_r=\hat\mu^{(\text{wtd})}_{Y|R}(y\mid r)$ for $r\in\{0,1\}$.
Since $\Pr(Y=y\mid R=1, G=g, X=x, S=s)>\Pr(Y=y\mid R=0, G=g, X=x, S=s)$ for all $g\in\cG$, $x\in\cX$, 
necessarily $\E[\Cov(\ind\{Y=y\}, \ind\{R=1\}\mid G,X,S)]>0$ and $\E[\Cov(\ind\{Y=y\}, \ind\{R=1\}\mid G,X,S)]<0$.
We note that the corollary could be stated under this more general condition, but was not for expositional clarity.
Thus by Theorem \ref{thm:wt-bias}, $\hat\mu_1-\mu_r<0$ and $\hat\mu_1-\mu_r>0$.
Then
\begin{align*}
    \hat\mu_1-\hat\mu_0
    &= \hat\mu_1-\mu_1+\mu_1-\mu_0+\mu_0-\hat\mu_0 \\
    &= (\hat\mu_1-\mu_1)-(\hat\mu_0-\mu_0)+(\mu_1-\mu_0) \\
    &< \mu_1-\mu_0,
\end{align*}
as claimed.
\end{proof}

\thmid*
\begin{proof}
Applying the law of total probability and our conditional independence relation $S\indep Y\mid R,G,X$, we have,
for all $y\in\cY$, $g\in\cG$, $x\in\cX$, and $s\in\cS$,
\begin{align*}
    \Pr(Y=y\mid &G=g, X=x, S=s) \\
    &= \sum_{r\in\cR} \Pr(Y=y\mid R=r, G=g, X=x, S=s)\Pr(R=r\mid G=g, X=x, S=s) \\
    &= \sum_{r\in\cR} \Pr(Y=y\mid R=r, G=g, X=x)\Pr(R=r\mid G=g, X=x, S=s).
\end{align*}
The left-hand side is estimable from the data and the rightmost term $\Pr(R=r\mid G=g, X=x, S=s)$ is assumed known.
So for each $y\in\cY$, $g\in\cG$, and $x\in\cX$, this relation is a linear system in unknown parameters $\Pr(Y=y\mid R=r, G=g, X=x)$.
These parameters are identified if and only if this system has a unique solution, i.e. if the coefficient matrix $\vb P$ has rank $|\cR|$ and so does the augmented matrix $\mqty(\vb P&\vb b)$.
\end{proof}

\thmolsunb*
\begin{proof}
Fix $y\in\cY$ and define $m_{gxr} = \E[\ind\{Y=y\}\mid R=r, G=g, X=x)]$.
Then under Assumptions \ref{a:indep-sgx}, \ref{a:cens-acc}, and \ref{a:indep-ys},
\begin{align*}
    \E[\ind\{Y=y\}&\mid G=g, X=x, S=s] \\
    &= \sum_{r\in\cR} \E[\ind\{Y=y\}\mid R=r, G=g,X=x)]\Pr(R=r\mid G=g,X=x,S=s) \\
    &= \sum_{r\in\cR} m_{gxr} \hat p_r,
\end{align*}
where as in the main text $\vb{\hat p}$ is the (random) vector of BISG probabilities.
In fact, since the right-hand side depends on $S$  only through $\hat{\vb p}$, we have \[
    \E[\ind\{Y=y\}\mid G=g, X=x, \hat{\vb p}] 
    = \sum_{r\in\cR} m_{gxr} \hat p_r.
\]
So the conditional expectation of $\ind\{Y=y\}$ given $X$, $G$, and the BISG probabilities $\vb{\hat p}$ is linear in those probabilities, with coefficients $m_{gxr}$.
Consequently, the OLS estimate $\hat{\vb*\mu}^{(\text{ols})}_{Y\mid RGX}(y\mid\cdot,g,x)$ will be unbiased for $m_{gxr}$, by the standard results.

Now, we can expand $\Pr(Y=y\mid R=r)$ as
\begin{align*}
    \Pr(Y=y\mid R=r) 
    &= \sum_{x\in\cX,g\in\cG} \Pr(Y=y\mid R=r, G=g, X=x)\Pr(G=g, X=x\mid R=r) \\
    &= \sum_{x\in\cX,g\in\cG} m_{gxr} q_{gx|r}.
\end{align*}
Since $\hat{\vb*\mu}^{(\text{ols})}_{Y\mid RGX}(y\mid \cdot, g,x)$ is unbiased for $m_{gxr}$,  by the linearity of expectation the poststratified estimator $\hat{\mu}^{(\text{p-ols})}_{Y\mid R}(y\mid r)$ is unbiased for $\Pr(Y=y\mid R=r)$.
\end{proof}

\thmwtdols*
\begin{proof}
Fix a $y\in\cY$, $g\in\cG$ and $x\in\cX$.
The weighting estimator of $\Pr(Y=y\mid R=r)$ within the set of individuals with $G_i=g$ and $X_i=x$ may be written $$
    \hat\mu^{(\text{wtd})}_{Y|RGX}(y\mid r,g,x) 
    = \frac{\hat{\vb P}_{\cI(xg) r}^\top \ind\{\vb Y_{\cI(xg)}=y\}}{\hat{\vb P}_{\cI(xg) r}^\top \vb 1}
    = \frac{\norm{\proj_{\hat{\vb P}_{\cI(xg) r}}(\ind\{\vb Y_{\cI(xg)}=y\})}}{
        \norm{\proj_{\hat{\vb P}_{\cI(xg) r}}(\vb 1)}},
$$ the ratio of the projected length of the outcome vector $\ind\{\vb Y=y\}$ and the constant vector $\vb 1$ onto $\hat{\vb P}_{\cdot r}$.
We can write the OLS estimator as $$
    \hat{\vb*\mu}^{(\text{ols})}_{Y|R} = (\hat{\vb P}_{\cI(xg)}^\top\hat{\vb P}_{\cI(xg)})^{-1}
            \hat{\vb P}_{\cI(xg)}^\top \ind\{{\vb Y}_{\cI(xg)}=y\}
    = \mathrm{coord}_{\hat{\vb P}_{\cI(xg)}}(\proj_{\hat{\vb P}_{\cI(xg)}}(\ind\{{\vb Y}_{\cI(xg)}=y\}),
$$ where $\mathrm{coord}_{\hat{\vb P}_{\cI(xg)}}$ is the function that returns the coordinates of its input vector in the $\hat{\vb P}_{\cI(xg)}$ basis (by assumption $\hat{\vb P}_{\cI(xg)}$ has rank $|\cR|$ and so its columns are linearly independent).
To make the comparison even easier, notice that we can break the projection $\proj_{\hat{\vb P}_{\cI(xg) r}}$ into two steps, writing it instead as $\proj_{\hat{\vb P}_{\cI(xg) r}} = \proj_{\hat{\vb P}_{\cI(xg) r}} \circ \proj_{\hat{\vb P}}$.
Letting $\vb Y_{\proj}=\proj_{\hat{\vb P}_{\cI(xg)}}(\ind\{{\vb Y}_{\cI(xg)}=y\})$, then, we can rewrite our estimators as \[
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid r) 
    = \frac{\norm{\proj_{\hat{\vb P}_{\cI(xg) r}}(\vb Y_\proj)}}{
        \norm{\proj_{\hat{\vb P}_{\cI(xg) r}}(\vb 1)}}  \qand
    \hat{\vb\mu}^{(\text{ols})}_{Y|R}(y\mid r)
    = \mathrm{coord}_{\hat{\vb P}_{\cI(xg)}}(\vb Y_\proj)_r.
\]

Now, since the individual BISG probabilities are nonnegative and sum to 1, a pair $j,k\in\cR$ of races has perfectly discriminating BISG probabilities if and only if the corresponding columns of $\hat{\vb P}$ are orthogonal, i.e., $\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}=I$.
Begin by writing $\vb Y_\proj$ in terms of the $\hat{\vb P}_{\cI(xg)}$ basis, so \[
    \vb Y_\proj = \sum_{j\in\cR} c_j\hat{\vb P}_{\cI(xg) j},
\] and thus $\hat{\vb\mu}^{(\text{ols})}_{Y|R}(y\mid j)=c_j$.
Without loss of generality, suppose the $c_j$ are numbered as $c_1\ge c_2\ge \cdots\ge c_{|\cR|}$.
We can also expand $\vb 1$ in the same basis. Since the individual probabilities must sum to one, in fact we have \(
    \vb 1 = \sum_{j\in\cR} \hat{\vb P}_{\cI(xg) j}.
\)

For the forward direction, we assume $\hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid j)=\hat{\mu}^{(\text{ols})}_{Y|R}(y\mid j)=c_j$;
multiplying out the denominator of the weighting estimator, we have $\hat{\vb P}_{\cI(xg) j}^\top \vb Y=c_j\hat{\vb P}_{\cI(xg) j}^\top\vb 1$ for all $j$; substituting the basis expansions of $\vb Y_\proj$ and $\vb 1$, this yields \[
    \sum_{k\in\cR} c_k \hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}
    = \sum_{k\in\cR} c_j \hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}, \qq{so}
    \sum_{k\in\cR} (c_j-c_k) \hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}=0.
\] Now fix $j\in J_1=\argmax_j c_j$; this relation still holds, but now every term in the sum is nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1$.
Therefore we must have $\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}=0$ for all $k\not\in J_1$.
Then fix $j\in J_2=\argmax_{j\not\in J_1} c_j$; since $\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) l}=0$ for all $l\in J_1$, every term in the sum is still nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1\cup J_2$.
Therefore we must have $\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}=0$ for all $k\not\in J_1\cup J_2$.
Proceeding this way through all sets of common values in the $c_j$ we find that for all $j,k\in\cR$, either $c_j=c_k$ or  $\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}=0$.

For the reverse direction, fix $j\in\cR$ and let $J=\{k\in\cR:c_k=c_j\}$, so that by assumption $\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}=0$ for all $k\not\in J$.
Then by the above basis expansion, $\hat{\mu}^{(\text{ols})}_{Y|RGX}(y\mid j)=c_j$, and \[
    \hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid j) 
    = \frac{\sum_{k\in\cR} c_k \hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg)k}}{
        \sum_{k\in\cR}\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}}
    = \frac{c_j \sum_{k\in J} \hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}}{
        \sum_{k\in J}\hat{\vb P}_{\cI(xg) j}^\top\hat{\vb P}_{\cI(xg) k}}
    = c_j = \hat{\mu}^{(\text{ols})}_{Y|R}(y\mid j). \qedhere
\]
\end{proof}

\thmidrel*
\begin{proof}
The argument is identical to the proof of Theorem \ref{thm:id}.

Applying the law of total probability and our conditional independence relation $S\indep Y\mid f(S),R,G,X$, we have,
for all $y\in\cY$, $g\in\cG$, $x\in\cX$, and $s\in\cS$,
\begin{align*}
    \Pr(Y=y\mid G=g, X=x, S=s)
    &= \sum_{r\in\cR} \Pr(Y=y\mid R=r, f(S)=s, G=g, X=x, S=s)\\
    &\qquad\times\Pr(R=r\mid G=g, X=x, S=s) \\
    &= \sum_{r\in\cR} \Pr(Y=y\mid R=r, f(S)=s, G=g, X=x)\\
    &\qquad\times\Pr(R=r\mid G=g, X=x, S=s).
\end{align*}
The left-hand side is estimable from the data and the rightmost term $\Pr(R=r\mid G=g, X=x, S=s)$ is assumed known.
So for each $y\in\cY$, $z\in f(\cS)$, $g\in\cG$, and $x\in\cX$, this relation is a linear system in unknown parameters $\Pr(Y=y\mid R=r, f(S)=s, G=g, X=x)$.
These parameters are identified if and only if this system has a unique solution, i.e. if the coefficient matrix $\vb P$ has rank $|\cR|$ and so does the augmented matrix $\mqty(\vb P&\vb b)$.
\end{proof}


# Additional Small-area Accuracy Evaluation {#app:small}

We evaluate the small-area estimates with two additional measures.
First, we calculate the root-mean-square error (RMSE) of the estimated conditional probabilities by race within each geographic area, and then average this across all geographic areas.
This captures the overall accuracy of the estimates.
Second, to measure how well each method captures relative differences between geographic areas, we calculate the correlation between the estimated and true conditional probabilities across all geographic areas by race.
As in the main text, we remove area-race cells with fewer than 5 voters.
A set of estimates which uniformly underestimates the proportion of Black voters which are registered Democrats, but which otherwise correctly orders geographic areas according to their proportion of Black Democrats, will score high on the correlation measure but also higher in RMSE.

Figure \ref{fig:nc-smallarea-app} summarizes our results, which closely track the findings of Section \ref{sec:small}.
The BIRDiE models are more accurate at all geographic levels and for both Black and White voters.
The weighting estimator performs the worst of all the methods.

```{r nc-smallarea-app, fig.pos="htb"}
#| fig.cap: |
#|     Accuracy of small-area estimates by race, as measureed by 
#|     root-mean-square error (RMSE; lower is better) and the 
#|     correlation between the estimates and ground truth (higher is better).
#| fig.scap: Alternate measures of accuracy of small-area estimates by race
knitr::include_graphics("figures/nc_smallarea_app.pdf")
```

# Sensitivity Analysis {#app:sens}

## Local sensitivity analysis

In this section, we develop a sensitivity analysis that assesses how the bias in BISG race probabilities affect the estimates of racial disparities.
In particular, we consider a setting where Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc} may be violated but Assumption \ref{a:indep-ys} still holds.
For example, consider the existence of unobserved confounder that affects some or all of the variables except the outcome, i.e., $(R,S,G,X)$.
This leads to the violation of Assumption \ref{a:indep-sgx}, but Assumption \ref{a:indep-ys} continues to be satisfied so long as such unobserved confounder does not affect the outcome.
Unfortunately, even inaccurate BISG predictions can still lead to biased estimates of racial disparities.

Specifically, if either the Census data are inaccurate, or the conditional independence relation does not hold $S\notindep G,X\mid R$, then the BISG predictions $\hat{\vb P}$ will differ from the "true" individual race probabilities $\vb P^*=\Pr(R\mid G, X, S)$.
Our goal is to quantify how an error in these probabilities $\vb P^* - \hat{\vb P}$ shifts the posterior and hence the estimates of racial disparities.

Denote by $\pi_{\vb*\delta}$ the posterior constructed using the error-corrected BISG race probabilities $\hat{\vb P}_i+\vb*\delta_i$ as the input probabilities for the model (see Equation \eqref{eq:posterior}), where $\pi_{\vb*\delta^*}$ is the true posterior with $\vb*\delta_i^* \dfeq \vb P^*_i - \hat{\vb P}_i$.
Estimating how $\pi$ and $\pi_{\vb*\delta}$ differ in general is difficult, but we focus on the settings where $\vb*\delta$ is small enough to make a linear approximation appropriate.
In sum, we aim to quantify how the small error in BISG probabilities can alter the estimates of racial disparities.

For clarity, in this section we will use $\vb*\theta_{r G_iX_iY_i}$ to denote the model parameter or function thereof that represents $\pi(Y_i\mid R_i=r, G_i, X_i)$.
This mirrors the notation of most of the specific models discussed in Section \@ref(sec:birdie) above.
Then define the following perturbation weight, which represents the ratio of posterior based on the biased and error-corrected BISG race probabilities: $$
    w(\Theta,\vb*\delta^*) \dfeq 
    \prod_{i=1}^N 
    \qty(1 + \frac{\vb*\theta_{\cdot G_iX_iY_i}^\top \vb*\delta_i^*}{
    \vb*\theta_{\cdot G_iX_iY_i}^\top \hat{\vb P}_i})
    \propto \frac{\pi_{\vb*\delta^*}(\Theta\mid\vb Y,\vb G,\vb X,\vb S)}{\pi(\Theta\mid\vb Y,\vb G,\vb X,\vb S)},
$$ Then, using a local linear approximation, we write the bias for a particular quantity of interest $g(\Theta)$ as \begin{align*}
    \E_{\pi_{\vb*\delta^*}}[g(\Theta)] - \E_\pi[g(\Theta)]
    &= \eval{\dv{\E_{\pi_{\vb*\delta}}[g(\Theta)]}{\vb*\delta}}_{\vb*\delta=0}^\top
        \vb*\delta^* + o(\norm{\vb*\delta^*}) \\
    &= \Cov_\pi\qty(g(\Theta), 
        \eval{\dv{\log w(\Theta,\vb*\delta)}{\vb*\delta}}_{\vb*\delta=0})^\top
        \vb*\delta^* + o(\norm{\vb*\delta^*}),
        \numberthis\label{eq:bias-dv}
\end{align*} where the second equality is due to Theorem 2.1 of [@giordano2018cov; see also the idea of *local sensitivity* from @gustafson1996local].

With this representation, we can bound the total error in $\E_\pi[g(\Theta)]$ for sufficiently small $\vb*\delta$ as the following theorem shows.

\begin{theorem}[Bias Bound] \label{thm:bound}
Define $\tilde\vartheta_{ir}\dfeq 
\frac{\theta_{rG_iX_iY_i}}{\theta_{\cdot G_iX_iY_i}^\top\hat{\vb P}_i}$.
Then for any input probabilities with total error $\norm{\vb*\delta^*}^2
=\sum_{i=1}^n\norm{\vb*\delta_i^*}^2\le \Delta^2$,
\begin{equation} \label{eq:covbound}
    |\E_{\pi^*}[g(\Theta)] - \E_\pi[g(\Theta)]|
    \lesssim \Delta\norm{\Cov_\pi(g(\Theta), \tilde\vartheta)},
\end{equation}
as $\Delta\to 0$.
\end{theorem}
\begin{proof}
This is immediate from \eqref{eq:bias-dv} once we compute 
\begin{align*}
    \dv{\log w(\Theta,\vb*\delta)}{\delta_{ir}} 
    &= \dv{\delta_{ir}} \sum_{i=1}^N 
        \log(1+\frac{\vb*\theta_{\cdot G_iX_iY_i}^\top \vb*\delta_i}{
        \vb*\theta_{\cdot G_iX_iY_i}^\top \hat{\vb P}_i}) \\
    &= \dv{\delta_{ir}}
        \log(1+\frac{\vb*\theta_{\cdot G_iX_iY_i}^\top \vb*\delta_i}{
        \vb*\theta_{\cdot G_iX_iY_i}^\top \hat{\vb P}_i}) \\
    &= \frac{1}{1+\frac{\vb*\theta_{\cdot G_iX_iY_i}^\top \vb*\delta_i}{
        \vb*\theta_{\cdot G_iX_iY_i}^\top \hat{\vb P}_i}} \times
        \frac{\vb*\theta_{rG_iX_iY_i}}{
        \vb*\theta_{\cdot G_iX_iY_i}^\top \hat{\vb P}_i} \\
    &= \frac{\vb*\theta_{rG_iX_iY_i}}{
        \vb*\theta_{\cdot G_iX_iY_i}^\top(\hat{\vb P}_i+\vb*\delta_i)}
\end{align*}
and evaluate at $\vb*\delta=0$, since the worst-case bias for a fixed total error can be obtained by having the maximum allowable $\vb*\delta$ point in the direction of the gradient of $\log w(\Theta,\vb*\delta)$.
\end{proof}

The theorem shows that once researchers choose the amount of total error $\Delta$, then the bound on the shift in a quantity of interest can be computed readily from posterior draws.
It is important to note that both $\vb*\delta^*$ and $\Cov_\pi(g(\Theta), \tilde\vartheta)$ are vectors whose dimension depends on the sample size $N$.
Thus, all else being equal, their norms will each grow as $\sqrt{N}$.
However, each entry $\Cov_\pi(g(\Theta), \tilde\vartheta_{ir})$ will tend to shrink as $N$ increases, since each observation exerts less leverage on the overall posterior.
Thus the overall impact of the sample size on the bound in Equation \eqref{eq:covbound} may depend on specific features of the data.
In particular, and as should be expected, the error is not guaranteed to vanish as $N\to\infty$.
Practitioners should evaluate Equation \eqref{eq:covbound} under a range of plausible $\Delta$ to understand how robust their findings are to worst-case linear violations of Assumptions $\ref{a:indep-sgx}$ and $\ref{a:cens-acc}$.

## OLS sensitivity analysis

For a different understanding of the effect of a particular $\vb\delta$, we can derive a result on the error in conditional probability estimates under the OLS estimator and particular configurations of $\vb*\delta$.
Unlike Theorem \ref{thm:bound}, this result holds across all sizes of $\vb*\delta$, and not just asymptotically as $\norm{\vb*\delta}\to 0$.
However, it applies to the OLS estimator, which, while unbiased, we do not recommend in practice.
Despite this difference, we expect many of the qualitative conclusions to hold for BIRDiE models.
The effect of any particular $\vb*\delta$ can of course be calculated directly by re-fitting the model to new race probabilities.

Here, we will work with a fixed $y\in\cY$ and among the subset of individuals with a particular $g\in\cG$ and $x\in\cX$.
Then for notational simplicity we let $\hat{\vb*\mu}^{\text{(ols)}}$ be the  vector of estimates of $\Pr(Y=y\mid R, G=g, X=x)$, and $\vb*\mu$ the corresponding true probabilities.
Similarly, we write $\hat{\vb P}$ for the matrix of individual race probability estimates for the subset of individuals with $g\in\cG$ and $x\in\cX$; elsewhere in the text this would be notated $\hat{\vb P}_{\cI(xg)}$ 

\begin{prop}[OLS Bias from incorrect $\hat{\vb P}$] \label{prop:ols-bias}
Let Assumption \ref{a:indep-ys} hold.
If the OLS estimator $\hat{\vb*\mu}^{\text{(ols)}}$ is 
calculated using race probabilities $\hat{\vb P}$ which differ from the true 
probabilities $\vb P^*=\hat{\vb P}+\vb*\delta$, then its bias satisfies \[
    \E[\hat{\vb*\mu}^{\text{(ols)}}]  - \vb*\mu
    = (\hat{\vb P}^\top \hat{\vb P})^{-1}
    \hat{\vb P}^\top \vb*\delta \vb*\mu
\]
\end{prop}
\begin{proof}
We can write the OLS estimate as \[
    \hat{\vb*\mu}^{\text{(ols)}} 
    = (\hat{\vb P}^\top \hat{\vb P})^{-1} \hat{\vb P}^\top \ind{\vb Y=y}.
\]
As shown in Theorem \ref{thm:ols-unb}, under Assumption \ref{a:indep-ys}, 
$\Pr(Y=y\mid R=r,G=g,X=x)$ is linear in the true $\vb P^*$.
Thus letting $\eps=\ind{\vb Y=y}-\Pr(Y=y\mid R=r,G=g,X=x)$, we can substitute and find
\begin{align*}
    \hat{\vb*\mu}^{\text{(ols)}} 
    &= (\hat{\vb P}^\top \hat{\vb P})^{-1} \hat{\vb P}^\top 
    \qty(\vb P^*\vb*\mu+\eps) \\
    &= (\hat{\vb P}^\top \hat{\vb P})^{-1} \hat{\vb P}^\top 
    \qty((\hat{\vb P}+\vb*\delta)\vb*\mu+\eps).
\end{align*}
Taking an expectation, since $\E[\eps]=0$ we find
\begin{align*}
    \E[\hat{\vb*\mu}^{\text{(ols)}}]
    &= (\hat{\vb P}^\top \hat{\vb P})^{-1} \hat{\vb P}^\top 
    \qty((\hat{\vb P}+\vb*\delta)\vb*\mu) \\
    &= \vb*\mu + (\hat{\vb P}^\top \hat{\vb P})^{-1} \hat{\vb P}^\top 
    \qty(\vb*\delta\vb*\mu);
\end{align*}
rearrangement yields the result.
\end{proof}

Informally, for BISG error $\vb*\delta$ to cause problems with the OLS estimate, two things must happen.
First, within individuals it must be "correlated" (i.e., have nonzero inner product) with the true conditional probabilities $\vb*\mu$.
Since $\vb*\delta_i$ must always sum to zero, practically, this means that  positive BISG errors must tend to occur in racial groups which have a relatively high occurrence of outcome $y$: $\Pr(Y=y\mid R=r,G=g,X-x) > \Pr(Y=y\mid G=g,X-x)$.
Second, the vector $\vb*\delta\vb*\mu$ (where each entry measures this "correlation" between errors and relative frequencies) must be correlated the BISG probabilities themselves $\hat{\vb P}$.
For example, if $\vb*\delta_i\vb*\mu$ is positive and tends to be larger for individuals with a high BISG probability of being Hispanic, then the overall OLS estimator the conditional probability of $Y=y$ among Hispanics will be biased upwards.

While Proposition \ref{prop:ols-bias} applies within a $(G,X)$ cell, if the same conditions hold across all $(G,X)$ combinations, then the overall poststratified estimator will be similarly biased.


# Surname Groupings for North Carolina Robustness Analysis {#app:surgrp}

We classify every surname in the voter file into one of nine groups, each containing surnames from one or more of 22 surname groups that we provide in the replication data and software.
These groups are organized mainly around different regions of the world and different waves of immigration to the United States.
To create the surname groups, each individual in the 1930 Census data was classified into one of the 22 groups.
Then among the set of individuals with each surname, the group with the highest number of individuals relative to the whole population was assigned to that surname.
For example, while most people named "Smith" fall into the Anglosphere group (containing 3rd or more generation White U.S. residents as of 1930, as well as immigrants from the U.K., Canada, Australia, etc.), there are relatively more Smiths among Black people than any other of the 22 groups.
Thus "Smith" is assigned to the Black surname group.
The full code for creating the 22 surname groupings from the 1930 Census data is available in the replication materials.

Because of the demographics of the United States, as well as limitations of the source data there is more geographic specificity in the surname groupings for some regions (e.g., Europe) than for others (e.g., South America and Africa).
We collapse the 22 surname groups to nine for the robustness analysis in Section \ref{sec:valid} based on the demographics of North Carolina specifically and to minimize the computational burden of performing the robust  analysis.

The 50 roughly most frequent surnames in each group, along with a brief description of the group, are listed below.
We stress that for the purposes of sensitivity analysis, the surname groups need only be correlated with countries of origin and racial subgroups.
Perfect alignment is neither possible nor necessary.

```{r}
surn = readRDS(here("paper/data/nm_grps.rds")) |> 
    split(~ nm_grp)

nm_tbl <- function(key) {
    surn[[key]]$last_name |> 
        paste0(1:50, ". ", nm=_) |> 
        matrix(ncol=5) |> 
        kable(format="latex", booktabs=TRUE, linesep="", toprule="", bottomrule="")
}
```

#### Anlglosphere and Black surname group.
Surnames which are relatively more prevalent among 3rd-or-more generation White
U.S. residents and Black U.S. residents in 1930.

\nopagebreak 
```{r}
nm_tbl("anglobl")
```

#### First wave European immigration surname group. 
Surnames associated with German, Nordic, and Irish immigrants.

\nopagebreak 
```{r}
nm_tbl("eur_wave1")
```

#### Second wave European immigration surname group.
Surnames associated with Eastern European, Italian, Jewish,
Russian, Greek, and other Southern European immigrants.

\nopagebreak 
```{r}
nm_tbl("eur_wave2")
```

#### East Asian surname group.
Surnames associated with Chinese, Japanese, and Korean immigrants.

\nopagebreak 
```{r}
nm_tbl("easia")
```

#### South Asian surname group.
Surnames associated with Indian and Southwest Asian immigrants.

\nopagebreak 
```{r}
nm_tbl("sasia")
```

#### Southeast Asian and Pacific surname group.
Surnames associated with Southeast Asian and Pacific Islander immigrants,
including Vietnamese and Filipino immigrants.
   
\nopagebreak 
```{r}
nm_tbl("seasiapac")
```
                     
#### Non-Cuban Hispanic surname group.
Surnames associated with Mexian and Latin American immigrants,
not including Cuban immigrants, and Puerto Rican residents.

\nopagebreak 
```{r}
nm_tbl("hisp")
```

#### Cuban surname group.
Surnames associated with Cuban immigrants.

\nopagebreak 
```{r}
nm_tbl("cuba")
```

#### "Other" surname group.
Surnames not associated with one of the other categories, including those associated with later Western European immigration, Middle Eastern & North African-associated surnames, Native-associated surnames and Afro-Carribean-associated surnames.

\nopagebreak 
```{r}
nm_tbl("other")
```
