---
title: "Estimating Racial Disparities when Race is Not Observed, with Application to U.S. Tax Claims Data"
shorttitle: "Estimating Racial Disparities when Race is Not Observed"
authors: 
    - name: Cory McCartan
      department: Department of Statistics
      affiliation: Harvard University
    - name: Robin Fisher
      department: Office of Tax Analysis
      affiliation: U.S. Department of the Treasury
    - name: Jacob Goldin
      department: Sanford Law School
    - name: Daniel E. Ho
      department: Stanford Law School
    - name: Kosuke Imai
      department: Department of Government and Department of Statistics
      affiliation: Harvard University
      thanks: "Corresponding author. Email: imai@harvard.edu, URL: https://imai.fas.harvard.edu.  We thank Bruce Willsie, CEO of L2, Inc., for providing us with the geocoded voter file we use in this paper."
date: '`r format(Sys.Date(), "%B %e, %Y")`'
abstract: |
    The estimation of racial disparities in health care, financial services, voting, and other contexts is often hampered by the lack of individual-level racial information in administrative records.
    In many cases, the law prohibits the collection of such information to prevent direct racial discrimination.
    As a result, many analysts have adopted Bayesian Improved Surname Geocoding (BISG), which combines individual names and addresses with the Census data to predict race.
    Although BISG tends to produce well-calibrated racial predictions, its prediction errors are often correlated with the outcomes of interest, yielding biased estimates of racial disparities.
    We propose a more credible identification strategy that corrects this bias.
    The proposed strategy is applicable whenever one's surname is conditionally independent of the outcome given their (unobserved) race, residence location, and other observed characteristics.
    Leveraging this identification strategy, we introduce a new model, Bayesian Instrumental Regression for Disparity Estimation (BIRDiE), that estimates racial disparity while accounting for the high-dimensionality of surnames.
    We also develop a sensitivity analysis to address the potential violation of the required assumption that BISG's race predictions are well calibrated.
    Our validation study based on North Carolina voter files shows that BIRDiE reduces error by about 75--85% in comparison to existing approaches in estimating party identification by race, and roughtly 25â€“50% lower error for turnout rates by race.
    A similarly substantial improvement is obtained in estimating racial disparities of partisan identification.
    We also apply BIRDiE to large-scale U.S. tax records to produce the first-ever estimates of claim rates of the home mortgage interest deduction by race.
    We find that ... Open-source software is available for applying BIRDiE in practice.
keywords: 
    - BISG
    - ecological inference
    - proxy variable
    - race imputation
    - sensitivity analysis
bibliography: references.bib
biblio-style: apalike
output:
    bookdown::pdf_document2:
        template: "template.tex"
        number_sections: true
        keep_tex: true
        includes: 
            in_header: "header.tex"
        latex_engine: pdflatex
        citation_package: natbib
editor_options: 
    markdown: 
        wrap: sentence
---

```{r setup, include=FALSE}
library(knitr)
library(here)
library(scales)

opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE,
               fig.path=here("paper/figures/"), fig.align="center",
               fig.width=(8.5-2*1)/2, out.width="100%", fig.asp=0.8)
options(bookdown.theorem.preamble = FALSE)

x = readRDS(here("paper/data/nc_disp_ex.rds"))
```

# Introduction

The identification and estimation of racial disparities is of paramount importance to researchers, policymakers and organizations in a variety of areas including public health [@van2003paved; @williams2005social], employment [@conway1983reverse; @greene1984reverse], voting [@gay2001effect; @hajnal2005turnout; @barreto2007isi], criminal justice [@berk2021fairness; @chouldechova2017fair; @dressel2018accuracy], taxation [@black2022algorithmic], housing [@kermani2021racial], and lending [@chen2018fair].
Within the U.S. government, efforts to identify and remedy racial disparities have taken on greater urgency with the recent issuance of Executive Order 13985, which in part directs agencies to conduct equity assessments by developing appropriate methodology.

In many of these areas, however, racial information is not available at the individual level.
The unavailability of individual racial information makes it impossible for analysts to simply tabulate variables of interest against race to identify disparities among different racial groups.
In fact, in some areas, the law explicitly prohibits the collection of racial information even as it demands fair treatment on the basis of race (see, e.g. the U.S. Equal Credit Opportunity Act).
This creates a dilemma for organizations who wish to measure possible disparities in order to monitor the fairness of their decision-making or service provision.

To estimate racial disparities without individual-level racial data, some researchers have turned to ecological inference methods [@goodman1953ecological; @king1997solution; @king2004ecological; @wakefield2004ecological; @imai2008bayesian; @greiner2009r].
These methods, however, require strong assumptions, which can be difficult to verify and may provide misleading results [@cho2008ecological].
Additionally, they all rely on accurate marginal information about race, which may not always be available.

Where the analysis of racial disparities involves large-scale administrative data, many analysts have adopted Bayesian Improved Surname Geocoding (BISG), which generates individual probabilities of belonging to different racial groups based on last names and geolocation [@fiscella2006bisg; @elliott2008bisg].
BISG leverages residential racial segregation and the hereditary association between race and surname to produce generally accurate and calibrated predictions of individual race [@imai2016improving; @kenny2021das; @deluca2022validating].

Unfortunately, the accuracy BISG racial predictions alone is not sufficient for estimating disparities without bias.
Indeed, the most common techniques for estimating disparities using BISG probabilities are known to be biased when prediction errors are correlated with the outcome variable [@chen2019fairness].
As formally discussed in Section \@ref(sec:id), the standard methodology of racial disparity estimation based on BISG predictions also requires individual's race to be conditionally independent of the outcome given their residence location, surnames, and other observable attributes.
This key identification assumption, however, is unlikely to hold because race affects many aspects of today's society even after accounting for residence location, surnames, and other observable attributes.

To address this challenge, in Section \@ref(sec:est), we propose an alternative identification strategy.
Specifically, we assume that the outcome is conditionally independent of surname given (unobserved) individual's race, residence location, and other observed attributes.
The assumption implies that for two individuals who live in the same area, belong to the same racial group, and share the observable attributes, their surnames have no predictive power of the outcome.
We argue that this new identification assumption is more credible unless surname is directly used to determine the outcome of interest (i.e., name-based discrimination).
Based on this identification strategy, we develop a Bayesian model that overcomes the high-dimensionality of surnames to accurately estimate racial disparities.
We derive a stochastic variational inference algorithm to fit the model to large administrative datasets.

In large datasets, sampling-based uncertainty is overwhelmed by error that arises from violations of our assumptions about the data.
This is particularly true in this context where the key variable of interest is unobserved.
Section \@ref(sec:sens) proposes a method to evaluate the sensitivity of estimates about racial disparities to the potential violation of model assumptions.
All of the proposed methodology is implemented in open-source software that is made available with the paper.

In Section \@ref(sec:valid), we validate the proposed methodology using real-world data taken from the voter file in North Carolina, where self-reported individual-race is observed and can be used to construct the ground-truth of racial disparities.
The proposed model substantially outperforms existing estimators across different error measures, two different outcome variables, and multiple levels of geolocation specificity.
For example, the most popular existing BISG disparity estimator pegs the gap at Democratic party registration between White and Black voters at `r round(-100*x$weight$disp_wb)` percentage points, while the actual gap is `r round(-100*x$weight$disp_wb_true)` percentage points---more than double.
The proposed method yields an estimate of `r round(-100*x$model$disp_wb)` percentage points.

In Section \@ref(sec:appl), we apply the proposed method to large-scale administrative data from the U.S. Internal Revenue Service.
We produce the first-ever estimates of claim rates of the home mortgage interest deduction (HMID) by race.... [fill in]. **KI: I would like to introduce this application in Section 2 as a motivating example.  Then, we can come back to its analysis in Section \@ref(sec:appl).**
Section \@ref(sec:disc) gives concluding remarks.

# Bias of the Standard Methodology {#sec:id}

In this section, we review the assumptions of the standard BISG-based methodology for estimating racial disparities when individual race is not observed.
We show that the racial disparity estimates based on the standard methodology are biased unless the outcome variable is independent of race given surnames, residence location, and other observed covariates.
We argue that this assumption is likely to be violated given the significant role race plays in many aspects of our society.

## Setup and BISG Procedure

Suppose that we have an i.i.d. sample of $N$ individuals from a population of infinite size.
For each individual $i=1,2,\ldots,N$, we define a tuple $(Y_i, R_i, G_i, X_i, S_i)$, where $Y_i\in\cY$ is the outcome of interest for individual $i$, $R_i\in \cR$ is the (unobserved) race of the individual, $G_i\in\cG$ is the (geo)location of the individual's residence, $X_i\in\cX$ are other observed characteristics, and $S_i\in\cS$ is the individual's surname.
When we are not referring to a particular individual, we will drop the subscripts.
Note that individual's race is unobservable but all other variables are observed.
The availability of particular (or any) $X$ is not required for either the standard or proposed methodology.

We assume throughout that these variables are discrete, taking a finite set of values, i.e., $|\cY|=C_Y$, $|\cR|=C_R$, $|\cG|=C_G$, $|\cX|=C_X$, and $|\cS|=C_S$ where $C_Y$, $C_R$, $C_G$, $C_X$, and $C_S$ are constants.
Note that typically $S$ is high-dimensional as there exist a large number of unique names.
In practice, residence location $G$ is also discrete, since joint information about location, race, and other variables is only available down to the Census block level.
For the sake of simplicity, we assume that the outcome variable $Y$ is also discrete, though it is possible to extend the standard and proposed methodologies to continuous outcome variables.

BISG relies on data from the decennial Census or the American Community Survey (ACS), which provide information on the joint distribution of $R$ and $G$ (and any other covariates $X$, such as gender or age).
It then combines this information with data from the Census Bureau's surname tables [@censusnames], which provide information on the joint distribution of $R$ and $S$.
We summarize this set of information from the Census by two conditional probabilities, $\vb q_{GX|R}$ and $\vb q_{S|R}$, and one marginal probability, $\vb q_R = P(R)$.

The BISG estimator of the probability that individual $i$ belongs to race $r \in \cR$ can then be written as [@fiscella2006bisg; @elliott2008bisg] \begin{equation} \label{eq:pr-bisg}
    \hat{P}_{ir} \dfeq \frac{q_{G_iX_i|r}\, q_{S_i|r}\, q_r}{
        \sum_{j\in\cR}q_{G_iX_i|j}\, q_{S_i|j}\, q_j},
\end{equation} where, for example, $q_{G_iX_i|r}$ indicates the estimated conditional probability of residence location $G_i$ and covariates $X_i$ given race $r$, taken from the Census table $\vb q_{GX|R}$.

The BISG estimator implicitly relies on two key assumptions.
The first is the following conditional independence relation between an individual's surname and residence location (as well as other characteristics) given their unobserved race.

```{=tex}
\begin{assump}[Conditional independence of name and other proxy variables] \label{a:indep-sgx}
    For all $i$, $$S_i \indep \{G_i, X_i\}\mid R_i.$$
\end{assump}
```
Assumption \ref{a:indep-sgx} implies, for example, that once we know an individual is White, knowing their surname is Smith tells us nothing about their residence location and other observed characteristics.
Although this assumption appears to be reasonable, the lack of granularity in the coding of race may lead to its violation.
For example, people with Chinese, Indian, Filipino, Vietnamese, Korean, or Japanese are all coded as one racial group "Asian" in the census.
These groups, however, have varying sets of surnames and have different demographic and geographic distributions.
For instance, unlike the Smith example, knowing that an Asian individual's surname is Gupta makes it much more likely that they have a higher income and live in the Eastern U.S [@budiman2019key].
<!--# Nevertheless, Assumption \ref{a:indep-sgx} appears to approximately hold in practice for White, Black, and Hispanic individuals, which make up a substantial majority of the U.S. population. -->

The second assumption required by BISG is that the Census tables reflect the true population distributions of $R$, $S$, $G$, and $X$.

```{=tex}
\begin{assump}[Data accuracy] \label{a:cens-acc}
    For all $i$, 
    \begin{align*}
        \Pr(R_i=r) &= q_r \\
        \Pr(S_i=s\mid R_i) &= q_{s|r} \\
        \Pr(G_i=g, X_i=x\mid R_i) &= q_{gx|r}
    \end{align*}
\end{assump}
```
Like Assumption \ref{a:indep-sgx}, Assumption \ref{a:cens-acc} never holds exactly in practice.
Despite the best efforts of the Census Bureau, the decennial census has intrinsic error, including undercounting minority racial groups [@census2022undercount; @censuscount; @racecounts], as well as error introduced by privacy-preserving mechanisms [@abowd2020].
And because of births, deaths, and moves, census data are often out-of-date from the moment of publication.
These errors have led further extensions of the BISG estimator to account for some of this measurement error [@imai2022addressing], which can help with accuracy for smaller racial groups.

Even though Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc} are never perfectly met, researchers find that BISG produces accurate and generally well-calibrated estimates in practice [@imai2016improving; @kenny2021das; @deluca2022validating].

Under Assumption \ref{a:indep-sgx}, by Bayes' Rule, $$
    \Pr(R_i=r\mid G_i,X_i,S_i)
    \propto \Pr(G_i, X_i\mid R_i=r)\Pr(S_i\mid R_i=r)\Pr(R_i=r).
$$ 
This justifies the estimator given in \eqref{eq:pr-bisg}, and provides us with the following immediate result.

```{=tex}
\begin{prop}[Accuracy of BISG] \label{p:bisg}
Under Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc}, the BISG estimator is accurate. That is, we have
$\hat P_{ir} = \Pr(R_i=r\mid G_i,X_i,S_i)$.
\end{prop}
```
## Bias of Racial Disparity Estimates based on BISG Probabilities

Unfortunately, accurate and calibrated estimates of individual race predictions alone are not sufficient for identifying racial disparities.
The reason is that the prediction error of race probabilities may be correlated with the outcome variable of interest.
Specifically, we consider the following weighting estimator, which attempts to capture the uncertainty inherent in race prediction and is most commonly used by researchers,[^1] $$
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid r) 
        = \frac{\sum_{i=1}^N \ind\{Y_i=y\}\hat P_{ir}}{
            \sum_{i=1}^N \hat P_{ir}}.
$$

[^1]: An alternative is a classification estimator, which deterministically assigns individuals to a predicted racial category based on the BISG estimates $\vb{\hat P}_i$ (either the largest $\hat P_{ir}$ or the one which exceeds a predetermined threshold).
    Unlike the weighting estimator, this classification estimator does not take into account prediction uncertainty.

\citet{chen2019fairness} show that the asymptotic bias of this weighting estimator is controlled by the residual correlation of $Y$ and $R$ after controlling for $G$, $X$, and $S$.
We reproduce this result here.

```{=tex}
\begin{theorem}[Theorem 3.1 of \citealt{chen2019fairness}]
If race is binary (so $\cR=\{0,1\}$), then as $N\to\infty$, \[
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid r) - \Pr(Y=y\mid R=r)
    \cvas -\frac{\E[\Cov(\ind\{Y=y\}, \ind\{R=r\}\mid G,X,S)]}{\Pr(R=r)}.
\]
\end{theorem}
```
This result implies that the conditional independence between individual's race and outcome given their surname, residence location, and other characteristics is sufficient to eliminate the bias of the weighting estimator.

```{=tex}
\begin{assump}[Conditional independence of outcome and race] \label{a:indep-yr}
For all $i$, $$Y_i\indep R_i\mid G_i,X_i,S_i.$$
\end{assump}
```

Figure \@ref(fig:dag)(a) shows a causal directed asyclic graph (DAG) that satisfies Assumption \ref{a:indep-yr} as well as Assumption \ref{a:indep-sgx}.
This causal structure implies that $Y\indep R\mid G,X,S$ because all paths from $R$ to $Y$ are blocked by $G$, $X$, or $S$.
The key causal assumption of this DAG is that the effect of race $R$ on the outcome $Y$ must be entirely mediated by surname $S$, residence location $G$, and other observed characteristics $X$.
This type of exclusion restriction is difficult to satisfy in many practical settings because race may affect the outcome through so many factors, biasing the weighting estimator.

But in other settings, Assumption \ref{a:indep-yr} may be more plausible.
For example, for a manager reviewing job applications on paper, race is typically unobserved, but the manager may be influenced by racial or gender cues in a candidate's name or address [@park2009names; @aaslund2012names].
So long as we observe all information used by the manager and use it in the BISG estimation, the weighting estimator would give an asymptotically unbiased estimate.
Outside of these cases, however, the weighting estimator is likely to be biased.

```{=tex}
\begin{figure}[ht]
\begin{center}
\begin{subfigure}[b]{0.4\textwidth}
\centering
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.6cm,thick]
    
        \node[main node] (G) {$G$};
        \node[main node] (Y) [right of=G] {$Y$};
        \node[main node] (S) [below left=1.3cm and 1.3cm of Y] {$S$};
        \node[main node] (R) [below left=0.8cm and 0.8cm of S] {$R$};
        \node[main node] (X) [below of=Y] {$X$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (X)
        (S) edge node  {} (Y)
        (R) edge node  {} (S)
        (G) edge node  {} (X)
        (G) edge node  {} (Y)
        (X) edge node  {} (Y);
    \end{tikzpicture}
    \caption{Assumption \ref{a:indep-yr}: Conditional independence of $Y$ and $R$ given $(G, S, X)$}
\end{subfigure}
\hspace{1em}
\begin{subfigure}[b]{0.4\textwidth}
\centering
    \tikzstyle{main node}=[circle,draw,font=\sffamily\Large\bfseries]
    \tikzstyle{sub node}=[circle,draw,dashed,font=\sffamily\Large\bfseries]
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.5cm,thick]
    
        \node[main node] (G) {$G$};
        \node[main node] (R) [below of=G] {$R$};
        \node[main node] (S) [below left=0.5cm and 0.5cm of R] {$S$};
        \node[main node] (Y) [right of=G] {$Y$};
        \node[main node] (X) [below of=Y] {$X$};
        
        \path[every node/.style={font=\sffamily\small}]
        (R) edge node  {} (G)
        (R) edge node  {} (X)
        (R) edge node  {} (Y)
        (R) edge node  {} (S)
        (G) edge node  {} (X)
        (G) edge node  {} (Y)
        (X) edge node  {} (Y);
        
        %\path[every node/.style={font=\sffamily\small}, <->]
        %(G) edge [dashed, bend right] node  {} (S)
        %(X) edge [dashed, bend left] node  {} (S);
    \end{tikzpicture}
    \caption{Assumption \ref{a:indep-ys}: Conditional independence of $Y$ and $S$ given $(G, R, X)$}
\end{subfigure}
\end{center}

\caption{Possible causal structures for which each of the labeled assumptions is satisfied, represented as a directed acyclic graph (DAG) where $G$ is residence location, $R$ is race, $S$ is surname, $X$ is observed covariates, and $Y$ is the outcome. Both DAGs also satisfy Assumption \ref{a:indep-sgx}: Conditional independence of $S$ and $(G,X)$ given $R$.}
\label{fig:dag}
\end{figure}
```

# The Proposed Methodology {#sec:est}

In this section, we propose an alternative identification strategy.
Specifically, we show that racial disparity is identifiable if one's surname is conditionally independent of the outcome given their race, residence geolocation, and other observed information, assuming that BISG can accurately identify race probabilities.
We then develop a statistical model that estimates racial disparity under this identification condition while accounting for the high-dimensionality of surnames.
Finally, we propose a sensitivity analysis that addresses the potential existence of unobserved confounders.

## Identification Strategy

To reduce the potential bias of the weighting estimator, we propose an alternative identification assumption that may be applicable when Assumption \ref{a:indep-yr} is not credible.
Specifically, we assume that surname, rather than race, satisfies the exclusion restriction conditional on (unobserved) race, residence location, and other observed characteristics.

```{=tex}
\begin{assump}[Conditional independence of outcome and name] \label{a:indep-ys}
For all $i$, $$Y_i\indep S_i\mid R_i,G_i,X_i.$$
\end{assump}
```

Figure \@ref(fig:dag)(b) shows one possible causal DAG that meets this assumption as well as Assumption \ref{a:indep-sgx}.
In this DAG, race can have a direct effect on the outcome $Y$ as well as on residence location $G$ and other observed characteristics $X$, while all paths from $S$ to $Y$ are blocked by $G$, $X$, or $R$.

This causal structure is plausible in many real-world settings, such as our motivating example of HMID claims.
In this case, we can rule out a direct effect of name on the outcome.
(**CM: revisit this more carefully once we've settled on outcomes. Dan notes e.g. mortgage application success may depend on discrimination via names**.) 
This assumption rules out the possibility that surname directly affects the outcome whereas such a direct effect is allowed under Assumption \ref{a:indep-yr}.
In the above hiring example, if the manager directly relies upon surname to make the decision, our assumption will be violated.
Similarly, if campaigns use the surnames of individual voters to decide whether to mobilize them, Assumption \ref{a:indep-ys} will be violated.
Yet another possible violation of the assumption is the existence of unobserved confounder that affects both outcome and surname.
For example, the country of origin for an immigrant may represent such a confounder.
Even in these cases, however, conditioning on (unobserved) race is likely to reduce the magnitude of association between outcome and surnames.
In Section \ref{sec:sens}, we develop a sensitivity analysis for potential violation of this assumption.

The following theorem shows that it is possible to identify racial disparities under Assumption \ref{a:indep-ys}.

```{=tex}
\begin{theoremrep}[Identification] \label{thm:id}
For all $g\in\cG$, $x\in\cX$, and $y\in\cY$, define a matrix $\vb P$ with entries $p_{rs}=\Pr(R=r\mid G=g, X=x, S=s)$  and a vector $\vb b$ with entries $b_s=\Pr(Y=y\mid G=g, X=x, S=s)$.
Then under Assumption~\ref{a:indep-ys}, and assuming knowledge of the joint distribution $\Pr(R,G,X,S)$, the conditional probabilities $\Pr(Y=y\mid R, G=g, X=x)$ are identified if and only if both $\vb P$ and the augmented matrix $\mqty(\vb P&\vb b)$ have rank $|\cR|$.
\end{theoremrep}
\begin{proof}
Applying the law of total probability and our conditional independence relation $S\indep Y\mid R,G,X$, we have,
for all $y\in\cY$, $g\in\cG$, $x\in\cX$, and $s\in\cS$,
\begin{align*}
    \Pr(Y=y\mid G=g, X=x, S=s)
    &= \sum_{r\in\cR} \Pr(Y=y\mid R=r, G=g, X=x, S=s)\Pr(R=r\mid G=g, X=x, S=s) \\
    &= \sum_{r\in\cR} \Pr(Y=y\mid R=r, G=g, X=x)\Pr(R=r\mid G=g, X=x, S=s).
\end{align*}
The left-hand side is estimable from the data and the rightmost term $\Pr(R=r\mid G=g, X=x, S=s)$ is assumed known.
So for each $y\in\cY$, $g\in\cG$, and $x\in\cX$, this relation is a linear system in unknown parameters $\Pr(Y=y\mid R=r, G=g, X=x)$.
These parameters are identified if and only if this system has a unique solution, i.e. if the coefficient matrix $\vb P$ has rank $|\cR|$ and so does the augmented matrix $\mqty(\vb P&\vb b)$.
\end{proof}
```
The essence of this identification result is the following key observation.
Under Assumption \ref{a:indep-ys}, we have, for all $y\in\cY$, $g\in\cG$, $x\in\cX$, and $s\in\cS$, \begin{equation} \label{eq:tprob}
    \Pr(Y=y\mid G=g, X=x, S=s)
    = \sum_{r\in\cR} \Pr(Y=y\mid R=r, G=g, X=x)\Pr(R=r\mid G=g, X=x, S=s).
\end{equation} The leftmost term is estimable from the data and corresponds to the vector $\vb b$ in Theorem \ref{thm:id}, while the rightmost term is the BISG estimand and corresponds to the matrix $\vb P$ in Theorem \ref{thm:id}.
Lastly, the remaining term in the middle can be solved for, since \eqref{eq:tprob} holds across all combinations of $Y$, $G$, $X$, and $S$, leading to a large system of equations.
Our result is closely related to causal identification based on proxy variables in the presence of unmeasured confounding [@kuroki2014measurement; @miao2018identifying].
Here, we use surname as a proxy variable for (unobserved) race to identify racial disparities.

Together with Proposition \ref{p:bisg}, Theorem \ref{thm:id} imply that racial disparities can be identified under Assumptions \ref{a:indep-sgx}, \ref{a:cens-acc}, and \ref{a:indep-ys}.
The identifying equation \eqref{eq:tprob} shows that $\Pr(Y=y\mid G=g, X=x, S=s)$ is linear in the BISG estimands $\Pr(R=r\mid G=g, X=x, S=s)$.
Thus, it is natural to consider the following least-squares estimator of $\Pr(Y=y\mid R)$ under this alternative identification strategy, $$
\hat{\vb*\mu}^{(\text{ols})}_{Y\mid R}(y\mid \cdot) 
= (\hat{\vb P}^\top \hat{\vb P})^{-1} \hat{\vb P}\,\ind\{\vb Y = y\},
$$ where as above $\hat{\vb P}$ is the matrix of BISG probabilities.
<!-- and where $\vb Y_{gx}$ is the vector of outcome variables for the individuals with $G_i=g$ and $X_i=x$. --> Indeed, this estimator is unbiased, as the following theorem shows.
**CM: cite to Dan & Jacob's working paper around here.**

```{=tex}
\begin{theoremrep}[OLS Estimator Unbiased] \label{thm:ols-bias}
If Assumptions \ref{a:indep-sgx}, \ref{a:cens-acc}, and \ref{a:indep-ys} hold, 
and the identification conditions in Theorem~\ref{thm:id} are satisfied,
then for all $y\in\cY$ and $r\in\cR$, \[
    \E[\hat{\mu}^{(\text{ols})}_{Y\mid R}(y\mid r)] = \Pr(Y=y\mid R=r).
\]
\end{theoremrep}
\begin{proof}
Fix $y\in\cY$ and define $m_{gxr} = \E[\ind\{Y=y\}\mid R=r, G=g, X=x)]$.
Then under Assumptions \ref{a:indep-sgx}, \ref{a:cens-acc}, and \ref{a:indep-ys},
\begin{align*}
    \E[\ind\{Y=y\}\mid G, X, S] &= 
    \sum_{r\in\cR} \E[\ind\{Y=y\}\mid R=r, G,X)]\Pr(R=r\mid G,X,S) \\
    &= \sum_{r\in\cR} m_{GXr} \hat p_r,
\end{align*}
where as in the main text $\vb{\hat p}$ is the (random) vector of BISG probabilities.
Now, taking conditional expectations given $G$, $X$, and $\vb{\hat P}$, we can rewrite
the above equality as \[
    \E[\ind\{Y=y\}\mid G, X, \vb{\hat p}] = \sum_{r\in\cR} m_{GXr} \hat p_r
    = \sum_{g\in\cG}\sum_{x\in\cX}\sum_{r\in\cR} m_{gxr}\ind\{X=x,G=g\}\hat p_r,
\] since $\E[\ind\{Y=y\}\mid G, X, S]$ depends only on $S$ through $\vb{\hat p}$.
Then averaging over $G$ and $X$, we obtain 
\begin{align*}
    \E[\ind\{Y=y\}\mid \vb{\hat p}] = \E[\ind\{Y=y\}\mid G, X, \vb{\hat p}] 
    &= \sum_{g\in\cG}\sum_{x\in\cX}\sum_{r\in\cR} m_{gxr}\Pr(X=x,G=g\mid\vb{\hat p})\hat p_r \\
    &= \sum_{r\in\cR}\qty(\sum_{g\in\cG}\sum_{x\in\cX}m_{gxr}\Pr(X=x,G=g\mid\vb{\hat p}))\hat p_r \\
    &= \sum_{r\in\cR}\E[m_{GXr}\mid\vb{\hat p}]\hat p_r.
\end{align*}
So the conditional expectation of $\ind\{Y=y\}$ given the BISG probabilities $\vb{\hat p}$ is linear in those probabilities,
with coefficients $\vb*\beta$, defined as $\beta_r=\E[m_{GXr}\mid\vb{\hat p}]$.
Consequently, the OLS estimate $(\vb{\hat P}^\top\vb{\hat P})^{-1}\vb{\hat P}\ind\{\vb Y=y\}$ will be unbiased for $\vb*\beta$, by the standard results.
\end{proof}
```
It is worth comparing this OLS estimator with the weighting estimator $\hat{\mu}^{(\text{wtd})}_{y\mid r}$.
The next theorem shows that these two estimators are guaranteed to disagree, unless either the BISG probabilities perfectly discriminate or the weighting estimator is constant across races.
Unfortunately, these two conditions are almost never met in practice.
This underscores the importance of selecting the appropriate assumption (Assumption \ref{a:indep-yr} or \ref{a:indep-ys}) for a particular analysis, since they imply different estimators with different results.

```{=tex}
\begin{theoremrep}[Necessary and Sufficient Condition for Equality of the Weighting and OLS Estimators] \label{thm:wtd-vs-ols}
For any $y\in\cY$, $\hat{\vb*\mu}^{(\text{wtd})}_{Y|R}(y\mid\cdot) = \hat{\vb*\mu}^{(\text{ols})}_{Y|R}(y\mid\cdot)$
if and only if for every pair $j,k\in\cR$, either the BISG probabilities perfectly discriminate 
(i.e., $\Pr(R_i=j\mid G_i,X_i,S_i)>0$ implies $\Pr(R_i=k\mid G_i,X_i,S_i)=0$ and vice versa) or 
$\hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid j)=\hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid k)$.
\end{theoremrep}
\begin{proof}
Fix a $y\in\cY$.
The weighting estimator of $\Pr[Y=y\mid R=r]$ may be written $$
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid r) 
    = \frac{\hat{\vb P}_{\cdot r}^\top \ind\{\vb Y=y\}}{\hat{\vb P}_{\cdot r}^\top \vb 1}
    = \frac{\norm{\proj_{\hat{\vb P}_{\cdot r}}(\ind\{\vb Y=y\})}}{
        \norm{\proj_{\hat{\vb P}_{\cdot r}}(\vb 1)}},
$$ the ratio of the projected length of the outcome vector $\ind\{\vb Y=y\}$ and the constant vector $\vb 1$ onto $\hat{\vb P}_{\cdot r}$.
We can write the OLS estimator as $$
    \hat{\vb*\mu}^{(\text{ols})}_{Y|R} = (\hat{\vb P}^\top\hat{\vb P})^{-1}\hat{\vb P}^\top \ind\{\vb Y=y\}
    = \mathrm{coord}_{\hat{\vb P}}(\proj_{\hat{\vb P}}(\ind\{\vb Y=y\}),
$$ where $\mathrm{coord}_{\hat{\vb P}}$ is the function that returns the coordinates of its input vector in the $\hat{\vb P}$ basis (by assumption $\hat{\vb P}$ has rank $|\cR|$ and so its columns are linearly independent).
To make the comparison even easier, notice that we can break the projection $\proj_{\hat{\vb P}_{\cdot r}}$ into two steps, writing it instead as $\proj_{\hat{\vb P}_{\cdot r}} = \proj_{\hat{\vb P}_{\cdot r}} \circ \proj_{\hat{\vb P}}$.
Letting $\vb Y_{\proj}=\proj_{\hat{\vb P}}(\ind\{\vb Y=y\})$, then, we can rewrite our estimators as $$
    \hat\mu^{(\text{wtd})}_{Y|R}(y\mid r) 
    = \frac{\norm{\proj_{\hat{\vb P}_{\cdot r}}(\vb Y_\proj)}}{
        \norm{\proj_{\hat{\vb P}_{\cdot r}}(\vb 1)}}  \qand
    \hat{\vb\mu}^{(\text{ols})}_{Y|R}(y\mid r)
    = \mathrm{coord}_{\hat{\vb P}}(\vb Y_\proj)_r.
$$ 

Now, since the individual BISG probabilities are nonnegative and sum to 1, a pair $j,k\in\cR$ of races has perfectly discriminating BISG probabilities if and only if the corresponding columns of $\hat{\vb P}$ are orthogonal, i.e., $\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}$.
Begin by writing $\vb Y_\proj$ in terms of the $\hat{\vb P}$ basis, so \[
    \vb Y_\proj = \sum_{j\in\cR} c_j\hat{\vb P}_{\cdot j},
\] and thus $\hat{\vb\mu}^{(\text{ols})}_{Y|R}(y\mid j)=c_j$.
Without loss of generality, suppose the $c_j$ are numbered as $c_1\ge c_2\ge \cdots\ge c_{|\cR|}$.
We can also expand $\vb 1$ in the same basis. Since the individual probabilities must sum to one, in fact we have \(
    \vb 1 = \sum_{j\in\cR} \hat{\vb P}_{\cdot j}.
\)

For the forward direction, we assume $\hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid j)=\hat{\mu}^{(\text{ols})}_{Y|R}(y\mid j)=c_j$;
multiplying out the denominator of the weighting estimator, we have $\hat{\vb P}_{\cdot j}^\top \vb Y=c_j\hat{\vb P}_{\cdot j}^\top\vb 1$ for all $j$; substituting the basis expansions of $\vb Y_\proj$ and $\vb 1$, this yields \[
    \sum_{k\in\cR} c_k \hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}
    = \sum_{k\in\cR} c_j \hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}, \qq{so}
    \sum_{k\in\cR} (c_j-c_k) \hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}=0.
\] Now fix $j\in J_1=\argmax_j c_j$; this relation still holds, but now every term in the sum is nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1$.
Therefore we must have $\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}=0$ for all $k\not\in J_1$.
Then fix $j\in J_2=\argmax_{j\not\in J_1} c_j$; since $\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot l}=0$ for all $l\in J_1$, every term in the sum is still nonnegative and in particular $c_j>c_k$ for all $k\not\in J_1\cup J_2$.
Therefore we must have $\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}=0$ for all $k\not\in J_1\cup J_2$.
Proceeding this way through all sets of common values in the $c_j$ we find that for all $j,k\in\cR$, either $c_j=c_k$ or  $\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}=0$.

For the reverse direction, fix $j\in\cR$ and let $J=\{k\in\cR:c_k=c_j\}$, so that by assumption $\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}=0$ for all $k\not\in J$.
Then by the above basis expansion, $\hat{\mu}^{(\text{ols})}_{Y|R}(y\mid j)=c_j$, and \[
    \hat{\mu}^{(\text{wtd})}_{Y|R}(y\mid j) 
    = \frac{\sum_{k\in\cR} c_k \hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}}{
        \sum_{k\in\cR}\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}}
    = \frac{c_j \sum_{k\in J} \hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}}{
        \sum_{k\in J}\hat{\vb P}_{\cdot j}^\top\hat{\vb P}_{\cdot k}}
    = c_j = \hat{\mu}^{(\text{ols})}_{Y|R}(y\mid j). \qedhere
\]
\end{proof}
```
Despite potential advantages over the weighting estimator, the OLS estimator is not well-suited to estimation in practice, since it ignores the fact that the unknown parameters are probabilities and thus constrained to be nonnegative and sum to 1.
As a result, in any particular sample, the estimator can produce impossible or contradictory estimates.
This is particularly problematic because a large number of unique surnames make both $\vb P$ and $\vb b$ high-dimensional.
To address this challenge, and to open the door to more flexible modeling, we next propose a Bayesian modeling approach that is based on our identification strategy and yet satisfies necessary constraints.

## Bayesian model

We first present the model and describe the choice of prior before briefly discussing computational issues.
We consider modeling the conditional distribution of the outcome $Y$ given race $R$, residence location $G$, and other observed characteristics $X$.
Under Assumption \ref{a:indep-ys}, we do not need to condition on $S$.
The top part of the model places no restrictions on the outcome process.
\begin{equation*}
    Y_i \mid R_i, G_i, X_i, \Theta \sim \Categorical_\cY(\vb*\theta_{\cdot R_i}(G_i,X_i)),
\end{equation*} where the parameters $\theta_{yr}(g,x)$ represent the probabilities for each level of the outcome $Y$, which can vary by $R$, $G$, and $X$.
We have written the parameters as $\theta_{yr}(g,x)$ rather than $\theta_{yrgx}$ to separate the conceptual role of $R$ and $G,X$ in the model.
The dot $\cdot$ indicates a vector built up out of all possible values of that index.
For example, $\vb*\theta_{\cdot R_i}(G_i,X_i)$ is a vector of length $|\cY|$ consisting of entries of $\theta_{yR_i}(G_i,X_i)$ for $y\in\cY$.
The conditional distribution of $Y$ given $R$ is of primary interest, which can be written as, $$
    \bar\theta_{yr} 
    = \mathbb{E} (\theta_{yr}(G_i, X_i)),
$$ where the expectation is taken over the joint distribution of $G$ and $X$.

Next, we model $G$, $X$, and $S$ given $R$ using the BISG methodology.
Specifically, we model $S$ and $(G,X)$ separately given $R$ using the Census data under Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc}): \begin{align*}
    (G_i, X_i) \mid R_i &\sim \Categorical_{\cG\times\cX}(\vb q_{GX|R_i}) \\
    S_i \mid R_i &\sim \Categorical_\cS(\vb q_{S|R_i}) \\
    R_i &\iid \Categorical_\cR(\vb q_R)
\end{align*} Finally, the model is completed with the prior distribution for $\Theta$, which is denoted by $\pi(\Theta)$ and discussed in detail below.

Putting all together, the posterior distribution can then be written as \begin{align}
    \pi(\Theta, \vb R\mid \vb Y, \vb G, \vb X, \vb S)
    &\propto \pi(\Theta)\prod_{i=1}^N \pi(Y_i\mid G_i, X_i, R_i, \Theta)
            \pi(G_i, X_i\mid R_i)\pi(S_i\mid R_i)\pi(R_i) \nonumber \\
    &= \pi(\Theta) \prod_{i=1}^N \theta_{Y_iR_i}(G_i, X_i) q_{G_iX_i\mid R_i} q_{S_i\mid R_i} q_{R_i} \nonumber \\
    &\propto \pi(\Theta) \prod_{i=1}^N \theta_{Y_iR_i}(G_i, X_i)\hat{P_i}_{R_i}, \label{eq:posterior}
\end{align} where as above $\hat{\vb P_i}$ are the BISG probability estimates for individual $i$, which depend on Census data represented in $\vb q_{GX\mid R}$, $\vb q_{S\mid R}$, and $\vb q_R$, but not on the parameters $\Theta$ and individual race $\vb R$.
<!--# Notice the similarities between the final form of the posterior and the identifying equation \eqref{eq:tprob}. -->

The final but critical component of the model is the prior $\pi(\Theta)$.
A natural prior would be to allow for maximum flexibility with a *fully saturated* specification: \begin{align*}
    \Theta_{yr}(g, \vb x) &= \theta_{yrg\vb x},  \\
    \vb*\theta_{\cdot rgx}&\iid\Dirichlet(\alpha).
\end{align*} This prior, however, suffers from the curse of dimensionality as more covariates are added to $X$, especially since in practice $G$ will be quite large, covering many blocks or ZIP codes.
For example, for a binary outcome and nationwide data, if we use block-level race data, five racial categories, and just four binary covariates, the model has over 650 *million* parameters, which is nearly double the entire U.S. population.
With all of these parameters *a priori* independent under the fully saturated prior, there is no opportunity for sharing information across parameters with the prior dominating the likelihood for individual parameters.
This problem could be partially remedied by adding some hierarchical structure to the priors on $\vb*\theta_{\cdot rgx}$, e.g., by grouping parameters at the county or state level.

We take a slightly different approach, motivated by the fact that we are primarily interested in $\bar\Theta$.
We assume that the relationship between the outcome $Y$ and race $R$ could vary widely by geographies, or within demographic subgroups but the structure of this interaction is relatively low-dimensional.
Under this assumption of no higher-order interaction, we propose the following *additive* prior specification, which can be thought about as a random intercept model, \begin{align*}
    \Theta_{yr}(g, \vb x) &= g^{-1}\qty(\log\theta_{yr} + \beta^{(0)}_{yrg} + 
        \sum_{k=1}^{K} \beta^{(k)}_{yrx_k})\\
        \vb*\theta_{\cdot r} &\iid \Dirichlet(\alpha) \\
        \vb*\beta^{(k)}\mid\sigma^2_k &\iid \Norm(0, \sigma^2_k), \quad k=0,1,\dots,K \\
        \sigma^2_k &\iid \Cauchy^+(0, L), \quad k=0,1,\dots,K,
\end{align*} where $K$ is the number of covariates, $\alpha$ and $L$ are hyperparameters, and $g^{-1}$ is a softmax link function.
The hierarchical priors on the $\vb*\beta^{(k)}$ allow the model to shrink towards zero for covariates which are not predictive of a varying relationship between $Y$ and $R$.
To the extent one wishes to model higher-order interactions, those can be included as additional random intercepts; the key efficiency gain is in separating the large number of geographic parameters $\beta^{(0)}_{yrg}$ from the other covariates.
We have found, on smaller datasets where fitting both the additive and fully saturated specifications is possible, that both specifications give similar inferences for $\bar\Theta$, but the additive specification tends to be more accurate even for medium-sized problems.

## Computation

Since the individual race $\vb R$ is high-dimensional, discrete, and not of primary interest, we can marginalize it out as follows: \begin{align*}
    \pi(\Theta\mid \vb Y, \vb G, \vb X, \vb S)
    &= \pi(\Theta) \prod_{i=1}^N \sum_{r\in\cR} \theta_{Y_i r}(G_i, X_i)\hat{P}_{ir}
    = \pi(\Theta) \prod_{i=1}^N \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i.
    \numberthis \label{eq:post-marg}
\end{align*} This makes it much easier to sample from the model, as the number of parameters is now a function only of the sizes of $\cY$, $\cR$, $\cG$, and $\cX$, and not the sample size $N$.

The marginalized posterior \eqref{eq:post-marg} is simple, but not conjugate to any prior distribution.
It has only continuous parameters, but these are constrained to be nonnegative and to sum to 1 across certain dimensions.
The model can be fit with any general Bayesian inference procedure such as Markov chain Monte Carlo (MCMC).
However, the large amount of data and moderate-to-high dimensionality in practical settings makes MCMC algorithms computationally infeasible.
In our applications here, and in our open-source software implementation of the proposed method, we use stochastic variational inference (SVI) with a mean-field approximation to fit the model instead [@hoffman2013stochastic], as implemented in the Pyro probabilistic programming language [@bingham2018pyro].
In the appendix, we perform a comparison at a small sample size which confirms the accuracy of this approach relative to full MCMC.
Both the posterior location and uncertainty are well-approximated with SVI.
When the number of parameters is not too large relative to the data size, simpler methods such as a Laplace approximation centered at the maximum a posteriori estimate would likely be applicable, too.

## Sensitivity Analysis {#sec:sens}

The proposed model, like the weighting and OLS estimators, relies upon Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc}, which are required for the BISG race probabilities to be accurate.
Unfortunately, these assumptions may not exactly hold in practice.
In this section, we address this challenge by developing a sensitivity analysis that assesses how the bias in BISG race probabilities affect the estimates of racial disparities.

In particular, we consider a setting where Assumptions \ref{a:indep-sgx} and \ref{a:cens-acc} may be violated but Assumption \ref{a:indep-ys} still holds.
For example, consider the existence of unobserved confounder that affects some or all of the variables except the outcome, i.e., $(R,S,G,X)$.
This leads to the violation of Assumption \ref{a:indep-sgx}, but Assumption \ref{a:indep-ys} continues to be satisfied so long as such unobserved confounder does not affect the outcome.
Unfortunately, inaccurate BISG predictions still lead to biased estimates of racial disparity.

Specifically, if either the Census data are inaccurate, or the conditional independence relation does not hold $S\notindep G,X\mid R$, then the BISG predictions $\hat{\vb P}$ will differ from the "true" individual race probabilities $\vb P^*$.
Our goal is to quantify how an error in these probabilities $\vb P^* - \hat{\vb P}$ shifts the posterior and hence the estimates of racial disparities.

Denote by $\pi_{\vb*\delta}$ the posterior constructed using the error-corrected BISG race probabilities $\hat{\vb P}_i+\vb*\delta_i$ as the input probabilities for the model (see Equation \eqref{eq:posterior}), where $\pi_{\vb*\delta^*}$ is the true posterior with $\vb*\delta_i^* \dfeq \vb P^*_i - \hat{\vb P}_i$.
Estimating how $\pi$ and $\pi_{\vb*\delta}$ differ in general is difficult, but we focus on the settings where $\vb*\delta$ is small enough to make a linear approximation appropriate.
In sum, we aim to quantify how the small error in BISG probabilities can alter the estimates of racial disparities.

Define the following perturbation weight, which represents the ratio of posterior based on the biased and error-corrected BISG race probabilities: $$
    w(\Theta,\vb*\delta^*) \dfeq 
    \prod_{i=1}^N 
    \qty(1 + \frac{\vb*\theta_{Y_i}(G_i, X_i)^\top \vb*\delta_i^*}{
    \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i})
    \propto \frac{\pi_{\vb*\delta^*}(\Theta\mid\vb Y,\vb G,\vb X,\vb S)}{\pi(\Theta\mid\vb Y,\vb G,\vb X,\vb S)},
$$ Then, using linear approximation, we write the bias for a particular quantity of interest $g(\Theta)$ as \begin{align*}
    \E_{\pi_{\vb*\delta^*}}[g(\Theta)] - \E_\pi[g(\Theta)]
    &= \eval{\dv{\E_{\pi_{\vb*\delta}}[g(\Theta)]}{\vb*\delta}}_{\vb*\delta=0}^\top
        \vb*\delta^* + o(\norm{\vb*\delta^*}) \\
    &= \Cov_\pi\qty(g(\Theta), 
        \eval{\dv{\log w(\Theta,\vb*\delta)}{\vb*\delta}}_{\vb*\delta=0})^\top
        \vb*\delta^* + o(\norm{\vb*\delta^*}),
        \numberthis\label{eq:bias-dv}
\end{align*} where the second equality is due to Theorem 2.1 of [@giordano2018cov; see also the idea of *local sensitivity* from @gustafson1996local].

With this representation, we can bound the total error in $\E_\pi[g(\Theta)]$ for sufficiently small $\vb*\delta$ as the following theorem shows.

```{=tex}
\begin{theoremrep}[Bias Bound] \label{thm:bound}
Define $\tilde\vartheta_{ir}\dfeq 
\frac{\theta_{Y_ir}(G_i,X_i)}{\vb*\theta_{Y_i}(G_i,X_i)^\top\hat{\vb P}_i}$.
Then for any input probabilities with total error $\norm{\vb*\delta^*}^2
=\sum_{i=1}^n\norm{\vb*\delta_i^*}^2\le \Delta^2$,
\begin{equation} \label{eq:covbound}
    |\E_{\pi^*}[g(\Theta)] - \E_\pi[g(\Theta)]|
    \lesssim \Delta\norm{\Cov_\pi(g(\Theta), \tilde\vartheta)},
\end{equation}
as $\Delta\to 0$.
\end{theoremrep}
\begin{proof}
This is immediate from \eqref{eq:bias-dv} once we compute 
\begin{align*}
    \dv{\log w(\Theta,\vb*\delta)}{\delta_{ir}} 
    &= \dv{\delta_{ir}} \sum_{i=1}^N 
        \log(1+\frac{\vb*\theta_{Y_i}(G_i, X_i)^\top \vb*\delta_i}{
        \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i}) \\
    &= \dv{\delta_{ir}}
        \log(1+\frac{\vb*\theta_{Y_i}(G_i, X_i)^\top \vb*\delta_i}{
        \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i}) \\
    &= \frac{1}{1+\frac{\vb*\theta_{Y_i}(G_i, X_i)^\top \vb*\delta_i}{
        \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i}} \times
        \frac{\theta_{Y_ir}(G_i, X_i)}{
        \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i} \\
    &= \frac{\theta_{Y_ir}(G_i, X_i)}{
        \vb*\theta_{Y_i}(G_i, X_i)^\top(\hat{\vb P}_i+\vb*\delta_i)}
\end{align*}
and evaluate at $\vb*\delta=0$, since the worst-case bias for a fixed total error can be obtained by having the maximum allowable $\vb*\delta$ point in the direction of the gradient of $\log w(\Theta,\vb*\delta)$.
\end{proof}
```
The theorem shows that once researchers choose the amount of total error $\Delta$, then the bound on the shift in a quantity of interest can be computed readily from posterior draws.
It is important to note that both $\vb*\delta^*$ and $\Cov_\pi(g(\Theta), \tilde\vartheta)$ are vectors whose dimension depends on the sample size $N$.
Thus, all else being equal, their norms will each grow as $\sqrt{N}$.
However, each entry $\Cov_\pi(g(\Theta), \tilde\vartheta_{ir})$ will tend to shrink as $N$ increases, since each observation exerts less leverage on the overall posterior.
Thus the overall impact of the sample size on the bound in Equation \eqref{eq:covbound} may depend on specific features of the data.
In particular, and as should be expected, the error is not guaranteed to vanish as $N\to\infty$.
Practitioners should evaluate Equation \eqref{eq:covbound} under a range of plausible $\Delta$ to understand how robust their findings are to worst-case linear violations of Assumptions $\ref{a:indep-sgx}$ and $\ref{a:cens-acc}$.

The appendix contains further results which are more qualitative but also do not require asymptotics.
The key takeaway is that a pattern of uniform bias can arise when the prior $\vb q_R$ doesn't match the population distribution of race.
Specifically, when the target population differs from the entire U.S. population that is enumerated by the census, or if the census is out of date, a $\vb q_R$ taken from census tables alone will be incorrect.
We recommend that practitioners other data sources, such as surveys, to estimate the overall race distribution, and use this instead of the national census estimates for $\vb q_R$.


# Validation Study {#sec:valid}

To better understand how the proposed model performs in real-world contexts, we apply it to the North Carolina voter registration data.
This voter file contains individual-level self-reported race and hence the "ground truth" relationship between outcome and race is known.
We compare the performance of the proposed model against those of other estimators.
We also evaluate how the estimation error depends on the level of geographic precision used in the BISG probabilities.

## North Carolina Voter File

Like most other Southern states with a history of disenfranchising minority voters, the state of North Carolina asks (and previously required) every voter to self-report their race upon registration.
This data, along with voters' names, addresses, gender, party registration (if any), and voting history, is part of the voter file that the secretary of state makes publicly available.
This feature makes the voter file an ideal validation setting for the proposed methodology.
The two outcomes we examine here, party registration and turnout, are the product of many unobservable factors, and are known to differ across racial groups.
Since self-reported race is available, inferences about these racial disparities using the estimators discussed here can be compared to the corresponding true values.

We use a subset of the October 2022 voter file which could be linked to a proprietary voter file provided by L2, Inc., a leading national non-partisan firm and the oldest organization in the United States that supplies voter data and related technology to candidates, political parties, pollsters, and consultants for use in campaigns.
The L2 file geocoded each address to a Census block, which allows for the finest block-level BISG predictions.
We also removed any records without individual race information, since our goal is validation compared to some ground truth, rather than inference about the entire population of registered North Carolina voters.
Alltogether, `r percent(1 - 5754912/7389159, 0.1)` of records either had missing race information or could not be linked to the L2 file.


```{r nc-overview, fig.width=6.5}
#| fig.cap: |
#|     Distribution of party identification (left) and turnout (right) by race, 
#|     for a sample of 500,000 North Carolina voters. Parties are Libetarian (LIB),
#|     Republican (REP), Independent (IND), and Democrat (DEM). Turnout ranges from 
#|     0 to 10, indicating the number of November general elections from 2012 through 2021, 
#|     in which a voter turned out.
knitr::include_graphics("figures/nc_overview.pdf")
```

The overall merged voter file contains 5,754,912 voters, 71.2% of which are White, 21.1% of which are black, and 7.7% of which belong to another race.
To reduce computational burden, we further subsampled this file by selecting 500,000 records at random without replacement.
A sample size of half a million is large enough to ensure that sampling error is essentially negligible.

Figure \@ref(fig:nc-overview) shows the distribution of party registration and turnout by race in this subsample.
White voters disproportionately register as Republicans, while Black voters disproportionately register as Democrats.
We measure turnout as the number of November general elections each individual voted in from 2012 through 2021, ranging from 0 to 10. **KI: For the turnout analysis, we need to further subset to those whose registration date is before the 2012 election.  If that's too few, we can reduce the number of elections we look at. **
For analysis with the methods discussed here, we treat turnout as a categorical variable.
Turnout rates are highest for Black and White voters, and lowest for Hispanic voters.

## Results

We first calculate BISG probabilities using 2010 Census data at the census block, tract, ZIP code tabulation area (ZCTA), and county level.
Every record in the voter file contains county information, while roughly 13% of records are missing ZIP codes and 27% of records are missing blocks/tracts; when these finer geographic identifiers were missing, we used county-level Census tables in the BISG calculations.

```{r}
x = readRDS("data/nc_bisg.rds")
acc = percent(x$acc, 0.1)
score = number(x$score, 0.001)
```

The BISG probabilities are broadly accurate. **KI: Can we add a table of BISG accuracy (false negative and positives) and calibration plots to the appendix?**
Using the maximum a posteriori racial category as a prediction, we obtain the accuracy of `r acc["county"]` for the county probabilities, `r acc["zip"]` for the ZIP code probabilities, `r acc["tract"]` for the tract probabilities, and `r acc["block"]` for the block probabilities.
An alternative measure of the quality of the BISG probabilities is the logarithmic score, a proper scoring rule which rewards accurate and calibrated probabilistic estimates (higher values are better).
The logarithmic scores for the BISG probabilities are -`r score["county"]` for counties, -`r score["zip"]` for ZIP codes, -`r score["tract"]` for tracts, and -`r score["block"]` for blocks.
For comparison, the prior-only logarithmic score (i.e., using no name or geographic information) is -`r number(x$base_score, 0.001)`.
The worsened performance for block-level versus tract-level probabilities likely stems from the larger impact of census measurement error at smaller geographies, a problem that could be addressed using newer BISG methods such as those of @imai2022addressing.

For a given set of BISG probabilities, we estimate the joint distribution of each outcome variable and race using the proposed method, the weighting estimator, the OLS estimator, and a thresholding estimator that deterministically assigns each individual the maximum *a posteriori* racial category.
For the proposed method, we use the additive specification described above, using geographic random effects matching the geographic level used in the BISG probabilities (e.g., county random effects for the county-level BISG probabilities), except for the block-level probabilities.
Due to the large number of individual census blocks, we use tract random effects instead for this particular model.

```{r nc-disp, fig.width=6.75}
#| fig.cap: |
#|     Error in the White-Black and White-Hispanic disparity estimates for
#|     party identification and turnout, by estimation method.
#|     All methods used block-level data for this figure; the results 
#|     for coarser levels of geographic detail are very similar.
#|     Estimation uncertainty is minimal and hence suppressed from 
#|     the figure for clarity.
x = readRDS(here("paper/data/nc_disp_ex.rds"))
knitr::include_graphics("figures/nc_disp.pdf")
```


We first examine the accuracy of the various methods in estimating the disparity between White and Black, and White and Hispanic voters, in party identification and turnout.
For example, the true difference in Democratic identification between Black and White voters in the sample is $`r percent(-x$model$disp_wb_true)`$ percentage points, meaning Black voters register Democratic at a much higher rate.
However, the standard weighting approach produces an estimate of only $`r percent(-x$weight$disp_wb)`$ percentage points for this disparity---less than half the true value.
In contrast, the proposed model produces an estimate of $`r percent(-x$model$disp_wb)`$ percentage points.

Figure \@ref(fig:nc-disp) compares all the methods across all of these possible disparity measurements, using the Census block-level BISG predictions.**KI: Can you change the legend to "Proposed", "OLS", "Weighting", "Thresholding" **
The true disparity is subtracted off for ease of comparison.
Across both party and turnout, and both inter-racial comparisons, the proposed model (blue dots) outperforms the other three estimators.
The two commonly used estimators -- weighting (green crosses) and thresholding (brown squares) -- exhibit greater estimation errors than the proposed model for almost all cases.
The OLS estimator (yellow triangles), which is based on our new identification strategy but, unlike the proposed model, does not account for certain probability constraints, gives erroneous results especially for Hispanics which represents a small minority in North Carolina.

```{r nc-tv, fig.width=6.75}
#| fig.cap: |
#|     Total variation distance (on a logarithmic scale)
#|     between the estimated and actual distribution of
#|     party identification and turnout, 
#|     by estimation method and level of geographic detail 
#|     used in the BISG predictions.
knitr::include_graphics("figures/nc_tv.pdf")
```

For a more comprehensive look at the error in the estimated joint distribution, we turn to the total variation (TV) distance, which is calculated as$$
d_\text{TV}(\hat{\vb*\mu}_{Y|R}, \vb*\mu_{Y|R}) 
= \half\sum_{y\in\cY}\sum_{r\in\cR} |\hat{\mu}_{Y,R}(y, r) - \mu_{Y,R}(y, r)|.
$$The TV distance is an upper bound on the error in *any* probability calculated from the estimated joint distribution, and as such is useful general-purpose measure of estimation error.

Figure \@ref(fig:nc-tv) shows the TV distance for each estimator, not just for the Census block-level BISG estimates used in Figure \@ref(fig:nc-disp) but also across the range of geographic levels used in the BISG calculation (x-axis). **KI: Again, change the labels of the estimators. Can you also add the label for the lowest line in y-axis of the left plot?**
We find that the proposed method (blue circles) outperforms every alternative, for both party ID and turnout, at every geographic level.
The conventional estimators (green crosses for the weighting estimator and brown squares for the thresholding estimator) exhibit much larger errors than the proposed methodology.
When compared to the error of the proposed estimator, their errors range from XX% to XX% greater for party registration and from XX% to XX% more for turnout, depending on the geography used for BISG predictions. **KI: Please fill in**
The OLS estimator (yellow triangles) outperforms the weighting and threshold estimators for party ID, but performs significantly worse for turnout than the weighting estimator. 

Finer geographic data translates to improved accuracy for the proposed model-based estimates for party ID.
For turnout, the proposed method yields accurate estimates, indicated by substantively small TV errors, in all cases except that the TV error of the tract-level estimates is somewhat greater than the other three.
While the weighting estimator also becomes more accurate as BISG predictions use finer geographical data, no clear pattern exists for the OLS and thresholding estimators. 
Although it is difficult to explain these patterns, the findings suggest that the use of finer geographical data is appropriate at least for the proposed model-based estimator.
Figure \@ref(fig:nc-tv-detail) in the appendix also shows the TV distance for each conditional distribution by race, to illuminate how the estimators perform on each subgroup.


```{r}
x = readRDS("data/nc_bounds.rds")
```

As noted above, the sampling error in all of these estimates is minimal---the widest 90% confidence interval in the block-level model estimates of party ID by race is $(`r percent(x$ci_min, 0.01)`, `r percent(x$ci_max, 0.01)`)$---not enough to explain the residual TV error.
This error results from a violation of Assumptions \ref{a:indep-sgx}--\ref{a:indep-ys}, likely in large part driven by outdated Census data and differences between the adult and registered voter populations.

We can apply the theoretical bounds from Theorem \ref{thm:bound} to gain a better understanding of how these data issues, which lead to errors in the BISG probabilities, translate to errors in the proposed model's estimates.
For the county-level party ID estimates, a total error of $\Delta=`r number(x[["tot_err_party"]], 0.001)`$ is sufficient to explain the largest error in the joint distribution.
**KI: Not sure exactly what this means. Can we show via a graph how changing $\Delta$ leads to different estimates of disparities? I'm also worried about if this sensitivity analysis may not be too informative...**
**CM: probably worth discussing this section in person.**
For the county-level turnout estimates, a total error of $\Delta=`r number(x[["tot_err_turnout"]], 0.001)`$ suffices.
These total errors translate to an average error in the individual BISG probabilities, $||\vb*\delta_i||$, of `r number(100*x[["avg_err_party"]], 0.01)` and `r number(100*x[["avg_err_turnout"]], 0.01)` percentage points, respectively.
The actual average error is almost certainly higher but is not in the worst-case direction (which is what Theorem \ref{thm:bound} assumes).
Moreover, the bounds on the individual entries of the joint distributions implied by even these small $\Delta$ are only useful for the larger racial groups; they are vacuous for the "Other" category, for instance.
Nevertheless, this kind of analysis can give an idea of the relative sensitivity of the party ID and turnout estimates to data error (namely, that the turnout estimates are more sensitive).

# Application {#sec:appl}

-   Treasury data, no covariates

# Discussion {#sec:disc}

-   In most real-world applications, the new model and identification condition are much appropriate and will produce much-improved estimates.

    1.  However, there is no one-size-fits-all approach for the estimation of disparities. Careful consideration of the underlying causal and information structure is required to choose the correct estimation approach and avoid making the wrong conclusions.

-   Future work:

    -   empirical analysis to determine useful additional variables to condition on to weaken \ref{a:indep-ys}

    -   incorporating measurement error model of @imai2022addressing while staying computationally tractable

    -   incorporating continuous variables

    -   computational improvements

-   Reiterate importance of specific causal structure

# (APPENDIX) Appendix {.unnumbered}

\renewcommand\thefigure{\thesection\arabic{figure}}
\setcounter{figure}{0}  

# Appendix: Comparison of Stochastic Variational Inference and MCMC

We sample 100 voters from the Dare county, NC voter file, and generate BISG predictions.
As in the validation section, we estimate the party-race relationship using the proposed Bayesian model.
We note that due to the small sample size, neither posterior is particularly close to the true distribution of party and race.
However, in this section, we are focused only on examining the accuracy of SVI in approximating the posterior that would be obtained without any approximation.
We fit the model both with the stochastic variational inference (SVI) routine and with full MCMC sampling using the No U-Turn Sampler (NUTS), which we treat as the "gold standard" for inference from thsi model.
All inference is done with routines provided by Pyro probabilistic programming framework [@bingham2018pyro].
Fitting with SVI took 4 seconds, while fitting with MCMC took 54 seconds.

In general, as the sample size grows, we expect the SVI approximation to improve, since the overall posterior will converge towards Normality.
As Figure \@ref(fig:valid-nuts-compare) shows, even with just 100 observations, SVI provides an excellent approximation to the posterior obtained by the NUTS sampler, not just for the posterior median but for the posterior uncertainty as well.

```{r valid-nuts-compare, fig.width=6.75}
#| fig.cap: |
#|     Posterior distributions for the model fit to a 100-person sample, 
#|     obtained with SVI (approximate) and NUTS ("gold standard") inference routines.
knitr::include_graphics("figures/valid_nuts_compare.pdf")
```

# Appendix: Total Variation Decomposition by Racial Group

Figure \@ref(fig:nc-tv-detail) below is the same as Figure \@ref(fig:nc-tv) but with the total variation distance decomposed by racial group.
Compared to Figure \@ref(fig:nc-tv), the results are more mixed for individual racial groups.
It is important to remember that Black and White voters together comprise over 92% of the sample.
The new model consistently outperforms the other estimators for Hispanic and Asian voters, and is either best or essentially tied for best for White, Black, and Native voters, across both outcomes and all geographic levels.
All of the methods struggle for the Other category, likely due to its small size and the heterogeneity of its members.
However, the weighting estimator is probably the "least bad" for this group.

```{r nc-tv-detail, fig.width=6}
#| fig.cap: |
#|     Total variation distance (on a logarithmic scale)
#|     between the estimated and actual distribution of
#|     party identification and turnout, 
#|     by estimation method, level of geographic detail 
#|     used in the BISG predictions, and race.
knitr::include_graphics("figures/nc_tv_detailed.pdf")
```

# Appendix: Nonasymptotic Sensitivity Analysis

For a more qualitative understanding of the effect of a particular $\vb\delta$, we can derive results on the direction of the error in $\E_\pi[g(\Theta)]$ for linear $g(\Theta)=\vec(\Theta)^\top\vb d$ and particular configurations of $\vb*\delta$.
Unlike Theorem \ref{thm:bound}, these results hold across all sizes of $\vb*\delta$, and not just asymptotically as $\norm{\vb*\delta}\to 0$. 

The class of linear $g$ is still large enough to contain many of the usual estimands of interest.
For example, estimating conditional expectations of $Y$ for certain racial groups can be achieved by setting the entries of $\vb d$ to 1 for those combinations of $Y$ and $R$.
The first theorem below establishes results for this case, and the subsequent corollary translates a common special case.

First, however, we establish an important lemma.
We can write the bias for a particular quantity of interest $g(\Theta)$ as
\begin{align*}
    \E_{\pi_{\vb*\delta^*}}[g(\Theta)] - \E_\pi[g(\Theta)]
    &= \frac{\E_\pi[w(\Theta,\vb*\delta^*)g(\Theta)]}{\E_\pi[w(\Theta,\vb*\delta^*)]} - 
        \E_\pi[g(\Theta)] \\
    &= \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta^*)}{\E_\pi[w(\Theta,\vb*\delta^*)]}, g(\Theta)),
\end{align*}
where $\vb Y=(\vb Y,\vb G, \vb X,\vb S)$ is the observed data.
This holds exactly, and is not just a linear approximations (notice the absence of a logarithm around the perturbation weights). 

```{=tex}
\begin{lemma} \label{lem:biassign}
For a vector $\vb d$, if \[
    \hat r_{ij}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} -
    \delta_{ij}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i} \ge 0
\] for all $1\le i\le N$ and $j\in\cR$, then \[
    \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]}, 
        \vec(\Theta)^\top\vb d\gvn\vb Y) \ge 0.
\] Conversely, if \(
    \hat r_{ij}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} -
    \delta_{ij}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i} \le 0
\) for all $1\le i\le N$ and $j\in\cR$, then \[
    \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]}, 
        \vec(\Theta)^\top\vb d\gvn\vb Y) \le 0.
\]
\end{lemma}
\begin{proof}
We will leverage the covariance inequality, which states that for a monotonically increasing function $g$ and monotonically increasing (decreasing) functions $f$, and any random variable $Y$, $\Cov(f(Y),g(Y))$ is nonnegative (nonpositive).
Since $w$ is nonnegative, we need only consider $\Cov_\pi\qty(w(\Theta,\vb*\delta), \vec(\Theta)^\top\vb d\mid\vb Y)$, without the normalization constant $\E_\pi[w(\Theta,\vb*\delta)\mid\vb Y]$.
In fact, since $w$ is a product of nonnegative terms, and the product of nonnegative monotonic functions is again monotonic, by the covariance inequality it suffices to show that each term \[
    w_i \dfeq 1 + \frac{\vb*\theta_{Y_i}(G_i, X_i)^\top \vb*\delta_i}{
        \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i} 
\] is monotonic in $\vec(\Theta)^\top\vb d$.

Partial derivatives with respect to elements of $\vec(\Theta)$ are clearly zero except for those corresponding to $\vec(\Theta)_{Y_i\cdot G_iX_i}$. These are \[
    \pdv{w_i}{\vec(\Theta)_{Y_irG_iX_i}}
    = \frac{\vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i\delta_{ir} -
        \vb*\theta_{Y_i}(G_i, X_i)^\top \vb*\delta_i \hat r_{ir}}{
        \vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i}.
\] The directional derivative along $\vb d$ is then 
\begin{align*}
    \partial_{\vb d}w_i 
    &= \frac{\vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i
                \vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} - 
                \vb*\theta_{Y_i}(G_i, X_i)^\top \vb*\delta_i
                \hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i}}{
            (\vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i)^2} \\
    &= \frac{\vb*\theta_{Y_i}(G_i, X_i)^\top}{
            (\vb*\theta_{Y_i}(G_i, X_i)^\top \hat{\vb P}_i)^2}
            (\hat{\vb P}_i\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} - 
            \vb*\delta_i\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i}).
\end{align*}
Since $\vb*\theta_{Y_i}(G_i, X_i)$ is nonnegative, a sufficient condition for the directional derivative to be nonnegative, and thus for $w_i$ to be monotonically increasing in $\vec(\Theta)^\top\vb d$, is for
$\hat r_{ij}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} - \delta_{ij}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i} \ge 0$ for all $j\in\cR$. Similarly, by reversing this inequality, we have a sufficient condition for $w_i$ to be monotonically decreasing in $\vec(\Theta)^\top\vb d$.
\end{proof}
```
```{=tex}
\begin{theorem}
Let $r\in\cR$, $I\subseteq\{1,\dots,N\}$, and $\vb d$ a vector such that $\vb d_{Y_i\cdot G_iX_i}= c\vb e_r$ for all $i\in I$ and $0$ elsewhere, where $\vb e_r\in\R^{|\cR|}$ is the standard basis vector in the $r$ direction and $c>0$.
Then if for all $i\in I$, $\delta_{ir}>0$ and $\delta_{ij}<\delta_{ir}$ for $j\neq r$, \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb d\mid\vb Y] \ge \E_{\pi}[\vec(\Theta)^\top\vb d\mid\vb Y].
\]
\end{theorem}
\begin{proof}
Let $i\in I$. Then by assumption $\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i}=c\delta_{ir}>0$. 
Let $j\in\cR$. If $j=r$, \[
    \hat r_{ir}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} -
    \delta_{ir}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i}
    = \hat r_{ir}c\delta_{ir} - \delta_{ir}\hat r_{ir}c = 0,
\]
If $j\neq r$, we have \[
    \hat r_{ij}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} -
    \delta_{ij}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i}
    = \hat r_{ij}c\delta_{ir} - \delta_{ij}\hat r_{ij}c
    = c\hat r_{ij}(\delta_{ir} - \delta_{ij})\ge 0.
\]
So the condition of Lemma~\ref{lem:biassign} is satisfied for all $j\in\cR$, and
thus by the lemma we conclude that \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb d\mid\vb Y] - \E_{\pi}[\vec(\Theta)^\top\vb d\mid\vb Y]
    = \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)]}, 
    \vec(\Theta)^\top\vb d \gvn\vb Y) \ge 0. \qedhere
\]
\end{proof}
```
By reversing the sign of $\vb*\delta_i$, we can reverse the direction of the bias.
For the binary race case, we have the following translation of this result.

```{=tex}
\begin{corollary} \label{cor:errdir1}
Suppose that race is binary, i.e., $\cR=\{0,1\}$.
Then for any $y\in\cY$, the model-based estimate of $\Pr[Y=y\mid R=1]$, denoted $\E_\pi[\bar\Theta_{y1}\mid\vb Y]$, 
will be biased downwards (upwards) if, for all $i$ with $Y_i=y$, the BISG probability $\hat r_{i1}$ is biased downwards (upwards).
\end{corollary}
```

Racial disparities can be similarly estimated by setting entries of $\vb d$ to $-1$ and $1$, to contrast racial groups.
The next theorem provides results for this case, and the subsequent corollary translates the common special case.

```{=tex}
\begin{theorem}
Let $r,s\in\cR$, $I\subseteq\{1,\dots,N\}$, and $\vb d$ a vector such that $\vb d_{Y_i\cdot G_iX_i}= c_1(\vb e_r-\vb e_s)$ for all $i\in I$ and $0$ elsewhere, for some $c_1>0$.
Then if for all $i\in I$, $\vb*\delta_i=c_{2i}(\vb e_r-\vb e_s)$ for some $c_{2i}>0$, \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb d\mid\vb Y] \ge \E_{\pi}[\vec(\Theta)^\top\vb d\mid\vb Y].
\]
\end{theorem}
\begin{proof}
Let $i\in I$. Then by assumption $\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i}=c(|\delta_{ir}|+|\delta_{is}|)>0$. 
Let $j\in\cR$. If $j=r$ we have \[
    \hat r_{ir}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} -
    \delta_{ir}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i}
    = \hat r_{ir}c_1(2c_{2i}) - c_{2i}c(\hat r_{ir}-\hat r_{is}) 
    = c_1c_{2i}(\hat r_{ir} +\hat r_{is}) \ge 0.
\]
If $j=s$ we have \[
    \hat r_{is}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} -
    \delta_{is}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i}
    = \hat r_{is}c_1(2c_{2i}) + c_{2i}c(\hat r_{ir}-\hat r_{is}) 
    = c_1c_{2i}(\hat r_{is} +\hat r_{i+}) \ge 0.
\] If $j\not\in\{r,s\}$ then $\delta_{ij}=0$, so \[
    \hat r_{ij}\vb*\delta_i^\top\vb d_{Y_i\cdot G_iX_i} -
    \delta_{ij}\hat{\vb P}_i^\top\vb d_{Y_i\cdot G_iX_i}
    = \hat r_{ij}c_1(2c_{2i}) \ge 0.
\] So the condition of Lemma~\ref{lem:biassign} is satisfied for all $j\in\cR$, and
thus by the lemma we conclude that \[
    \E_{\pi_{\vb*\delta^*}}[\vec(\Theta)^\top\vb d\mid\vb Y] - \E_{\pi}[\vec(\Theta)^\top\vb d\mid\vb Y]
    = \Cov_\pi\qty(\frac{w(\Theta,\vb*\delta)}{\E_\pi[w(\Theta,\vb*\delta)]}, 
    \vec(\Theta)^\top\vb d \gvn\vb Y) \ge 0. \qedhere
\]
\end{proof}
```
As above, we can reverse the direction of the bias by reversing the sign of $\vb*\delta_i$.

```{=tex}
\begin{corollary} \label{cor:errdir2}
Suppose both the race and outcome variables are binary, i.e., $\cY=\cR=\{0,1\}$.
Suppose that race is binary, i.e., $\cR=\{0,1\}$.
Then for any $y\in\cY$, the model-based estimate of the disparity $\Pr[Y=y\mid R=1]-\Pr[Y=y\mid R=0]$, denoted $\E_\pi[\bar\Theta_{11}-\bar\Theta_{10}\mid\vb Y]$, 
will be biased downwards (upwards) if, for all $i$ with $Y_i=y$, the BISG probability $\hat r_{i1}$ is biased downwards (upwards).
\end{corollary}
```
Taken together, Corollaries \ref{cor:errdir1} and \ref{cor:errdir2} show that if the individual probabilities of belonging to a racial group are uniformly biased, so too will be the corresponding estimates of the conditional probabilities and disparities.
Most of the time, these bias in the BISG probabilities will not be be so one-sided, and the error patterns will be more complex.
As noted in the main text, a pattern of uniform bias will arise when the prior $\vb q_R$ doesn't match the population distribution of race.
When the target population differs from the entire U.S. population that is enumerated by the census, or if the census is out of date, a $\vb q_R$ taken from census tables alone will be incorrect and will lead to exactly the kind of bias described in Corollaries \ref{cor:errdir1} and \ref{cor:errdir2}.
