---
title: Proxy Race Estimation in North Carolina
description: Correcting confounding with a simple Bayesian model
author:
    - name: Cory McCartan
    - name: Kosuke Imai
date: January 11, 2022
output: distill::distill_article
---

$\newcommand{\vb}[1]{\boldsymbol{#1}} \newcommand{\indep}{\perp\!\!\!\perp}$

```{r setup, include=F}
library(tidyverse)
suppressMessages(library(here))
devtools::load_all(here("."))
```

# Model

Outcome $\vb X$, race $\vb R$, covariates $\vb Z$, location $\vb G$, surname
$\vb S$.

We have that$$
\begin{align}
    \Pr(&X=x\mid Z=z,G=g,S=s) \\
    &= \sum_{r\in\mathcal{R}}  \Pr(X=x\mid R=r,Z=z,G=g,S=s) \Pr(R=r\mid Z=z,G=g,S=s)  \\
    &= \sum_{r\in\mathcal{R}}  \Pr(X=x\mid R=r,Z=z,G=g) \Pr(R=r\mid Z=z,G=g,S=s).
\end{align}
$$The final term $\Pr(R=r\mid Z=z,G=g,S=s)$ is just the simple Bayes-formula
race probabilities; call these $\hat{\vb r_i}$ for each individual $i$. Then the
model is$$
\begin{align}
    X_i \mid Z_i, G_i, S_i, \vb P&\sim \mathrm{Cat}\left(\vb{P}(Z_i, G_i)\,\hat{\vb r_i}\right) \\
    \vb{p}(Z_i, G_i)_{r} &\sim \mathrm{Dir}(\vb{\alpha}),
\end{align}
$$where each column $\vb{p}(Z_i, G_i)_{r}$ of $\vb P(Z_i, G_i)$ is therefore
constrained to be nonnegative and to sum to 1. Despite the Categorical-Dirichlet
setup, the matrix product involved in the categorical probabilities makes direct
or conjugate sampling impossible.

# Data

To evaluate the model in a realistic setting, we can use the voter files from
the various counties of North Carolina, which include, among other things, race,
party, ZIP code, and surname.

```{r prep}
if (!interactive()) {
    voterfile <- here("data/nc_voters_hoke.rds")
    voters = read_rds(voterfile) 
} else {
    # try Duplin, Hoke, Robeson, Durham, Orange, Wake (largest)
    voters = make_nc_df("Wake")
}

# tables
p_xr = prop.table(table(voters$party, voters$race))
p_x = rowSums(p_xr)
p_r = colSums(p_xr)
```

The figure in the margin illustrates the joint distribution of party and race in
one such county, with `r scales::comma(nrow(voters))` voters. Our goal is to
estimate this joint distribution, using only voters' party, ZIP code, and
surname.

<aside>

```{r joint-fig, echo=F}
ggplot(voters, aes(substr(race, 1, 3), party)) + 
    geom_count(aes(size=after_stat(prop), group=1), shape=15) + 
    scale_size_area(max_size=20, guide="none") +
    labs(x=NULL, y=NULL) +
    theme_minimal(base_size=42)
```

</aside>

# Fitting the model

The `model_race()` function fits the above model using automatic differentiation
variational inference (ADVI) as provided by the Stan library. It also computes
the Bayes probabilities using name and location and just name, as well as runs a
Gibbs sampler that elaborates on the basic Bayes setup. Finally, it also
computes a least-squares and a nonnegative-least-squares estimate of
$\Pr(X, R)$, based on the identifying equation above. For each method,
`calc_joints()` computes the implied estimate of $\Pr(X, R)$.

```{r fit, message=F, warning=F, results="hide"}
fit = model_race(party, last_name, zip, Z=c(age, gender), data=voters, p_r=p_r,
                 regularize=T, alpha=3,
                 methods=c("bis", "bisg"),
                 stan_method="vb", 
                 iter=400, verbose=TRUE)

{
fit$pyro = est_additive_pyro(d$X, d$GZ_mat, d$GZ_var, d$pr_base,
                             iter=5000, lr=0.005, subsamp=2048, 
                             smooth=300, tol=0.01,
                             reload=TRUE)
plot(fit$pyro$loss, type='l', ylim=c(min(c(0, fit$pyro$loss)), max(fit$pyro$loss)))
lines(seq_along(fit$pyro$slope)*100, fit$pyro$slope, col='red')
}
xr = calc_joints(p_xr, voters, fit)
eval_joints(xr$true, "tv",
            bis = xr$bis,
            bisg = xr$bisg,
            nonparam = xr$nonparam,
            additive = xr$additive,
            pyro = xr$pyro)
```

# Evaluating the methods

First, by visual inspection, the above model is closer to the true joint
distribution than the naive imputations.

```{r peek, results="hold"}
print_cond(xr$bisg, "bisg")
print_cond(xr$true, "true")
print_cond(xr$additive, "additive")
print_cond(xr$pyro, "pyro")
```

We gain a more comprehensive picture by computing the total variation (TV)
distance between each method's estimate of $\Pr(X, R)$ and the true
distribution. The `eval_joints()` function computes all the TV distances and
ranks the methods by their performance. Smaller is better.

Included in the table are several "raked" estimates, which were constructed by
taking the original $\Pr(X,R)$ estimate and adjusting it (via raking, a.k.a.
iterative proportional fitting) so that the margins match the known sample
marginal distribution $\Pr(X)$ and the assumed $\Pr(R)$ (here, we use the true
$\Pr(R)$; in some applications $\Pr(R)$ may be unknown). As we can see, raking
generally improves the quality of the estimate. Raking is not useful for the
model explored here, since that model estimates $\Pr(X|R)$ directly, and the
$X$-margin is already extremely close to correct.

```{r eval}
eval_joints(xr$true, "tv",
            bis = xr$bis,
            bisg = xr$bisg,
            nonparam = xr$nonparam,
            pyro = xr$pyro) %>%
            #additive = xr$additive) %>%
     knitr::kable()
```

# Implied individual race probabilities

We can use the estimated $\vb P(G, Z)$ parameters to compute
$\Pr(R\mid G,Z,X,S)$ for each individual. To measure the accuracy of these
probabilities, we can use the logarithmic scoring function. Larger (smaller in
absolute value) scores indicate better calibration and sharpness. Once again,
the above model outperforms the original Bayes-rule probabilities, as well as
the Gibbs sampler.

```{r indiv}
m_stan = matrix(rstan::get_posterior_mean(fit$stan, pars="p_r")[, 1], 
                ncol=5, byrow=TRUE) %>%
    `colnames<-`(colnames(p_xr))
m_gibbs = t(apply(fit$gibbs, 1, \(x) prop.table(tabulate(x[-1:-100], nbins=5))))

eval_log_score(naive_name = fit$base_surn,
               naive = fit$baseline,
               gibbs = m_gibbs,
               stan = m_stan) %>%
     knitr::kable()
```

# Applications to algorithmic decisionmaking

We can briefly illustrate one way this method could be applied to an algorithmic
decisionmaking / bias evaluation context. First, we'll simulate a binary outcome
$Y$ based on party and ZIP code. As we can see in the conditional tables below,
$Y$ is strongly biased by party, which has the effect of introducing racial
bias. How well can we discover this racial bias without observing race jointly
with $Y$?

```{r algo-gen, results="hold"}
set.seed(5118)
form = ~ party * zip
XGZ = model.matrix(form, data=model.frame(form, voters, na.action=NULL)) %>%
    apply(2, \(x) coalesce(x, 0))
y_coef = rt(ncol(XGZ), df=3)
y_coef[1] = y_coef[1] - mean(XGZ %*% y_coef) # re-center

voters$Y = as.integer(rbernoulli(nrow(XGZ), pnorm(XGZ %*% y_coef)))
voters$Y_est = fitted(glm.fit(XGZ, voters$Y, family=binomial("probit")))

p_yr = prop.table(table(voters$Y, voters$race))
p_yx = prop.table(table(voters$Y, voters$party), margin=2) # Y|X
print_cond(p_yr, "Y|R")
cat("Y|X\n")
print(round(100 * p_yx))
```

Quite well, it turns out. We can either use the individually imputed race
probabilities to compute an estimated joint distribution, or we can directly
compute an estimate of $\Pr(Y,R)$ using our estimate of $\Pr(X,R)$ and the known
$\Pr(Y\mid X)$ distribution. Both methods work better than the naive
probabilities, but the direct method performs the best.

```{r algo-eval}
yr = list(
    true = p_yr,
    baseline = map(0:1, ~ colMeans(fit$base_surn * (voters$Y == .))) %>%
        do.call(rbind, .),
    gibbs = map(0:1, ~ colMeans(m_gibbs * (voters$Y == .))) %>%
        do.call(rbind, .),
    lsq_raked = p_yx %*%  rake(fit$lsq, p_x, p_r),
    stan_indiv = map(0:1, ~ colMeans(m_stan * (voters$Y == .))) %>%
        do.call(rbind, .),
    stan_matrix = p_yx %*% xr$stan
)

eval_joints(p_yr, "tv",
            naive = yr$baseline,
            gibbs = yr$gibbs,
            lsq = yr$lsq_raked,
            stan_indiv = yr$stan_indiv,
            stan_matrix = yr$stan_matrix) %>%
     knitr::kable()
```

## Calibration

We can show, since $Y$ depends only on $X$ and $G$, that
$Y\indep \hat R \mid \hat Y$, where $\hat R$ are the race probabilities, and
$\hat Y$ is the predicted probability of $Y$ given $X, G$. If this is violated
in our actual data, then we know we have a problem, and might try adjusting the
$\hat R$ to fix it. At least in this county, there appears to be no problem.

```{r}
# drop intercept since probabilities sum to 1 => use 1 df
m = glm(Y ~ 0 + Y_est + m_stan, data=voters)
summary(m)$coefficients %>%
     knitr::kable()
```

# Checking marginal race probabilities {.appendix}

```{r marginal}
rbind(true = p_r,
      base_surn = colMeans(fit$base_surn),
      baseline = colMeans(fit$baseline),
      gibbs = colMeans(m_gibbs),
      stan = colMeans(m_stan))
```

# Error correlation {.appendix}

Are the errors in $p(X, R)$ correlated between the methods?

```{r errors}
c(`Gibbs/Stan (vs. true)` = cor(as.numeric(xr$gibbs - xr$true), 
                                as.numeric(xr$stan - xr$true)),
  `Gibbs/Stan (vs. base)` = cor(as.numeric(xr$gibbs - xr$base), 
                                as.numeric(xr$stan - xr$base)),
  `NNLS/Stan (vs. true)` = cor(as.numeric(rake(fit$nnls, p_x, p_r) - xr$true), 
                               as.numeric(xr$stan - xr$true)))
```
